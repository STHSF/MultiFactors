{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础因子risk—netural回测结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加费后曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../../')\n",
    "from datetime import datetime, timedelta\n",
    "from m1_xgb import *\n",
    "from src.conf.configuration import regress_conf\n",
    "import xgboost as xgb\n",
    "import gc\n",
    "import pandas as pd\n",
    "from PyFin.api import *\n",
    "from alphamind.api import *\n",
    "from src.conf.models import *\n",
    "import numpy as np\n",
    "from alphamind.execution.naiveexecutor import NaiveExecutor\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data_source = 'postgresql+psycopg2://alpha:alpha@180.166.26.82:8889/alpha'\n",
    "engine = SqlEngine(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe = Universe('zz500')\n",
    "freq = '2b'\n",
    "benchmark_code = 905\n",
    "start_date = '2019-01-01'\n",
    "end_date = '2019-08-01'\n",
    "ref_dates = makeSchedule(start_date, end_date, freq, 'china.sse')\n",
    "horizon = map_freq(freq)\n",
    "industry_name = 'sw'\n",
    "industry_level = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_factor_store = {\n",
    "    'AccountsPayablesTDays': CSQuantiles(LAST('AccountsPayablesTDays'), groups='sw1'), \n",
    "    'AccountsPayablesTRate': CSQuantiles(LAST('AccountsPayablesTRate'), groups='sw1'), \n",
    "    'AdminiExpenseRate': CSQuantiles(LAST('AdminiExpenseRate'), groups='sw1'), \n",
    "    'ARTDays': CSQuantiles(LAST('ARTDays'), groups='sw1'), \n",
    "    'ARTRate': CSQuantiles(LAST('ARTRate'), groups='sw1'), \n",
    "    'ASSI': CSQuantiles(LAST('ASSI'), groups='sw1'), \n",
    "    'BLEV': CSQuantiles(LAST('BLEV'), groups='sw1'), \n",
    "    'BondsPayableToAsset': CSQuantiles(LAST('BondsPayableToAsset'), groups='sw1'), \n",
    "    'CashRateOfSales': CSQuantiles(LAST('CashRateOfSales'), groups='sw1'), \n",
    "    'CashToCurrentLiability': CSQuantiles(LAST('CashToCurrentLiability'), groups='sw1'), \n",
    "    'CMRA': CSQuantiles(LAST('CMRA'), groups='sw1'), \n",
    "    'CTOP': CSQuantiles(LAST('CTOP'), groups='sw1'), \n",
    "    'CTP5': CSQuantiles(LAST('CTP5'), groups='sw1'), \n",
    "    'CurrentAssetsRatio': CSQuantiles(LAST('CurrentAssetsRatio'), groups='sw1'), \n",
    "    'CurrentAssetsTRate': CSQuantiles(LAST('CurrentAssetsTRate'), groups='sw1'), \n",
    "    'CurrentRatio': CSQuantiles(LAST('CurrentRatio'), groups='sw1'), \n",
    "    'DAVOL10': CSQuantiles(LAST('DAVOL10'), groups='sw1'), \n",
    "    'DAVOL20': CSQuantiles(LAST('DAVOL20'), groups='sw1'), \n",
    "    'DAVOL5': CSQuantiles(LAST('DAVOL5'), groups='sw1'), \n",
    "    'DDNBT': CSQuantiles(LAST('DDNBT'), groups='sw1'), \n",
    "    'DDNCR': CSQuantiles(LAST('DDNCR'), groups='sw1'), \n",
    "    'DDNSR': CSQuantiles(LAST('DDNSR'), groups='sw1'), \n",
    "    'DebtEquityRatio': CSQuantiles(LAST('DebtEquityRatio'), groups='sw1'), \n",
    "    'DebtsAssetRatio': CSQuantiles(LAST('DebtsAssetRatio'), groups='sw1'), \n",
    "    'DHILO': CSQuantiles(LAST('DHILO'), groups='sw1'), \n",
    "    'DilutedEPS': CSQuantiles(LAST('DilutedEPS'), groups='sw1'), \n",
    "    'DVRAT': CSQuantiles(LAST('DVRAT'), groups='sw1'), \n",
    "    'EBITToTOR': CSQuantiles(LAST('EBITToTOR'), groups='sw1'), \n",
    "    'EGRO': CSQuantiles(LAST('EGRO'), groups='sw1'), \n",
    "    'EMA10': CSQuantiles(LAST('EMA10'), groups='sw1'), \n",
    "    'EMA120': CSQuantiles(LAST('EMA120'), groups='sw1'), \n",
    "    'EMA20': CSQuantiles(LAST('EMA20'), groups='sw1'), \n",
    "    'EMA5': CSQuantiles(LAST('EMA5'), groups='sw1'), \n",
    "    'EMA60': CSQuantiles(LAST('EMA60'), groups='sw1'), \n",
    "    'EPS': CSQuantiles(LAST('EPS'), groups='sw1'), \n",
    "    'EquityFixedAssetRatio': CSQuantiles(LAST('EquityFixedAssetRatio'), groups='sw1'), \n",
    "    'EquityToAsset': CSQuantiles(LAST('EquityToAsset'), groups='sw1'), \n",
    "    'EquityTRate': CSQuantiles(LAST('EquityTRate'), groups='sw1'), \n",
    "    'ETOP': CSQuantiles(LAST('ETOP'), groups='sw1'), \n",
    "    'ETP5': CSQuantiles(LAST('ETP5'), groups='sw1'), \n",
    "    'FinancialExpenseRate': CSQuantiles(LAST('FinancialExpenseRate'), groups='sw1'), \n",
    "    'FinancingCashGrowRate': CSQuantiles(LAST('FinancingCashGrowRate'), groups='sw1'), \n",
    "    'FixAssetRatio': CSQuantiles(LAST('FixAssetRatio'), groups='sw1'), \n",
    "    'FixedAssetsTRate': CSQuantiles(LAST('FixedAssetsTRate'), groups='sw1'), \n",
    "    'GrossIncomeRatio': CSQuantiles(LAST('GrossIncomeRatio'), groups='sw1'), \n",
    "    'HBETA': CSQuantiles(LAST('HBETA'), groups='sw1'), \n",
    "    'HSIGMA': CSQuantiles(LAST('HBETA'), groups='sw1'), \n",
    "    'IntangibleAssetRatio': CSQuantiles(LAST('IntangibleAssetRatio'), groups='sw1'), \n",
    "    'InventoryTDays': CSQuantiles(LAST('InventoryTDays'), groups='sw1'), \n",
    "    'InventoryTRate': CSQuantiles(LAST('InventoryTRate'), groups='sw1'), \n",
    "    'InvestCashGrowRate': CSQuantiles(LAST('InvestCashGrowRate'), groups='sw1'), \n",
    "    'LCAP': CSQuantiles(LAST('LCAP'), groups='sw1'), \n",
    "    'LFLO': CSQuantiles(LAST('LFLO'), groups='sw1'), \n",
    "    'LongDebtToAsset': CSQuantiles(LAST('LongDebtToAsset'), groups='sw1'), \n",
    "    'LongDebtToWorkingCapital': CSQuantiles(LAST('LongDebtToWorkingCapital'), groups='sw1'), \n",
    "    'LongTermDebtToAsset': CSQuantiles(LAST('LongTermDebtToAsset'), groups='sw1'), \n",
    "    'MA10': CSQuantiles(LAST('MA10'), groups='sw1'), \n",
    "    'MA120': CSQuantiles(LAST('MA120'), groups='sw1'), \n",
    "    'MA20': CSQuantiles(LAST('MA20'), groups='sw1'), \n",
    "    'MA5': CSQuantiles(LAST('MA5'), groups='sw1'), \n",
    "    'MA60': CSQuantiles(LAST('MA60'), groups='sw1'), \n",
    "    'MAWVAD': CSQuantiles(LAST('MAWVAD'), groups='sw1'), \n",
    "    'MFI': CSQuantiles(LAST('MFI'), groups='sw1'), \n",
    "    'MLEV': CSQuantiles(LAST('MLEV'), groups='sw1'), \n",
    "    'NetAssetGrowRate': CSQuantiles(LAST('NetAssetGrowRate'), groups='sw1'), \n",
    "    'NetProfitGrowRate': CSQuantiles(LAST('NetProfitGrowRate'), groups='sw1'), \n",
    "    'NetProfitRatio': CSQuantiles(LAST('NetProfitRatio'), groups='sw1'), \n",
    "    'NOCFToOperatingNI': CSQuantiles(LAST('NetProfitRatio'), groups='sw1'), \n",
    "    'NonCurrentAssetsRatio': CSQuantiles(LAST('NonCurrentAssetsRatio'), groups='sw1'), \n",
    "    'NPParentCompanyGrowRate': CSQuantiles(LAST('NPParentCompanyGrowRate'), groups='sw1'), \n",
    "    'NPToTOR': CSQuantiles(LAST('NPToTOR'), groups='sw1'), \n",
    "    'OperatingExpenseRate': CSQuantiles(LAST('OperatingExpenseRate'), groups='sw1'), \n",
    "    'OperatingProfitGrowRate': CSQuantiles(LAST('OperatingProfitGrowRate'), groups='sw1'), \n",
    "    'OperatingProfitRatio': CSQuantiles(LAST('OperatingProfitRatio'), groups='sw1'), \n",
    "    'OperatingProfitToTOR': CSQuantiles(LAST('OperatingProfitToTOR'), groups='sw1'), \n",
    "    'OperatingRevenueGrowRate': CSQuantiles(LAST('OperatingRevenueGrowRate'), groups='sw1'), \n",
    "    'OperCashGrowRate': CSQuantiles(LAST('OperCashGrowRate'), groups='sw1'), \n",
    "    'OperCashInToCurrentLiability': CSQuantiles(LAST('OperCashInToCurrentLiability'), groups='sw1'), \n",
    "    'PB': CSQuantiles(LAST('PB'), groups='sw1'), \n",
    "    'PCF': CSQuantiles(LAST('PCF'), groups='sw1'), \n",
    "    'PE': CSQuantiles(LAST('PE'), groups='sw1'), \n",
    "    'PS': CSQuantiles(LAST('PS'), groups='sw1'), \n",
    "    'PSY': CSQuantiles(LAST('PSY'), groups='sw1'), \n",
    "    'QuickRatio': CSQuantiles(LAST('QuickRatio'), groups='sw1'), \n",
    "    'REVS10': CSQuantiles(LAST('REVS10'), groups='sw1'), \n",
    "    'REVS20': CSQuantiles(LAST('REVS20'), groups='sw1'), \n",
    "    'REVS5': CSQuantiles(LAST('REVS5'), groups='sw1'), \n",
    "    'ROA': CSQuantiles(LAST('REVS5'), groups='sw1'), \n",
    "    'ROA5': CSQuantiles(LAST('ROA5'), groups='sw1'), \n",
    "    'ROE': CSQuantiles(LAST('ROE'), groups='sw1'), \n",
    "    'ROE5': CSQuantiles(LAST('ROE5'), groups='sw1'), \n",
    "    'RSI': CSQuantiles(LAST('RSI'), groups='sw1'), \n",
    "    'RSTR12': CSQuantiles(LAST('RSTR12'), groups='sw1'), \n",
    "    'RSTR24': CSQuantiles(LAST('RSTR24'), groups='sw1'), \n",
    "    'SalesCostRatio': CSQuantiles(LAST('SalesCostRatio'), groups='sw1'), \n",
    "    'SaleServiceCashToOR': CSQuantiles(LAST('SaleServiceCashToOR'), groups='sw1'), \n",
    "    'SUE': CSQuantiles(LAST('SUE'), groups='sw1')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "factor_data_org = engine.fetch_factor_range(universe, basic_factor_store, dates=ref_dates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "industry = engine.fetch_industry_range(universe, dates=ref_dates)\n",
    "factor_data = pd.merge(factor_data_org, industry, on=['trade_date', 'code']).fillna(0.)\n",
    "risk_total = engine.fetch_risk_model_range(universe, dates=ref_dates)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "return_data = engine.fetch_dx_return_range(universe, dates=ref_dates, horizon=horizon, offset=0,benchmark = benchmark_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "benchmark_total = engine.fetch_benchmark_range(dates=ref_dates, benchmark=benchmark_code)\n",
    "industry_total = engine.fetch_industry_matrix_range(universe, dates=ref_dates, category=industry_name, level=industry_level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraintes settings\n",
    "industry_names = industry_list(industry_name, industry_level)\n",
    "constraint_risk = ['EARNYILD', 'LIQUIDTY', 'GROWTH', 'SIZE', 'BETA', 'MOMENTUM'] + industry_names\n",
    "total_risk_names = constraint_risk + ['benchmark', 'total']\n",
    "\n",
    "b_type = []\n",
    "l_val = []\n",
    "u_val = []\n",
    "\n",
    "for name in total_risk_names:\n",
    "    if name == 'benchmark':\n",
    "        b_type.append(BoundaryType.RELATIVE)\n",
    "        l_val.append(0.0)\n",
    "        u_val.append(1.0)\n",
    "    elif name == 'total':\n",
    "        b_type.append(BoundaryType.ABSOLUTE)\n",
    "        l_val.append(.0)\n",
    "        u_val.append(.0)\n",
    "    else:\n",
    "        b_type.append(BoundaryType.ABSOLUTE)\n",
    "        l_val.append(-1.005)\n",
    "        u_val.append(1.005)\n",
    "\n",
    "bounds = create_box_bounds(total_risk_names, b_type, l_val, u_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.merge(factor_data, return_data, on=['trade_date', 'code']).dropna()\n",
    "# train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['AccountsPayablesTDays', 'AccountsPayablesTRate', 'AdminiExpenseRate', 'ARTDays', \n",
    "    'ARTRate', 'ASSI', 'BLEV', 'BondsPayableToAsset', 'CashRateOfSales', 'CashToCurrentLiability', \n",
    "    'CMRA', 'CTOP', 'CTP5', 'CurrentAssetsRatio', 'CurrentAssetsTRate', 'CurrentRatio', 'DAVOL10', \n",
    "    'DAVOL20', 'DAVOL5', 'DDNBT', 'DDNCR', 'DDNSR', 'DebtEquityRatio', 'DebtsAssetRatio', 'DHILO', \n",
    "    'DilutedEPS', 'DVRAT', 'EBITToTOR', 'EGRO', 'EMA10', 'EMA120', 'EMA20', 'EMA5', 'EMA60', 'EPS',\n",
    "    'EquityFixedAssetRatio', 'EquityToAsset', 'EquityTRate', 'ETOP', 'ETP5', 'FinancialExpenseRate', \n",
    "    'FinancingCashGrowRate', 'FixAssetRatio', 'FixedAssetsTRate', 'GrossIncomeRatio', 'HBETA', \n",
    "    'HSIGMA', 'IntangibleAssetRatio', 'InventoryTDays', 'InventoryTRate', 'InvestCashGrowRate', \n",
    "    'LCAP', 'LFLO', 'LongDebtToAsset', 'LongDebtToWorkingCapital', 'LongTermDebtToAsset', \n",
    "    'MA10', 'MA120', 'MA20', 'MA5', 'MA60', 'MAWVAD', 'MFI', 'MLEV', 'NetAssetGrowRate', \n",
    "    'NetProfitGrowRate', 'NetProfitRatio', 'NOCFToOperatingNI', 'NonCurrentAssetsRatio', \n",
    "    'NPParentCompanyGrowRate', 'NPToTOR', 'OperatingExpenseRate', 'OperatingProfitGrowRate', \n",
    "    'OperatingProfitRatio', 'OperatingProfitToTOR', 'OperatingRevenueGrowRate', 'OperCashGrowRate',\n",
    "    'OperCashInToCurrentLiability', 'PB','PCF', 'PE','PS','PSY', 'QuickRatio', 'REVS10', \n",
    "    'REVS20', 'REVS5', 'ROA','ROA5', 'ROE', 'ROE5', 'RSI', 'RSTR12', 'RSTR24', 'SalesCostRatio', \n",
    "    'SaleServiceCashToOR' , 'SUE']\n",
    "\n",
    "label = ['dx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from datetime import datetime, timedelta\n",
    "from m1_xgb import *\n",
    "from src.conf.configuration import regress_conf\n",
    "import xgboost as xgb\n",
    "import gc\n",
    "\n",
    "def create_scenario():\n",
    "    weight_gap = 1\n",
    "    transact_cost = 0.003\n",
    "\n",
    "    executor = NaiveExecutor()\n",
    "    turn_overs = []\n",
    "    leverags = []\n",
    "    trade_dates = []\n",
    "    current_pos = pd.DataFrame()\n",
    "    previous_pos = pd.DataFrame()\n",
    "    tune_record = pd.DataFrame()  \n",
    "    rets = []\n",
    "    net_rets = []\n",
    "    turn_overs = []\n",
    "    leverags = []\n",
    "    ics = []\n",
    "    # take ref_dates[i] as an example\n",
    "    for i, ref_date in enumerate(ref_dates):\n",
    "        alpha_logger.info('{0} is start'.format(ref_date))\n",
    "\n",
    "        # machine learning model\n",
    "        # Filter Training data\n",
    "        # train data\n",
    "        trade_date_pre = ref_date - timedelta(days=1)\n",
    "        trade_date_pre_80 = ref_date - timedelta(days=80)\n",
    "        \n",
    "        # train = train_data[(train_data.trade_date <= trade_date_pre) & (trade_date_pre_80 <= train_data.trade_date)].dropna()\n",
    "        train = train_data[train_data.trade_date <= trade_date_pre].dropna()\n",
    "\n",
    "        if len(train) <= 0:\n",
    "            continue\n",
    "        x_train = train[features]\n",
    "        y_train = train[label]\n",
    "        alpha_logger.info('len_x_train: {0}, len_y_train: {1}'.format(len(x_train.values), len(y_train.values)))\n",
    "        alpha_logger.info('X_train.shape={0}, X_test.shape = {1}'.format(np.shape(x_train), np.shape(y_train)))\n",
    "\n",
    "        # xgb_configuration\n",
    "        regress_conf.xgb_config_r()\n",
    "        regress_conf.cv_folds = None\n",
    "        regress_conf.early_stop_round = 10\n",
    "        regress_conf.max_round = 800\n",
    "        tic = time.time()\n",
    "        # training\n",
    "        xgb_model = XGBooster(regress_conf)\n",
    "        xgb_model.set_params(tree_method='gpu_hist', max_depth=5)\n",
    "        # xgb_model.set_params(max_depth=5)\n",
    "        print(xgb_model.get_params)\n",
    "        best_score, best_round, cv_rounds, best_model = xgb_model.fit(x_train, y_train)\n",
    "        alpha_logger.info('Training time cost {}s'.format(time.time() - tic))\n",
    "        alpha_logger.info('best_score = {}, best_round = {}'.format(best_score, best_round))\n",
    "    \n",
    "        # Test data\n",
    "        total_data_test_excess = train_data[train_data.trade_date == ref_date]\n",
    "        alpha_logger.info('{0} total_data_test_excess: {1}'.format(ref_date, len(total_data_test_excess)))\n",
    "\n",
    "        if len(total_data_test_excess) <= 0:\n",
    "            alpha_logger.info('{0} HAS NO DATA!!!'.format(ref_date))\n",
    "            continue\n",
    "\n",
    "        industry_matrix = industry_total[industry_total.trade_date == ref_date]\n",
    "        benchmark_w = benchmark_total[benchmark_total.trade_date == ref_date]\n",
    "        risk_matrix = risk_total[risk_total.trade_date == ref_date]\n",
    "\n",
    "        total_data = pd.merge(industry_matrix, benchmark_w, on=['code'], how='left').fillna(0.)\n",
    "        total_data = pd.merge(total_data, risk_matrix, on=['code'])\n",
    "        alpha_logger.info('{0} len_of_total_data: {1}'.format(ref_date, len(total_data)))\n",
    "\n",
    "        total_data_test_excess = pd.merge(total_data, total_data_test_excess, on=['code'])\n",
    "        alpha_logger.info('{0} len_of_total_data_test_excess: {1}'.format(ref_date, len(total_data_test_excess)))\n",
    "        \n",
    "        codes = total_data_test_excess.code.values.tolist()\n",
    "        alpha_logger.info('{0} full re-balance: {1}'.format(ref_date, len(codes)))\n",
    "        dx_returns = return_data[return_data.trade_date == ref_date][['code', 'dx']]\n",
    "\n",
    "        benchmark_w = total_data_test_excess.weight.values\n",
    "        alpha_logger.info('shape_of_benchmark_w: {}'.format(np.shape(benchmark_w)))\n",
    "        is_in_benchmark = (benchmark_w > 0.).astype(float).reshape((-1, 1))\n",
    "        total_risk_exp = np.concatenate([total_data_test_excess[constraint_risk].values.astype(float),\n",
    "                                         is_in_benchmark,\n",
    "                                         np.ones_like(is_in_benchmark)],\n",
    "                                         axis=1)\n",
    "        alpha_logger.info('shape_of_total_risk_exp_pre: {}'.format(np.shape(total_risk_exp)))\n",
    "        total_risk_exp = pd.DataFrame(total_risk_exp, columns=total_risk_names)\n",
    "        alpha_logger.info('shape_of_total_risk_exp: {}'.format(np.shape(total_risk_exp)))\n",
    "        constraints = LinearConstraints(bounds, total_risk_exp, benchmark_w)\n",
    "        alpha_logger.info('constraints: {0} in {1}'.format(np.shape(constraints.risk_targets()), ref_date))\n",
    "\n",
    "        lbound = np.maximum(0., benchmark_w - weight_gap)\n",
    "        ubound = weight_gap + benchmark_w\n",
    "        alpha_logger.info('lbound: {0} in {1}'.format(np.shape(lbound), ref_date))\n",
    "        alpha_logger.info('ubound: {0} in {1}'.format(np.shape(ubound), ref_date))\n",
    "\n",
    "        # predict\n",
    "        x_pred = total_data_test_excess[features]\n",
    "        predict_xgboost = xgb_model.predict(best_model, x_pred)\n",
    "        a = np.shape(predict_xgboost)\n",
    "        predict_xgboost = np.reshape(predict_xgboost, (a[0], -1)).astype(np.float64)\n",
    "        alpha_logger.info('shape_of_predict_xgboost: {}'.format(np.shape(predict_xgboost)))\n",
    "        # alpha_logger.info('predict_xgboost: {}'.format(predict_xgboost))\n",
    "        del xgb_model\n",
    "        del best_model\n",
    "        gc.collect()\n",
    "        \n",
    "        # backtest\n",
    "        try:\n",
    "            target_pos, _ = er_portfolio_analysis(predict_xgboost,\n",
    "                                                  total_data_test_excess['industry'].values,\n",
    "                                                  None,\n",
    "                                                  constraints,\n",
    "                                                  False,\n",
    "                                                  benchmark_w,\n",
    "                                                  method = 'risk_neutral',\n",
    "                                                  lbound=lbound,\n",
    "                                                  ubound=ubound)\n",
    "        except:\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "            alpha_logger.info('target_pos: {}'.format(target_pos))\n",
    "        alpha_logger.info('target_pos_shape: {}'.format(np.shape(target_pos)))\n",
    "        alpha_logger.info('len_codes:{}'.format(np.shape(codes)))\n",
    "        target_pos['code'] = codes\n",
    "        \n",
    "        result = pd.merge(target_pos, dx_returns, on=['code'])\n",
    "        result['trade_date'] = ref_date\n",
    "        tune_record = tune_record.append(result)\n",
    "        alpha_logger.info('len_result: {}'.format(len(result)))\n",
    "\n",
    "        # excess_return = np.exp(result.dx.values) - 1. - index_return.loc[ref_date, 'dx']\n",
    "        excess_return = np.exp(result.dx.values) - 1.\n",
    "        ret = result.weight.values @ excess_return\n",
    "        \n",
    "        trade_dates.append(ref_date)\n",
    "        rets.append(np.log(1. + ret))\n",
    "        alpha_logger.info('len_rets: {}, len_trade_dates: {}'.format(len(rets), len(trade_dates)))\n",
    "        \n",
    "        turn_over_org, current_pos = executor.execute(target_pos=target_pos)\n",
    "        turn_over = turn_over_org / sum(target_pos.weight.values)\n",
    "        executor.set_current(current_pos)\n",
    "        net_rets.append(np.log(1. + ret - transact_cost * turn_over))        \n",
    "        alpha_logger.info('len_net_rets: {}, len_trade_dates: {}'.format(len(net_rets), len(trade_dates)))\n",
    "\n",
    "        alpha_logger.info('{} is finished'.format(ref_date))\n",
    "        \n",
    "    # ret_df = pd.DataFrame({'xgb_regress': rets}, index=trade_dates)\n",
    "    ret_df = pd.DataFrame({'xgb_regress': rets, 'net_xgb_regress':net_rets}, index=trade_dates)\n",
    "    ret_df.loc[advanceDateByCalendar('china.sse', ref_dates[-1], freq).strftime('%Y-%m-%d')] = 0.\n",
    "    ret_df = ret_df.shift(1)\n",
    "    ret_df.iloc[0] = 0.\n",
    "    return ret_df, tune_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_df, tune_record = create_scenario()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyecharts import options as opts\n",
    "from example.commons import Collector\n",
    "from pyecharts.charts import Line\n",
    "\n",
    "plot = ret_df[['xgb_regress', 'net_xgb_regress']].cumsum()\n",
    "v1 = list(plot.index)\n",
    "v2 = list(plot.xgb_regress)\n",
    "v3 = list(plot.net_xgb_regress)\n",
    "\n",
    "line_chart = (\n",
    "    Line()\n",
    "    .add_xaxis(v1)\n",
    "    .add_yaxis(\"xgb_regress\", v2)\n",
    "    .add_yaxis(\"net_xgb_regress\", v3)\n",
    "    .set_series_opts(\n",
    "        label_opts=opts.LabelOpts(is_show=False),\n",
    "    )\n",
    "    .set_global_opts(\n",
    "        xaxis_opts=opts.AxisOpts(is_scale=True),\n",
    "        yaxis_opts=opts.AxisOpts(\n",
    "            is_scale=True,\n",
    "            splitarea_opts=opts.SplitAreaOpts(\n",
    "                is_show=True, areastyle_opts=opts.AreaStyleOpts(opacity=1)\n",
    "            ),\n",
    "        ),\n",
    "        datazoom_opts=[opts.DataZoomOpts(pos_bottom=\"-1%\")],\n",
    "        title_opts=opts.TitleOpts(title='Fixed freq rebalanced: {0}'.format(freq)),\n",
    "    )\n",
    ")\n",
    "\n",
    "line_chart.render_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"年化收益: {0:.2f}\".format(len(ret_df) * np.mean(ret_df['net_xgb_regress'])))\n",
    "print(\"夏普比率: {0:.2f}\".format(np.sqrt(126) * np.mean(ret_df['net_xgb_regress']/ np.std(ret_df['net_xgb_regress']))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"年化收益: {0:.2f}\".format(len(ret_df) * np.mean(ret_df['xgb_regress'])))\n",
    "print(\"夏普比率: {0:.2f}\".format(np.sqrt(126) * np.mean(ret_df['xgb_regress']/ np.std(ret_df['xgb_regress']))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
