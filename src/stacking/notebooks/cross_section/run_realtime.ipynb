{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础因子加alpha191实时计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../../')\n",
    "import pandas as pd\n",
    "from PyFin.api import *\n",
    "from alphamind.api import *\n",
    "from conf.models import *\n",
    "import numpy as np\n",
    "from alphamind.execution.naiveexecutor import NaiveExecutor\n",
    "from matplotlib import pyplot as plt\n",
    "from stacking import factor_store, feature_list\n",
    "from optimization.bayes_optimization_xgb import *\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('max_colwidth',100)\n",
    "\n",
    "data_source = 'postgresql+psycopg2://alpha:alpha@180.166.26.82:8889/alpha'\n",
    "engine = SqlEngine(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2009, 12, 1, 0, 0),\n",
       " datetime.datetime(2009, 12, 8, 0, 0),\n",
       " datetime.datetime(2009, 12, 15, 0, 0),\n",
       " datetime.datetime(2009, 12, 22, 0, 0),\n",
       " datetime.datetime(2009, 12, 29, 0, 0),\n",
       " datetime.datetime(2010, 1, 6, 0, 0),\n",
       " datetime.datetime(2010, 1, 13, 0, 0),\n",
       " datetime.datetime(2010, 1, 20, 0, 0),\n",
       " datetime.datetime(2010, 1, 27, 0, 0),\n",
       " datetime.datetime(2010, 2, 3, 0, 0),\n",
       " datetime.datetime(2010, 2, 10, 0, 0),\n",
       " datetime.datetime(2010, 2, 24, 0, 0),\n",
       " datetime.datetime(2010, 3, 3, 0, 0),\n",
       " datetime.datetime(2010, 3, 10, 0, 0),\n",
       " datetime.datetime(2010, 3, 17, 0, 0),\n",
       " datetime.datetime(2010, 3, 24, 0, 0),\n",
       " datetime.datetime(2010, 3, 31, 0, 0),\n",
       " datetime.datetime(2010, 4, 8, 0, 0),\n",
       " datetime.datetime(2010, 4, 15, 0, 0),\n",
       " datetime.datetime(2010, 4, 22, 0, 0),\n",
       " datetime.datetime(2010, 4, 29, 0, 0),\n",
       " datetime.datetime(2010, 5, 7, 0, 0),\n",
       " datetime.datetime(2010, 5, 14, 0, 0),\n",
       " datetime.datetime(2010, 5, 21, 0, 0),\n",
       " datetime.datetime(2010, 5, 28, 0, 0),\n",
       " datetime.datetime(2010, 6, 4, 0, 0),\n",
       " datetime.datetime(2010, 6, 11, 0, 0),\n",
       " datetime.datetime(2010, 6, 23, 0, 0),\n",
       " datetime.datetime(2010, 6, 30, 0, 0),\n",
       " datetime.datetime(2010, 7, 7, 0, 0),\n",
       " datetime.datetime(2010, 7, 14, 0, 0),\n",
       " datetime.datetime(2010, 7, 21, 0, 0),\n",
       " datetime.datetime(2010, 7, 28, 0, 0),\n",
       " datetime.datetime(2010, 8, 4, 0, 0),\n",
       " datetime.datetime(2010, 8, 11, 0, 0),\n",
       " datetime.datetime(2010, 8, 18, 0, 0),\n",
       " datetime.datetime(2010, 8, 25, 0, 0),\n",
       " datetime.datetime(2010, 9, 1, 0, 0),\n",
       " datetime.datetime(2010, 9, 8, 0, 0),\n",
       " datetime.datetime(2010, 9, 15, 0, 0),\n",
       " datetime.datetime(2010, 9, 27, 0, 0),\n",
       " datetime.datetime(2010, 10, 11, 0, 0),\n",
       " datetime.datetime(2010, 10, 18, 0, 0),\n",
       " datetime.datetime(2010, 10, 25, 0, 0),\n",
       " datetime.datetime(2010, 11, 1, 0, 0),\n",
       " datetime.datetime(2010, 11, 8, 0, 0),\n",
       " datetime.datetime(2010, 11, 15, 0, 0),\n",
       " datetime.datetime(2010, 11, 22, 0, 0),\n",
       " datetime.datetime(2010, 11, 29, 0, 0),\n",
       " datetime.datetime(2010, 12, 6, 0, 0),\n",
       " datetime.datetime(2010, 12, 13, 0, 0),\n",
       " datetime.datetime(2010, 12, 20, 0, 0),\n",
       " datetime.datetime(2010, 12, 27, 0, 0),\n",
       " datetime.datetime(2011, 1, 4, 0, 0),\n",
       " datetime.datetime(2011, 1, 11, 0, 0),\n",
       " datetime.datetime(2011, 1, 18, 0, 0),\n",
       " datetime.datetime(2011, 1, 25, 0, 0),\n",
       " datetime.datetime(2011, 2, 1, 0, 0),\n",
       " datetime.datetime(2011, 2, 15, 0, 0),\n",
       " datetime.datetime(2011, 2, 22, 0, 0),\n",
       " datetime.datetime(2011, 3, 1, 0, 0),\n",
       " datetime.datetime(2011, 3, 8, 0, 0),\n",
       " datetime.datetime(2011, 3, 15, 0, 0),\n",
       " datetime.datetime(2011, 3, 22, 0, 0),\n",
       " datetime.datetime(2011, 3, 29, 0, 0),\n",
       " datetime.datetime(2011, 4, 7, 0, 0),\n",
       " datetime.datetime(2011, 4, 14, 0, 0),\n",
       " datetime.datetime(2011, 4, 21, 0, 0),\n",
       " datetime.datetime(2011, 4, 28, 0, 0),\n",
       " datetime.datetime(2011, 5, 6, 0, 0),\n",
       " datetime.datetime(2011, 5, 13, 0, 0),\n",
       " datetime.datetime(2011, 5, 20, 0, 0),\n",
       " datetime.datetime(2011, 5, 27, 0, 0),\n",
       " datetime.datetime(2011, 6, 3, 0, 0),\n",
       " datetime.datetime(2011, 6, 13, 0, 0),\n",
       " datetime.datetime(2011, 6, 20, 0, 0),\n",
       " datetime.datetime(2011, 6, 27, 0, 0),\n",
       " datetime.datetime(2011, 7, 4, 0, 0),\n",
       " datetime.datetime(2011, 7, 11, 0, 0),\n",
       " datetime.datetime(2011, 7, 18, 0, 0),\n",
       " datetime.datetime(2011, 7, 25, 0, 0),\n",
       " datetime.datetime(2011, 8, 1, 0, 0),\n",
       " datetime.datetime(2011, 8, 8, 0, 0),\n",
       " datetime.datetime(2011, 8, 15, 0, 0),\n",
       " datetime.datetime(2011, 8, 22, 0, 0),\n",
       " datetime.datetime(2011, 8, 29, 0, 0),\n",
       " datetime.datetime(2011, 9, 5, 0, 0),\n",
       " datetime.datetime(2011, 9, 13, 0, 0),\n",
       " datetime.datetime(2011, 9, 20, 0, 0),\n",
       " datetime.datetime(2011, 9, 27, 0, 0),\n",
       " datetime.datetime(2011, 10, 11, 0, 0),\n",
       " datetime.datetime(2011, 10, 18, 0, 0),\n",
       " datetime.datetime(2011, 10, 25, 0, 0),\n",
       " datetime.datetime(2011, 11, 1, 0, 0),\n",
       " datetime.datetime(2011, 11, 8, 0, 0),\n",
       " datetime.datetime(2011, 11, 15, 0, 0),\n",
       " datetime.datetime(2011, 11, 22, 0, 0),\n",
       " datetime.datetime(2011, 11, 29, 0, 0),\n",
       " datetime.datetime(2011, 12, 6, 0, 0),\n",
       " datetime.datetime(2011, 12, 13, 0, 0),\n",
       " datetime.datetime(2011, 12, 20, 0, 0),\n",
       " datetime.datetime(2011, 12, 27, 0, 0),\n",
       " datetime.datetime(2012, 1, 5, 0, 0),\n",
       " datetime.datetime(2012, 1, 12, 0, 0),\n",
       " datetime.datetime(2012, 1, 19, 0, 0),\n",
       " datetime.datetime(2012, 2, 2, 0, 0),\n",
       " datetime.datetime(2012, 2, 9, 0, 0),\n",
       " datetime.datetime(2012, 2, 16, 0, 0),\n",
       " datetime.datetime(2012, 2, 23, 0, 0),\n",
       " datetime.datetime(2012, 3, 1, 0, 0),\n",
       " datetime.datetime(2012, 3, 8, 0, 0),\n",
       " datetime.datetime(2012, 3, 15, 0, 0),\n",
       " datetime.datetime(2012, 3, 22, 0, 0),\n",
       " datetime.datetime(2012, 3, 29, 0, 0),\n",
       " datetime.datetime(2012, 4, 10, 0, 0),\n",
       " datetime.datetime(2012, 4, 17, 0, 0),\n",
       " datetime.datetime(2012, 4, 24, 0, 0),\n",
       " datetime.datetime(2012, 5, 3, 0, 0),\n",
       " datetime.datetime(2012, 5, 10, 0, 0),\n",
       " datetime.datetime(2012, 5, 17, 0, 0),\n",
       " datetime.datetime(2012, 5, 24, 0, 0),\n",
       " datetime.datetime(2012, 5, 31, 0, 0),\n",
       " datetime.datetime(2012, 6, 7, 0, 0),\n",
       " datetime.datetime(2012, 6, 14, 0, 0),\n",
       " datetime.datetime(2012, 6, 21, 0, 0),\n",
       " datetime.datetime(2012, 6, 29, 0, 0),\n",
       " datetime.datetime(2012, 7, 6, 0, 0),\n",
       " datetime.datetime(2012, 7, 13, 0, 0),\n",
       " datetime.datetime(2012, 7, 20, 0, 0),\n",
       " datetime.datetime(2012, 7, 27, 0, 0),\n",
       " datetime.datetime(2012, 8, 3, 0, 0),\n",
       " datetime.datetime(2012, 8, 10, 0, 0),\n",
       " datetime.datetime(2012, 8, 17, 0, 0),\n",
       " datetime.datetime(2012, 8, 24, 0, 0),\n",
       " datetime.datetime(2012, 8, 31, 0, 0),\n",
       " datetime.datetime(2012, 9, 7, 0, 0),\n",
       " datetime.datetime(2012, 9, 14, 0, 0),\n",
       " datetime.datetime(2012, 9, 21, 0, 0),\n",
       " datetime.datetime(2012, 9, 28, 0, 0),\n",
       " datetime.datetime(2012, 10, 12, 0, 0),\n",
       " datetime.datetime(2012, 10, 19, 0, 0),\n",
       " datetime.datetime(2012, 10, 26, 0, 0),\n",
       " datetime.datetime(2012, 11, 2, 0, 0),\n",
       " datetime.datetime(2012, 11, 9, 0, 0),\n",
       " datetime.datetime(2012, 11, 16, 0, 0),\n",
       " datetime.datetime(2012, 11, 23, 0, 0),\n",
       " datetime.datetime(2012, 11, 30, 0, 0),\n",
       " datetime.datetime(2012, 12, 7, 0, 0),\n",
       " datetime.datetime(2012, 12, 14, 0, 0),\n",
       " datetime.datetime(2012, 12, 21, 0, 0),\n",
       " datetime.datetime(2012, 12, 28, 0, 0),\n",
       " datetime.datetime(2013, 1, 9, 0, 0),\n",
       " datetime.datetime(2013, 1, 16, 0, 0),\n",
       " datetime.datetime(2013, 1, 23, 0, 0),\n",
       " datetime.datetime(2013, 1, 30, 0, 0),\n",
       " datetime.datetime(2013, 2, 6, 0, 0),\n",
       " datetime.datetime(2013, 2, 20, 0, 0),\n",
       " datetime.datetime(2013, 2, 27, 0, 0),\n",
       " datetime.datetime(2013, 3, 6, 0, 0),\n",
       " datetime.datetime(2013, 3, 13, 0, 0),\n",
       " datetime.datetime(2013, 3, 20, 0, 0),\n",
       " datetime.datetime(2013, 3, 27, 0, 0),\n",
       " datetime.datetime(2013, 4, 3, 0, 0),\n",
       " datetime.datetime(2013, 4, 12, 0, 0),\n",
       " datetime.datetime(2013, 4, 19, 0, 0),\n",
       " datetime.datetime(2013, 4, 26, 0, 0),\n",
       " datetime.datetime(2013, 5, 8, 0, 0),\n",
       " datetime.datetime(2013, 5, 15, 0, 0),\n",
       " datetime.datetime(2013, 5, 22, 0, 0),\n",
       " datetime.datetime(2013, 5, 29, 0, 0),\n",
       " datetime.datetime(2013, 6, 5, 0, 0),\n",
       " datetime.datetime(2013, 6, 17, 0, 0),\n",
       " datetime.datetime(2013, 6, 24, 0, 0),\n",
       " datetime.datetime(2013, 7, 1, 0, 0),\n",
       " datetime.datetime(2013, 7, 8, 0, 0),\n",
       " datetime.datetime(2013, 7, 15, 0, 0),\n",
       " datetime.datetime(2013, 7, 22, 0, 0),\n",
       " datetime.datetime(2013, 7, 29, 0, 0),\n",
       " datetime.datetime(2013, 8, 5, 0, 0),\n",
       " datetime.datetime(2013, 8, 12, 0, 0),\n",
       " datetime.datetime(2013, 8, 19, 0, 0),\n",
       " datetime.datetime(2013, 8, 26, 0, 0),\n",
       " datetime.datetime(2013, 9, 2, 0, 0),\n",
       " datetime.datetime(2013, 9, 9, 0, 0),\n",
       " datetime.datetime(2013, 9, 16, 0, 0),\n",
       " datetime.datetime(2013, 9, 25, 0, 0),\n",
       " datetime.datetime(2013, 10, 9, 0, 0),\n",
       " datetime.datetime(2013, 10, 16, 0, 0),\n",
       " datetime.datetime(2013, 10, 23, 0, 0),\n",
       " datetime.datetime(2013, 10, 30, 0, 0),\n",
       " datetime.datetime(2013, 11, 6, 0, 0),\n",
       " datetime.datetime(2013, 11, 13, 0, 0),\n",
       " datetime.datetime(2013, 11, 20, 0, 0),\n",
       " datetime.datetime(2013, 11, 27, 0, 0),\n",
       " datetime.datetime(2013, 12, 4, 0, 0),\n",
       " datetime.datetime(2013, 12, 11, 0, 0),\n",
       " datetime.datetime(2013, 12, 18, 0, 0),\n",
       " datetime.datetime(2013, 12, 25, 0, 0),\n",
       " datetime.datetime(2014, 1, 2, 0, 0),\n",
       " datetime.datetime(2014, 1, 9, 0, 0),\n",
       " datetime.datetime(2014, 1, 16, 0, 0),\n",
       " datetime.datetime(2014, 1, 23, 0, 0),\n",
       " datetime.datetime(2014, 1, 30, 0, 0),\n",
       " datetime.datetime(2014, 2, 13, 0, 0),\n",
       " datetime.datetime(2014, 2, 20, 0, 0),\n",
       " datetime.datetime(2014, 2, 27, 0, 0),\n",
       " datetime.datetime(2014, 3, 6, 0, 0),\n",
       " datetime.datetime(2014, 3, 13, 0, 0),\n",
       " datetime.datetime(2014, 3, 20, 0, 0),\n",
       " datetime.datetime(2014, 3, 27, 0, 0),\n",
       " datetime.datetime(2014, 4, 3, 0, 0),\n",
       " datetime.datetime(2014, 4, 11, 0, 0),\n",
       " datetime.datetime(2014, 4, 18, 0, 0),\n",
       " datetime.datetime(2014, 4, 25, 0, 0),\n",
       " datetime.datetime(2014, 5, 6, 0, 0),\n",
       " datetime.datetime(2014, 5, 13, 0, 0),\n",
       " datetime.datetime(2014, 5, 20, 0, 0),\n",
       " datetime.datetime(2014, 5, 27, 0, 0),\n",
       " datetime.datetime(2014, 6, 4, 0, 0),\n",
       " datetime.datetime(2014, 6, 11, 0, 0),\n",
       " datetime.datetime(2014, 6, 18, 0, 0),\n",
       " datetime.datetime(2014, 6, 25, 0, 0),\n",
       " datetime.datetime(2014, 7, 2, 0, 0),\n",
       " datetime.datetime(2014, 7, 9, 0, 0),\n",
       " datetime.datetime(2014, 7, 16, 0, 0),\n",
       " datetime.datetime(2014, 7, 23, 0, 0),\n",
       " datetime.datetime(2014, 7, 30, 0, 0),\n",
       " datetime.datetime(2014, 8, 6, 0, 0),\n",
       " datetime.datetime(2014, 8, 13, 0, 0),\n",
       " datetime.datetime(2014, 8, 20, 0, 0),\n",
       " datetime.datetime(2014, 8, 27, 0, 0),\n",
       " datetime.datetime(2014, 9, 3, 0, 0),\n",
       " datetime.datetime(2014, 9, 11, 0, 0),\n",
       " datetime.datetime(2014, 9, 18, 0, 0),\n",
       " datetime.datetime(2014, 9, 25, 0, 0),\n",
       " datetime.datetime(2014, 10, 9, 0, 0),\n",
       " datetime.datetime(2014, 10, 16, 0, 0),\n",
       " datetime.datetime(2014, 10, 23, 0, 0),\n",
       " datetime.datetime(2014, 10, 30, 0, 0),\n",
       " datetime.datetime(2014, 11, 6, 0, 0),\n",
       " datetime.datetime(2014, 11, 13, 0, 0),\n",
       " datetime.datetime(2014, 11, 20, 0, 0),\n",
       " datetime.datetime(2014, 11, 27, 0, 0),\n",
       " datetime.datetime(2014, 12, 4, 0, 0),\n",
       " datetime.datetime(2014, 12, 11, 0, 0),\n",
       " datetime.datetime(2014, 12, 18, 0, 0),\n",
       " datetime.datetime(2014, 12, 25, 0, 0),\n",
       " datetime.datetime(2015, 1, 5, 0, 0),\n",
       " datetime.datetime(2015, 1, 12, 0, 0),\n",
       " datetime.datetime(2015, 1, 19, 0, 0),\n",
       " datetime.datetime(2015, 1, 26, 0, 0),\n",
       " datetime.datetime(2015, 2, 2, 0, 0),\n",
       " datetime.datetime(2015, 2, 9, 0, 0),\n",
       " datetime.datetime(2015, 2, 16, 0, 0),\n",
       " datetime.datetime(2015, 3, 2, 0, 0),\n",
       " datetime.datetime(2015, 3, 9, 0, 0),\n",
       " datetime.datetime(2015, 3, 16, 0, 0),\n",
       " datetime.datetime(2015, 3, 23, 0, 0),\n",
       " datetime.datetime(2015, 3, 30, 0, 0),\n",
       " datetime.datetime(2015, 4, 7, 0, 0),\n",
       " datetime.datetime(2015, 4, 14, 0, 0),\n",
       " datetime.datetime(2015, 4, 21, 0, 0),\n",
       " datetime.datetime(2015, 4, 28, 0, 0),\n",
       " datetime.datetime(2015, 5, 6, 0, 0),\n",
       " datetime.datetime(2015, 5, 13, 0, 0),\n",
       " datetime.datetime(2015, 5, 20, 0, 0),\n",
       " datetime.datetime(2015, 5, 27, 0, 0),\n",
       " datetime.datetime(2015, 6, 3, 0, 0),\n",
       " datetime.datetime(2015, 6, 10, 0, 0),\n",
       " datetime.datetime(2015, 6, 17, 0, 0),\n",
       " datetime.datetime(2015, 6, 25, 0, 0),\n",
       " datetime.datetime(2015, 7, 2, 0, 0),\n",
       " datetime.datetime(2015, 7, 9, 0, 0),\n",
       " datetime.datetime(2015, 7, 16, 0, 0),\n",
       " datetime.datetime(2015, 7, 23, 0, 0),\n",
       " datetime.datetime(2015, 7, 30, 0, 0),\n",
       " datetime.datetime(2015, 8, 6, 0, 0),\n",
       " datetime.datetime(2015, 8, 13, 0, 0),\n",
       " datetime.datetime(2015, 8, 20, 0, 0),\n",
       " datetime.datetime(2015, 8, 27, 0, 0),\n",
       " datetime.datetime(2015, 9, 7, 0, 0),\n",
       " datetime.datetime(2015, 9, 14, 0, 0),\n",
       " datetime.datetime(2015, 9, 21, 0, 0),\n",
       " datetime.datetime(2015, 9, 28, 0, 0),\n",
       " datetime.datetime(2015, 10, 12, 0, 0),\n",
       " datetime.datetime(2015, 10, 19, 0, 0),\n",
       " datetime.datetime(2015, 10, 26, 0, 0),\n",
       " datetime.datetime(2015, 11, 2, 0, 0),\n",
       " datetime.datetime(2015, 11, 9, 0, 0),\n",
       " datetime.datetime(2015, 11, 16, 0, 0),\n",
       " datetime.datetime(2015, 11, 23, 0, 0),\n",
       " datetime.datetime(2015, 11, 30, 0, 0),\n",
       " datetime.datetime(2015, 12, 7, 0, 0),\n",
       " datetime.datetime(2015, 12, 14, 0, 0),\n",
       " datetime.datetime(2015, 12, 21, 0, 0),\n",
       " datetime.datetime(2015, 12, 28, 0, 0),\n",
       " datetime.datetime(2016, 1, 5, 0, 0),\n",
       " datetime.datetime(2016, 1, 12, 0, 0),\n",
       " datetime.datetime(2016, 1, 19, 0, 0),\n",
       " datetime.datetime(2016, 1, 26, 0, 0),\n",
       " datetime.datetime(2016, 2, 2, 0, 0),\n",
       " datetime.datetime(2016, 2, 16, 0, 0),\n",
       " datetime.datetime(2016, 2, 23, 0, 0),\n",
       " datetime.datetime(2016, 3, 1, 0, 0),\n",
       " datetime.datetime(2016, 3, 8, 0, 0),\n",
       " datetime.datetime(2016, 3, 15, 0, 0),\n",
       " datetime.datetime(2016, 3, 22, 0, 0),\n",
       " datetime.datetime(2016, 3, 29, 0, 0),\n",
       " datetime.datetime(2016, 4, 6, 0, 0),\n",
       " datetime.datetime(2016, 4, 13, 0, 0),\n",
       " datetime.datetime(2016, 4, 20, 0, 0),\n",
       " datetime.datetime(2016, 4, 27, 0, 0),\n",
       " datetime.datetime(2016, 5, 5, 0, 0),\n",
       " datetime.datetime(2016, 5, 12, 0, 0),\n",
       " datetime.datetime(2016, 5, 19, 0, 0),\n",
       " datetime.datetime(2016, 5, 26, 0, 0),\n",
       " datetime.datetime(2016, 6, 2, 0, 0),\n",
       " datetime.datetime(2016, 6, 13, 0, 0),\n",
       " datetime.datetime(2016, 6, 20, 0, 0),\n",
       " datetime.datetime(2016, 6, 27, 0, 0),\n",
       " datetime.datetime(2016, 7, 4, 0, 0),\n",
       " datetime.datetime(2016, 7, 11, 0, 0),\n",
       " datetime.datetime(2016, 7, 18, 0, 0),\n",
       " datetime.datetime(2016, 7, 25, 0, 0),\n",
       " datetime.datetime(2016, 8, 1, 0, 0),\n",
       " datetime.datetime(2016, 8, 8, 0, 0),\n",
       " datetime.datetime(2016, 8, 15, 0, 0),\n",
       " datetime.datetime(2016, 8, 22, 0, 0),\n",
       " datetime.datetime(2016, 8, 29, 0, 0),\n",
       " datetime.datetime(2016, 9, 5, 0, 0),\n",
       " datetime.datetime(2016, 9, 12, 0, 0),\n",
       " datetime.datetime(2016, 9, 21, 0, 0),\n",
       " datetime.datetime(2016, 9, 28, 0, 0),\n",
       " datetime.datetime(2016, 10, 12, 0, 0),\n",
       " datetime.datetime(2016, 10, 19, 0, 0),\n",
       " datetime.datetime(2016, 10, 26, 0, 0),\n",
       " datetime.datetime(2016, 11, 2, 0, 0),\n",
       " datetime.datetime(2016, 11, 9, 0, 0),\n",
       " datetime.datetime(2016, 11, 16, 0, 0),\n",
       " datetime.datetime(2016, 11, 23, 0, 0),\n",
       " datetime.datetime(2016, 11, 30, 0, 0),\n",
       " datetime.datetime(2016, 12, 7, 0, 0),\n",
       " datetime.datetime(2016, 12, 14, 0, 0),\n",
       " datetime.datetime(2016, 12, 21, 0, 0),\n",
       " datetime.datetime(2016, 12, 28, 0, 0),\n",
       " datetime.datetime(2017, 1, 5, 0, 0),\n",
       " datetime.datetime(2017, 1, 12, 0, 0),\n",
       " datetime.datetime(2017, 1, 19, 0, 0),\n",
       " datetime.datetime(2017, 1, 26, 0, 0),\n",
       " datetime.datetime(2017, 2, 9, 0, 0),\n",
       " datetime.datetime(2017, 2, 16, 0, 0),\n",
       " datetime.datetime(2017, 2, 23, 0, 0),\n",
       " datetime.datetime(2017, 3, 2, 0, 0),\n",
       " datetime.datetime(2017, 3, 9, 0, 0),\n",
       " datetime.datetime(2017, 3, 16, 0, 0),\n",
       " datetime.datetime(2017, 3, 23, 0, 0),\n",
       " datetime.datetime(2017, 3, 30, 0, 0),\n",
       " datetime.datetime(2017, 4, 10, 0, 0),\n",
       " datetime.datetime(2017, 4, 17, 0, 0),\n",
       " datetime.datetime(2017, 4, 24, 0, 0),\n",
       " datetime.datetime(2017, 5, 2, 0, 0),\n",
       " datetime.datetime(2017, 5, 9, 0, 0),\n",
       " datetime.datetime(2017, 5, 16, 0, 0),\n",
       " datetime.datetime(2017, 5, 23, 0, 0),\n",
       " datetime.datetime(2017, 6, 1, 0, 0),\n",
       " datetime.datetime(2017, 6, 8, 0, 0),\n",
       " datetime.datetime(2017, 6, 15, 0, 0),\n",
       " datetime.datetime(2017, 6, 22, 0, 0),\n",
       " datetime.datetime(2017, 6, 29, 0, 0),\n",
       " datetime.datetime(2017, 7, 6, 0, 0),\n",
       " datetime.datetime(2017, 7, 13, 0, 0),\n",
       " datetime.datetime(2017, 7, 20, 0, 0),\n",
       " datetime.datetime(2017, 7, 27, 0, 0),\n",
       " datetime.datetime(2017, 8, 3, 0, 0),\n",
       " datetime.datetime(2017, 8, 10, 0, 0),\n",
       " datetime.datetime(2017, 8, 17, 0, 0),\n",
       " datetime.datetime(2017, 8, 24, 0, 0),\n",
       " datetime.datetime(2017, 8, 31, 0, 0),\n",
       " datetime.datetime(2017, 9, 7, 0, 0),\n",
       " datetime.datetime(2017, 9, 14, 0, 0),\n",
       " datetime.datetime(2017, 9, 21, 0, 0),\n",
       " datetime.datetime(2017, 9, 28, 0, 0),\n",
       " datetime.datetime(2017, 10, 12, 0, 0),\n",
       " datetime.datetime(2017, 10, 19, 0, 0),\n",
       " datetime.datetime(2017, 10, 26, 0, 0),\n",
       " datetime.datetime(2017, 11, 2, 0, 0),\n",
       " datetime.datetime(2017, 11, 9, 0, 0),\n",
       " datetime.datetime(2017, 11, 16, 0, 0),\n",
       " datetime.datetime(2017, 11, 23, 0, 0),\n",
       " datetime.datetime(2017, 11, 30, 0, 0),\n",
       " datetime.datetime(2017, 12, 7, 0, 0),\n",
       " datetime.datetime(2017, 12, 14, 0, 0),\n",
       " datetime.datetime(2017, 12, 21, 0, 0),\n",
       " datetime.datetime(2017, 12, 28, 0, 0),\n",
       " datetime.datetime(2018, 1, 5, 0, 0),\n",
       " datetime.datetime(2018, 1, 12, 0, 0),\n",
       " datetime.datetime(2018, 1, 19, 0, 0),\n",
       " datetime.datetime(2018, 1, 26, 0, 0),\n",
       " datetime.datetime(2018, 2, 2, 0, 0),\n",
       " datetime.datetime(2018, 2, 9, 0, 0),\n",
       " datetime.datetime(2018, 2, 23, 0, 0),\n",
       " datetime.datetime(2018, 3, 2, 0, 0),\n",
       " datetime.datetime(2018, 3, 9, 0, 0),\n",
       " datetime.datetime(2018, 3, 16, 0, 0),\n",
       " datetime.datetime(2018, 3, 23, 0, 0),\n",
       " datetime.datetime(2018, 3, 30, 0, 0),\n",
       " datetime.datetime(2018, 4, 10, 0, 0),\n",
       " datetime.datetime(2018, 4, 17, 0, 0),\n",
       " datetime.datetime(2018, 4, 24, 0, 0),\n",
       " datetime.datetime(2018, 5, 3, 0, 0),\n",
       " datetime.datetime(2018, 5, 10, 0, 0),\n",
       " datetime.datetime(2018, 5, 17, 0, 0),\n",
       " datetime.datetime(2018, 5, 24, 0, 0),\n",
       " datetime.datetime(2018, 5, 31, 0, 0),\n",
       " datetime.datetime(2018, 6, 7, 0, 0),\n",
       " datetime.datetime(2018, 6, 14, 0, 0),\n",
       " datetime.datetime(2018, 6, 22, 0, 0),\n",
       " datetime.datetime(2018, 6, 29, 0, 0),\n",
       " datetime.datetime(2018, 7, 6, 0, 0),\n",
       " datetime.datetime(2018, 7, 13, 0, 0),\n",
       " datetime.datetime(2018, 7, 20, 0, 0),\n",
       " datetime.datetime(2018, 7, 27, 0, 0),\n",
       " datetime.datetime(2018, 8, 3, 0, 0),\n",
       " datetime.datetime(2018, 8, 10, 0, 0),\n",
       " datetime.datetime(2018, 8, 17, 0, 0),\n",
       " datetime.datetime(2018, 8, 24, 0, 0),\n",
       " datetime.datetime(2018, 8, 31, 0, 0),\n",
       " datetime.datetime(2018, 9, 7, 0, 0),\n",
       " datetime.datetime(2018, 9, 14, 0, 0),\n",
       " datetime.datetime(2018, 9, 21, 0, 0),\n",
       " datetime.datetime(2018, 10, 8, 0, 0),\n",
       " datetime.datetime(2018, 10, 15, 0, 0),\n",
       " datetime.datetime(2018, 10, 22, 0, 0),\n",
       " datetime.datetime(2018, 10, 29, 0, 0),\n",
       " datetime.datetime(2018, 11, 5, 0, 0),\n",
       " datetime.datetime(2018, 11, 12, 0, 0),\n",
       " datetime.datetime(2018, 11, 19, 0, 0),\n",
       " datetime.datetime(2018, 11, 26, 0, 0),\n",
       " datetime.datetime(2018, 12, 3, 0, 0),\n",
       " datetime.datetime(2018, 12, 10, 0, 0),\n",
       " datetime.datetime(2018, 12, 17, 0, 0),\n",
       " datetime.datetime(2018, 12, 24, 0, 0),\n",
       " datetime.datetime(2019, 1, 2, 0, 0),\n",
       " datetime.datetime(2019, 1, 9, 0, 0),\n",
       " datetime.datetime(2019, 1, 16, 0, 0),\n",
       " datetime.datetime(2019, 1, 23, 0, 0),\n",
       " datetime.datetime(2019, 1, 30, 0, 0),\n",
       " datetime.datetime(2019, 2, 13, 0, 0),\n",
       " datetime.datetime(2019, 2, 20, 0, 0),\n",
       " datetime.datetime(2019, 2, 27, 0, 0),\n",
       " datetime.datetime(2019, 3, 6, 0, 0),\n",
       " datetime.datetime(2019, 3, 13, 0, 0),\n",
       " datetime.datetime(2019, 3, 20, 0, 0),\n",
       " datetime.datetime(2019, 3, 27, 0, 0),\n",
       " datetime.datetime(2019, 4, 3, 0, 0),\n",
       " datetime.datetime(2019, 4, 11, 0, 0),\n",
       " datetime.datetime(2019, 4, 18, 0, 0),\n",
       " datetime.datetime(2019, 4, 25, 0, 0),\n",
       " datetime.datetime(2019, 5, 3, 0, 0),\n",
       " datetime.datetime(2019, 5, 10, 0, 0),\n",
       " datetime.datetime(2019, 5, 17, 0, 0),\n",
       " datetime.datetime(2019, 5, 24, 0, 0),\n",
       " datetime.datetime(2019, 5, 31, 0, 0),\n",
       " datetime.datetime(2019, 6, 10, 0, 0),\n",
       " datetime.datetime(2019, 6, 17, 0, 0),\n",
       " datetime.datetime(2019, 6, 24, 0, 0),\n",
       " datetime.datetime(2019, 7, 1, 0, 0),\n",
       " datetime.datetime(2019, 7, 8, 0, 0),\n",
       " datetime.datetime(2019, 7, 15, 0, 0),\n",
       " datetime.datetime(2019, 7, 22, 0, 0),\n",
       " datetime.datetime(2019, 7, 29, 0, 0),\n",
       " datetime.datetime(2019, 8, 5, 0, 0),\n",
       " datetime.datetime(2019, 8, 12, 0, 0),\n",
       " datetime.datetime(2019, 8, 19, 0, 0),\n",
       " datetime.datetime(2019, 8, 26, 0, 0),\n",
       " datetime.datetime(2019, 9, 2, 0, 0),\n",
       " datetime.datetime(2019, 9, 9, 0, 0),\n",
       " datetime.datetime(2019, 9, 17, 0, 0),\n",
       " datetime.datetime(2019, 9, 24, 0, 0),\n",
       " datetime.datetime(2019, 10, 8, 0, 0),\n",
       " datetime.datetime(2019, 10, 15, 0, 0),\n",
       " datetime.datetime(2019, 10, 22, 0, 0),\n",
       " datetime.datetime(2019, 10, 29, 0, 0),\n",
       " datetime.datetime(2019, 11, 5, 0, 0),\n",
       " datetime.datetime(2019, 11, 12, 0, 0),\n",
       " datetime.datetime(2019, 11, 19, 0, 0),\n",
       " datetime.datetime(2019, 11, 26, 0, 0),\n",
       " datetime.datetime(2019, 12, 3, 0, 0),\n",
       " datetime.datetime(2019, 12, 10, 0, 0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "universe = Universe('zz500')\n",
    "freq = '5b'\n",
    "benchmark_code = 905\n",
    "start_date = '2009-12-01'    # 训练集的起始时间\n",
    "# start_date = '2019-10-17'  # 训练集的起始时间\n",
    "end_date = '2019-12-10'\n",
    "ref_dates = makeSchedule(start_date, end_date, freq, 'china.sse')\n",
    "horizon = map_freq(freq)\n",
    "industry_name = 'sw'\n",
    "industry_level = 1\n",
    "ref_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-03 00:00:00 2019-12-10 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# 前一个调仓日, 用于获取前一个调仓日的持仓信息\n",
    "ref_date_pre = ref_dates[-2]\n",
    "# 当前调仓日\n",
    "ref_date = ref_dates[-1]\n",
    "print(ref_date_pre, ref_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uqer因子列表\n",
    "basic_factor_store = factor_store.basic_factor_store\n",
    "# alpha191因子列表\n",
    "alpha_factor_store = factor_store.alpha_factor_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 32s, sys: 1min 20s, total: 18min 52s\n",
      "Wall time: 21min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 提取Uqer因子\n",
    "basic_factor_org = engine.fetch_factor_range(universe, basic_factor_store, dates=ref_dates)\n",
    "# 提取alpha191因子\n",
    "alpha191_factor_org = engine.fetch_factor_range(universe, \n",
    "                                                alpha_factor_store, \n",
    "                                                dates=ref_dates, \n",
    "                                                used_factor_tables=[Alpha191]).drop(['chgPct','secShortName'], axis=1)\n",
    "# 合并所有的因子\n",
    "factor_data_org = pd.merge(basic_factor_org, alpha191_factor_org, on=['trade_date', 'code'], how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Timestamp('2010-01-06 00:00:00'),\n",
       " Timestamp('2010-01-13 00:00:00'),\n",
       " Timestamp('2010-01-20 00:00:00'),\n",
       " Timestamp('2010-01-27 00:00:00'),\n",
       " Timestamp('2010-02-03 00:00:00'),\n",
       " Timestamp('2010-02-10 00:00:00'),\n",
       " Timestamp('2010-02-24 00:00:00'),\n",
       " Timestamp('2010-03-03 00:00:00'),\n",
       " Timestamp('2010-03-10 00:00:00'),\n",
       " Timestamp('2010-03-17 00:00:00'),\n",
       " Timestamp('2010-03-24 00:00:00'),\n",
       " Timestamp('2010-03-31 00:00:00'),\n",
       " Timestamp('2010-04-08 00:00:00'),\n",
       " Timestamp('2010-04-15 00:00:00'),\n",
       " Timestamp('2010-04-22 00:00:00'),\n",
       " Timestamp('2010-04-29 00:00:00'),\n",
       " Timestamp('2010-05-07 00:00:00'),\n",
       " Timestamp('2010-05-14 00:00:00'),\n",
       " Timestamp('2010-05-21 00:00:00'),\n",
       " Timestamp('2010-05-28 00:00:00'),\n",
       " Timestamp('2010-06-04 00:00:00'),\n",
       " Timestamp('2010-06-11 00:00:00'),\n",
       " Timestamp('2010-06-23 00:00:00'),\n",
       " Timestamp('2010-06-30 00:00:00'),\n",
       " Timestamp('2010-07-07 00:00:00'),\n",
       " Timestamp('2010-07-14 00:00:00'),\n",
       " Timestamp('2010-07-21 00:00:00'),\n",
       " Timestamp('2010-07-28 00:00:00'),\n",
       " Timestamp('2010-08-04 00:00:00'),\n",
       " Timestamp('2010-08-11 00:00:00'),\n",
       " Timestamp('2010-08-18 00:00:00'),\n",
       " Timestamp('2010-08-25 00:00:00'),\n",
       " Timestamp('2010-09-01 00:00:00'),\n",
       " Timestamp('2010-09-08 00:00:00'),\n",
       " Timestamp('2010-09-15 00:00:00'),\n",
       " Timestamp('2010-09-27 00:00:00'),\n",
       " Timestamp('2010-10-11 00:00:00'),\n",
       " Timestamp('2010-10-18 00:00:00'),\n",
       " Timestamp('2010-10-25 00:00:00'),\n",
       " Timestamp('2010-11-01 00:00:00'),\n",
       " Timestamp('2010-11-08 00:00:00'),\n",
       " Timestamp('2010-11-15 00:00:00'),\n",
       " Timestamp('2010-11-22 00:00:00'),\n",
       " Timestamp('2010-11-29 00:00:00'),\n",
       " Timestamp('2010-12-06 00:00:00'),\n",
       " Timestamp('2010-12-13 00:00:00'),\n",
       " Timestamp('2010-12-20 00:00:00'),\n",
       " Timestamp('2010-12-27 00:00:00'),\n",
       " Timestamp('2011-01-04 00:00:00'),\n",
       " Timestamp('2011-01-11 00:00:00'),\n",
       " Timestamp('2011-01-18 00:00:00'),\n",
       " Timestamp('2011-01-25 00:00:00'),\n",
       " Timestamp('2011-02-01 00:00:00'),\n",
       " Timestamp('2011-02-15 00:00:00'),\n",
       " Timestamp('2011-02-22 00:00:00'),\n",
       " Timestamp('2011-03-01 00:00:00'),\n",
       " Timestamp('2011-03-08 00:00:00'),\n",
       " Timestamp('2011-03-15 00:00:00'),\n",
       " Timestamp('2011-03-22 00:00:00'),\n",
       " Timestamp('2011-03-29 00:00:00'),\n",
       " Timestamp('2011-04-07 00:00:00'),\n",
       " Timestamp('2011-04-14 00:00:00'),\n",
       " Timestamp('2011-04-21 00:00:00'),\n",
       " Timestamp('2011-04-28 00:00:00'),\n",
       " Timestamp('2011-05-06 00:00:00'),\n",
       " Timestamp('2011-05-13 00:00:00'),\n",
       " Timestamp('2011-05-20 00:00:00'),\n",
       " Timestamp('2011-05-27 00:00:00'),\n",
       " Timestamp('2011-06-03 00:00:00'),\n",
       " Timestamp('2011-06-13 00:00:00'),\n",
       " Timestamp('2011-06-20 00:00:00'),\n",
       " Timestamp('2011-06-27 00:00:00'),\n",
       " Timestamp('2011-07-04 00:00:00'),\n",
       " Timestamp('2011-07-11 00:00:00'),\n",
       " Timestamp('2011-07-18 00:00:00'),\n",
       " Timestamp('2011-07-25 00:00:00'),\n",
       " Timestamp('2011-08-01 00:00:00'),\n",
       " Timestamp('2011-08-08 00:00:00'),\n",
       " Timestamp('2011-08-15 00:00:00'),\n",
       " Timestamp('2011-08-22 00:00:00'),\n",
       " Timestamp('2011-08-29 00:00:00'),\n",
       " Timestamp('2011-09-05 00:00:00'),\n",
       " Timestamp('2011-09-13 00:00:00'),\n",
       " Timestamp('2011-09-20 00:00:00'),\n",
       " Timestamp('2011-09-27 00:00:00'),\n",
       " Timestamp('2011-10-11 00:00:00'),\n",
       " Timestamp('2011-10-18 00:00:00'),\n",
       " Timestamp('2011-10-25 00:00:00'),\n",
       " Timestamp('2011-11-01 00:00:00'),\n",
       " Timestamp('2011-11-08 00:00:00'),\n",
       " Timestamp('2011-11-15 00:00:00'),\n",
       " Timestamp('2011-11-22 00:00:00'),\n",
       " Timestamp('2011-11-29 00:00:00'),\n",
       " Timestamp('2011-12-06 00:00:00'),\n",
       " Timestamp('2011-12-13 00:00:00'),\n",
       " Timestamp('2011-12-20 00:00:00'),\n",
       " Timestamp('2011-12-27 00:00:00'),\n",
       " Timestamp('2012-01-05 00:00:00'),\n",
       " Timestamp('2012-01-12 00:00:00'),\n",
       " Timestamp('2012-01-19 00:00:00'),\n",
       " Timestamp('2012-02-02 00:00:00'),\n",
       " Timestamp('2012-02-09 00:00:00'),\n",
       " Timestamp('2012-02-16 00:00:00'),\n",
       " Timestamp('2012-02-23 00:00:00'),\n",
       " Timestamp('2012-03-01 00:00:00'),\n",
       " Timestamp('2012-03-08 00:00:00'),\n",
       " Timestamp('2012-03-15 00:00:00'),\n",
       " Timestamp('2012-03-22 00:00:00'),\n",
       " Timestamp('2012-03-29 00:00:00'),\n",
       " Timestamp('2012-04-10 00:00:00'),\n",
       " Timestamp('2012-04-17 00:00:00'),\n",
       " Timestamp('2012-04-24 00:00:00'),\n",
       " Timestamp('2012-05-03 00:00:00'),\n",
       " Timestamp('2012-05-10 00:00:00'),\n",
       " Timestamp('2012-05-17 00:00:00'),\n",
       " Timestamp('2012-05-24 00:00:00'),\n",
       " Timestamp('2012-05-31 00:00:00'),\n",
       " Timestamp('2012-06-07 00:00:00'),\n",
       " Timestamp('2012-06-14 00:00:00'),\n",
       " Timestamp('2012-06-21 00:00:00'),\n",
       " Timestamp('2012-06-29 00:00:00'),\n",
       " Timestamp('2012-07-06 00:00:00'),\n",
       " Timestamp('2012-07-13 00:00:00'),\n",
       " Timestamp('2012-07-20 00:00:00'),\n",
       " Timestamp('2012-07-27 00:00:00'),\n",
       " Timestamp('2012-08-03 00:00:00'),\n",
       " Timestamp('2012-08-10 00:00:00'),\n",
       " Timestamp('2012-08-17 00:00:00'),\n",
       " Timestamp('2012-08-24 00:00:00'),\n",
       " Timestamp('2012-08-31 00:00:00'),\n",
       " Timestamp('2012-09-07 00:00:00'),\n",
       " Timestamp('2012-09-14 00:00:00'),\n",
       " Timestamp('2012-09-21 00:00:00'),\n",
       " Timestamp('2012-09-28 00:00:00'),\n",
       " Timestamp('2012-10-12 00:00:00'),\n",
       " Timestamp('2012-10-19 00:00:00'),\n",
       " Timestamp('2012-10-26 00:00:00'),\n",
       " Timestamp('2012-11-02 00:00:00'),\n",
       " Timestamp('2012-11-09 00:00:00'),\n",
       " Timestamp('2012-11-16 00:00:00'),\n",
       " Timestamp('2012-11-23 00:00:00'),\n",
       " Timestamp('2012-11-30 00:00:00'),\n",
       " Timestamp('2012-12-07 00:00:00'),\n",
       " Timestamp('2012-12-14 00:00:00'),\n",
       " Timestamp('2012-12-21 00:00:00'),\n",
       " Timestamp('2012-12-28 00:00:00'),\n",
       " Timestamp('2013-01-09 00:00:00'),\n",
       " Timestamp('2013-01-16 00:00:00'),\n",
       " Timestamp('2013-01-23 00:00:00'),\n",
       " Timestamp('2013-01-30 00:00:00'),\n",
       " Timestamp('2013-02-06 00:00:00'),\n",
       " Timestamp('2013-02-20 00:00:00'),\n",
       " Timestamp('2013-02-27 00:00:00'),\n",
       " Timestamp('2013-03-06 00:00:00'),\n",
       " Timestamp('2013-03-13 00:00:00'),\n",
       " Timestamp('2013-03-20 00:00:00'),\n",
       " Timestamp('2013-03-27 00:00:00'),\n",
       " Timestamp('2013-04-03 00:00:00'),\n",
       " Timestamp('2013-04-12 00:00:00'),\n",
       " Timestamp('2013-04-19 00:00:00'),\n",
       " Timestamp('2013-04-26 00:00:00'),\n",
       " Timestamp('2013-05-08 00:00:00'),\n",
       " Timestamp('2013-05-15 00:00:00'),\n",
       " Timestamp('2013-05-22 00:00:00'),\n",
       " Timestamp('2013-05-29 00:00:00'),\n",
       " Timestamp('2013-06-05 00:00:00'),\n",
       " Timestamp('2013-06-17 00:00:00'),\n",
       " Timestamp('2013-06-24 00:00:00'),\n",
       " Timestamp('2013-07-01 00:00:00'),\n",
       " Timestamp('2013-07-08 00:00:00'),\n",
       " Timestamp('2013-07-15 00:00:00'),\n",
       " Timestamp('2013-07-22 00:00:00'),\n",
       " Timestamp('2013-07-29 00:00:00'),\n",
       " Timestamp('2013-08-05 00:00:00'),\n",
       " Timestamp('2013-08-12 00:00:00'),\n",
       " Timestamp('2013-08-19 00:00:00'),\n",
       " Timestamp('2013-08-26 00:00:00'),\n",
       " Timestamp('2013-09-02 00:00:00'),\n",
       " Timestamp('2013-09-09 00:00:00'),\n",
       " Timestamp('2013-09-16 00:00:00'),\n",
       " Timestamp('2013-09-25 00:00:00'),\n",
       " Timestamp('2013-10-09 00:00:00'),\n",
       " Timestamp('2013-10-16 00:00:00'),\n",
       " Timestamp('2013-10-23 00:00:00'),\n",
       " Timestamp('2013-10-30 00:00:00'),\n",
       " Timestamp('2013-11-06 00:00:00'),\n",
       " Timestamp('2013-11-13 00:00:00'),\n",
       " Timestamp('2013-11-20 00:00:00'),\n",
       " Timestamp('2013-11-27 00:00:00'),\n",
       " Timestamp('2013-12-04 00:00:00'),\n",
       " Timestamp('2013-12-11 00:00:00'),\n",
       " Timestamp('2013-12-18 00:00:00'),\n",
       " Timestamp('2013-12-25 00:00:00'),\n",
       " Timestamp('2014-01-02 00:00:00'),\n",
       " Timestamp('2014-01-09 00:00:00'),\n",
       " Timestamp('2014-01-16 00:00:00'),\n",
       " Timestamp('2014-01-23 00:00:00'),\n",
       " Timestamp('2014-01-30 00:00:00'),\n",
       " Timestamp('2014-02-13 00:00:00'),\n",
       " Timestamp('2014-02-20 00:00:00'),\n",
       " Timestamp('2014-02-27 00:00:00'),\n",
       " Timestamp('2014-03-06 00:00:00'),\n",
       " Timestamp('2014-03-13 00:00:00'),\n",
       " Timestamp('2014-03-20 00:00:00'),\n",
       " Timestamp('2014-03-27 00:00:00'),\n",
       " Timestamp('2014-04-03 00:00:00'),\n",
       " Timestamp('2014-04-11 00:00:00'),\n",
       " Timestamp('2014-04-18 00:00:00'),\n",
       " Timestamp('2014-04-25 00:00:00'),\n",
       " Timestamp('2014-05-06 00:00:00'),\n",
       " Timestamp('2014-05-13 00:00:00'),\n",
       " Timestamp('2014-05-20 00:00:00'),\n",
       " Timestamp('2014-05-27 00:00:00'),\n",
       " Timestamp('2014-06-04 00:00:00'),\n",
       " Timestamp('2014-06-11 00:00:00'),\n",
       " Timestamp('2014-06-18 00:00:00'),\n",
       " Timestamp('2014-06-25 00:00:00'),\n",
       " Timestamp('2014-07-02 00:00:00'),\n",
       " Timestamp('2014-07-09 00:00:00'),\n",
       " Timestamp('2014-07-16 00:00:00'),\n",
       " Timestamp('2014-07-23 00:00:00'),\n",
       " Timestamp('2014-07-30 00:00:00'),\n",
       " Timestamp('2014-08-06 00:00:00'),\n",
       " Timestamp('2014-08-13 00:00:00'),\n",
       " Timestamp('2014-08-20 00:00:00'),\n",
       " Timestamp('2014-08-27 00:00:00'),\n",
       " Timestamp('2014-09-03 00:00:00'),\n",
       " Timestamp('2014-09-11 00:00:00'),\n",
       " Timestamp('2014-09-18 00:00:00'),\n",
       " Timestamp('2014-09-25 00:00:00'),\n",
       " Timestamp('2014-10-09 00:00:00'),\n",
       " Timestamp('2014-10-16 00:00:00'),\n",
       " Timestamp('2014-10-23 00:00:00'),\n",
       " Timestamp('2014-10-30 00:00:00'),\n",
       " Timestamp('2014-11-06 00:00:00'),\n",
       " Timestamp('2014-11-13 00:00:00'),\n",
       " Timestamp('2014-11-20 00:00:00'),\n",
       " Timestamp('2014-11-27 00:00:00'),\n",
       " Timestamp('2014-12-04 00:00:00'),\n",
       " Timestamp('2014-12-11 00:00:00'),\n",
       " Timestamp('2014-12-18 00:00:00'),\n",
       " Timestamp('2014-12-25 00:00:00'),\n",
       " Timestamp('2015-01-05 00:00:00'),\n",
       " Timestamp('2015-01-12 00:00:00'),\n",
       " Timestamp('2015-01-19 00:00:00'),\n",
       " Timestamp('2015-01-26 00:00:00'),\n",
       " Timestamp('2015-02-02 00:00:00'),\n",
       " Timestamp('2015-02-09 00:00:00'),\n",
       " Timestamp('2015-02-16 00:00:00'),\n",
       " Timestamp('2015-03-02 00:00:00'),\n",
       " Timestamp('2015-03-09 00:00:00'),\n",
       " Timestamp('2015-03-16 00:00:00'),\n",
       " Timestamp('2015-03-23 00:00:00'),\n",
       " Timestamp('2015-03-30 00:00:00'),\n",
       " Timestamp('2015-04-07 00:00:00'),\n",
       " Timestamp('2015-04-14 00:00:00'),\n",
       " Timestamp('2015-04-21 00:00:00'),\n",
       " Timestamp('2015-04-28 00:00:00'),\n",
       " Timestamp('2015-05-06 00:00:00'),\n",
       " Timestamp('2015-05-13 00:00:00'),\n",
       " Timestamp('2015-05-20 00:00:00'),\n",
       " Timestamp('2015-05-27 00:00:00'),\n",
       " Timestamp('2015-06-03 00:00:00'),\n",
       " Timestamp('2015-06-10 00:00:00'),\n",
       " Timestamp('2015-06-17 00:00:00'),\n",
       " Timestamp('2015-06-25 00:00:00'),\n",
       " Timestamp('2015-07-02 00:00:00'),\n",
       " Timestamp('2015-07-09 00:00:00'),\n",
       " Timestamp('2015-07-16 00:00:00'),\n",
       " Timestamp('2015-07-23 00:00:00'),\n",
       " Timestamp('2015-07-30 00:00:00'),\n",
       " Timestamp('2015-08-06 00:00:00'),\n",
       " Timestamp('2015-08-13 00:00:00'),\n",
       " Timestamp('2015-08-20 00:00:00'),\n",
       " Timestamp('2015-08-27 00:00:00'),\n",
       " Timestamp('2015-09-07 00:00:00'),\n",
       " Timestamp('2015-09-14 00:00:00'),\n",
       " Timestamp('2015-09-21 00:00:00'),\n",
       " Timestamp('2015-09-28 00:00:00'),\n",
       " Timestamp('2015-10-12 00:00:00'),\n",
       " Timestamp('2015-10-19 00:00:00'),\n",
       " Timestamp('2015-10-26 00:00:00'),\n",
       " Timestamp('2015-11-02 00:00:00'),\n",
       " Timestamp('2015-11-09 00:00:00'),\n",
       " Timestamp('2015-11-16 00:00:00'),\n",
       " Timestamp('2015-11-23 00:00:00'),\n",
       " Timestamp('2015-11-30 00:00:00'),\n",
       " Timestamp('2015-12-07 00:00:00'),\n",
       " Timestamp('2015-12-14 00:00:00'),\n",
       " Timestamp('2015-12-21 00:00:00'),\n",
       " Timestamp('2015-12-28 00:00:00'),\n",
       " Timestamp('2016-01-05 00:00:00'),\n",
       " Timestamp('2016-01-12 00:00:00'),\n",
       " Timestamp('2016-01-19 00:00:00'),\n",
       " Timestamp('2016-01-26 00:00:00'),\n",
       " Timestamp('2016-02-02 00:00:00'),\n",
       " Timestamp('2016-02-16 00:00:00'),\n",
       " Timestamp('2016-02-23 00:00:00'),\n",
       " Timestamp('2016-03-01 00:00:00'),\n",
       " Timestamp('2016-03-08 00:00:00'),\n",
       " Timestamp('2016-03-15 00:00:00'),\n",
       " Timestamp('2016-03-22 00:00:00'),\n",
       " Timestamp('2016-03-29 00:00:00'),\n",
       " Timestamp('2016-04-06 00:00:00'),\n",
       " Timestamp('2016-04-13 00:00:00'),\n",
       " Timestamp('2016-04-20 00:00:00'),\n",
       " Timestamp('2016-04-27 00:00:00'),\n",
       " Timestamp('2016-05-05 00:00:00'),\n",
       " Timestamp('2016-05-12 00:00:00'),\n",
       " Timestamp('2016-05-19 00:00:00'),\n",
       " Timestamp('2016-05-26 00:00:00'),\n",
       " Timestamp('2016-06-02 00:00:00'),\n",
       " Timestamp('2016-06-13 00:00:00'),\n",
       " Timestamp('2016-06-20 00:00:00'),\n",
       " Timestamp('2016-06-27 00:00:00'),\n",
       " Timestamp('2016-07-04 00:00:00'),\n",
       " Timestamp('2016-07-11 00:00:00'),\n",
       " Timestamp('2016-07-18 00:00:00'),\n",
       " Timestamp('2016-07-25 00:00:00'),\n",
       " Timestamp('2016-08-01 00:00:00'),\n",
       " Timestamp('2016-08-08 00:00:00'),\n",
       " Timestamp('2016-08-15 00:00:00'),\n",
       " Timestamp('2016-08-22 00:00:00'),\n",
       " Timestamp('2016-08-29 00:00:00'),\n",
       " Timestamp('2016-09-05 00:00:00'),\n",
       " Timestamp('2016-09-12 00:00:00'),\n",
       " Timestamp('2016-09-21 00:00:00'),\n",
       " Timestamp('2016-09-28 00:00:00'),\n",
       " Timestamp('2016-10-12 00:00:00'),\n",
       " Timestamp('2016-10-19 00:00:00'),\n",
       " Timestamp('2016-10-26 00:00:00'),\n",
       " Timestamp('2016-11-02 00:00:00'),\n",
       " Timestamp('2016-11-09 00:00:00'),\n",
       " Timestamp('2016-11-16 00:00:00'),\n",
       " Timestamp('2016-11-23 00:00:00'),\n",
       " Timestamp('2016-11-30 00:00:00'),\n",
       " Timestamp('2016-12-07 00:00:00'),\n",
       " Timestamp('2016-12-14 00:00:00'),\n",
       " Timestamp('2016-12-21 00:00:00'),\n",
       " Timestamp('2016-12-28 00:00:00'),\n",
       " Timestamp('2017-01-05 00:00:00'),\n",
       " Timestamp('2017-01-12 00:00:00'),\n",
       " Timestamp('2017-01-19 00:00:00'),\n",
       " Timestamp('2017-01-26 00:00:00'),\n",
       " Timestamp('2017-02-09 00:00:00'),\n",
       " Timestamp('2017-02-16 00:00:00'),\n",
       " Timestamp('2017-02-23 00:00:00'),\n",
       " Timestamp('2017-03-02 00:00:00'),\n",
       " Timestamp('2017-03-09 00:00:00'),\n",
       " Timestamp('2017-03-16 00:00:00'),\n",
       " Timestamp('2017-03-23 00:00:00'),\n",
       " Timestamp('2017-03-30 00:00:00'),\n",
       " Timestamp('2017-04-10 00:00:00'),\n",
       " Timestamp('2017-04-17 00:00:00'),\n",
       " Timestamp('2017-04-24 00:00:00'),\n",
       " Timestamp('2017-05-02 00:00:00'),\n",
       " Timestamp('2017-05-09 00:00:00'),\n",
       " Timestamp('2017-05-16 00:00:00'),\n",
       " Timestamp('2017-05-23 00:00:00'),\n",
       " Timestamp('2017-06-01 00:00:00'),\n",
       " Timestamp('2017-06-08 00:00:00'),\n",
       " Timestamp('2017-06-15 00:00:00'),\n",
       " Timestamp('2017-06-22 00:00:00'),\n",
       " Timestamp('2017-06-29 00:00:00'),\n",
       " Timestamp('2017-07-06 00:00:00'),\n",
       " Timestamp('2017-07-13 00:00:00'),\n",
       " Timestamp('2017-07-20 00:00:00'),\n",
       " Timestamp('2017-07-27 00:00:00'),\n",
       " Timestamp('2017-08-03 00:00:00'),\n",
       " Timestamp('2017-08-10 00:00:00'),\n",
       " Timestamp('2017-08-17 00:00:00'),\n",
       " Timestamp('2017-08-24 00:00:00'),\n",
       " Timestamp('2017-08-31 00:00:00'),\n",
       " Timestamp('2017-09-07 00:00:00'),\n",
       " Timestamp('2017-09-14 00:00:00'),\n",
       " Timestamp('2017-09-21 00:00:00'),\n",
       " Timestamp('2017-09-28 00:00:00'),\n",
       " Timestamp('2017-10-12 00:00:00'),\n",
       " Timestamp('2017-10-19 00:00:00'),\n",
       " Timestamp('2017-10-26 00:00:00'),\n",
       " Timestamp('2017-11-02 00:00:00'),\n",
       " Timestamp('2017-11-09 00:00:00'),\n",
       " Timestamp('2017-11-16 00:00:00'),\n",
       " Timestamp('2017-11-23 00:00:00'),\n",
       " Timestamp('2017-11-30 00:00:00'),\n",
       " Timestamp('2017-12-07 00:00:00'),\n",
       " Timestamp('2017-12-14 00:00:00'),\n",
       " Timestamp('2017-12-21 00:00:00'),\n",
       " Timestamp('2017-12-28 00:00:00'),\n",
       " Timestamp('2018-01-05 00:00:00'),\n",
       " Timestamp('2018-01-12 00:00:00'),\n",
       " Timestamp('2018-01-19 00:00:00'),\n",
       " Timestamp('2018-01-26 00:00:00'),\n",
       " Timestamp('2018-02-02 00:00:00'),\n",
       " Timestamp('2018-02-09 00:00:00'),\n",
       " Timestamp('2018-02-23 00:00:00'),\n",
       " Timestamp('2018-03-02 00:00:00'),\n",
       " Timestamp('2018-03-09 00:00:00'),\n",
       " Timestamp('2018-03-16 00:00:00'),\n",
       " Timestamp('2018-03-23 00:00:00'),\n",
       " Timestamp('2018-03-30 00:00:00'),\n",
       " Timestamp('2018-04-10 00:00:00'),\n",
       " Timestamp('2018-04-17 00:00:00'),\n",
       " Timestamp('2018-04-24 00:00:00'),\n",
       " Timestamp('2018-05-03 00:00:00'),\n",
       " Timestamp('2018-05-10 00:00:00'),\n",
       " Timestamp('2018-05-17 00:00:00'),\n",
       " Timestamp('2018-05-24 00:00:00'),\n",
       " Timestamp('2018-05-31 00:00:00'),\n",
       " Timestamp('2018-06-07 00:00:00'),\n",
       " Timestamp('2018-06-14 00:00:00'),\n",
       " Timestamp('2018-06-22 00:00:00'),\n",
       " Timestamp('2018-06-29 00:00:00'),\n",
       " Timestamp('2018-07-06 00:00:00'),\n",
       " Timestamp('2018-07-13 00:00:00'),\n",
       " Timestamp('2018-07-20 00:00:00'),\n",
       " Timestamp('2018-07-27 00:00:00'),\n",
       " Timestamp('2018-08-03 00:00:00'),\n",
       " Timestamp('2018-08-10 00:00:00'),\n",
       " Timestamp('2018-08-17 00:00:00'),\n",
       " Timestamp('2018-08-24 00:00:00'),\n",
       " Timestamp('2018-08-31 00:00:00'),\n",
       " Timestamp('2018-09-07 00:00:00'),\n",
       " Timestamp('2018-09-14 00:00:00'),\n",
       " Timestamp('2018-09-21 00:00:00'),\n",
       " Timestamp('2018-10-08 00:00:00'),\n",
       " Timestamp('2018-10-15 00:00:00'),\n",
       " Timestamp('2018-10-22 00:00:00'),\n",
       " Timestamp('2018-10-29 00:00:00'),\n",
       " Timestamp('2018-11-05 00:00:00'),\n",
       " Timestamp('2018-11-12 00:00:00'),\n",
       " Timestamp('2018-11-19 00:00:00'),\n",
       " Timestamp('2018-11-26 00:00:00'),\n",
       " Timestamp('2018-12-03 00:00:00'),\n",
       " Timestamp('2018-12-10 00:00:00'),\n",
       " Timestamp('2018-12-17 00:00:00'),\n",
       " Timestamp('2018-12-24 00:00:00'),\n",
       " Timestamp('2019-01-02 00:00:00'),\n",
       " Timestamp('2019-01-09 00:00:00'),\n",
       " Timestamp('2019-01-16 00:00:00'),\n",
       " Timestamp('2019-01-23 00:00:00'),\n",
       " Timestamp('2019-01-30 00:00:00'),\n",
       " Timestamp('2019-02-13 00:00:00'),\n",
       " Timestamp('2019-02-20 00:00:00'),\n",
       " Timestamp('2019-02-27 00:00:00'),\n",
       " Timestamp('2019-03-06 00:00:00'),\n",
       " Timestamp('2019-03-13 00:00:00'),\n",
       " Timestamp('2019-03-20 00:00:00'),\n",
       " Timestamp('2019-03-27 00:00:00'),\n",
       " Timestamp('2019-04-03 00:00:00'),\n",
       " Timestamp('2019-04-11 00:00:00'),\n",
       " Timestamp('2019-04-18 00:00:00'),\n",
       " Timestamp('2019-04-25 00:00:00'),\n",
       " Timestamp('2019-05-10 00:00:00'),\n",
       " Timestamp('2019-05-17 00:00:00'),\n",
       " Timestamp('2019-05-24 00:00:00'),\n",
       " Timestamp('2019-05-31 00:00:00'),\n",
       " Timestamp('2019-06-10 00:00:00'),\n",
       " Timestamp('2019-06-17 00:00:00'),\n",
       " Timestamp('2019-06-24 00:00:00'),\n",
       " Timestamp('2019-07-01 00:00:00'),\n",
       " Timestamp('2019-07-08 00:00:00'),\n",
       " Timestamp('2019-07-15 00:00:00'),\n",
       " Timestamp('2019-07-22 00:00:00'),\n",
       " Timestamp('2019-07-29 00:00:00'),\n",
       " Timestamp('2019-08-05 00:00:00'),\n",
       " Timestamp('2019-08-12 00:00:00'),\n",
       " Timestamp('2019-08-26 00:00:00'),\n",
       " Timestamp('2019-09-02 00:00:00'),\n",
       " Timestamp('2019-09-09 00:00:00'),\n",
       " Timestamp('2019-09-17 00:00:00'),\n",
       " Timestamp('2019-09-24 00:00:00'),\n",
       " Timestamp('2019-10-08 00:00:00'),\n",
       " Timestamp('2019-10-15 00:00:00'),\n",
       " Timestamp('2019-10-22 00:00:00'),\n",
       " Timestamp('2019-10-29 00:00:00'),\n",
       " Timestamp('2019-11-05 00:00:00'),\n",
       " Timestamp('2019-11-12 00:00:00'),\n",
       " Timestamp('2019-11-19 00:00:00'),\n",
       " Timestamp('2019-11-26 00:00:00'),\n",
       " Timestamp('2019-12-03 00:00:00'),\n",
       " Timestamp('2019-12-10 00:00:00')}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set(factor_data_org['trade_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACCA</th>\n",
       "      <th>ACD20</th>\n",
       "      <th>ACD6</th>\n",
       "      <th>AD</th>\n",
       "      <th>AD20</th>\n",
       "      <th>AD6</th>\n",
       "      <th>ADTM</th>\n",
       "      <th>ADX</th>\n",
       "      <th>ADXR</th>\n",
       "      <th>APBMA</th>\n",
       "      <th>AR</th>\n",
       "      <th>ARBR</th>\n",
       "      <th>ARC</th>\n",
       "      <th>ARTDays</th>\n",
       "      <th>ARTRate</th>\n",
       "      <th>ASI</th>\n",
       "      <th>ASSI</th>\n",
       "      <th>ATR14</th>\n",
       "      <th>ATR6</th>\n",
       "      <th>AccountsPayablesTDays</th>\n",
       "      <th>AccountsPayablesTRate</th>\n",
       "      <th>AdminExpenseTTM</th>\n",
       "      <th>AdminiExpenseRate</th>\n",
       "      <th>Alpha120</th>\n",
       "      <th>Alpha20</th>\n",
       "      <th>Alpha60</th>\n",
       "      <th>Aroon</th>\n",
       "      <th>AroonDown</th>\n",
       "      <th>AroonUp</th>\n",
       "      <th>AssetImpairLossTTM</th>\n",
       "      <th>BBI</th>\n",
       "      <th>BBIC</th>\n",
       "      <th>BIAS10</th>\n",
       "      <th>BIAS20</th>\n",
       "      <th>BIAS5</th>\n",
       "      <th>BIAS60</th>\n",
       "      <th>BLEV</th>\n",
       "      <th>BR</th>\n",
       "      <th>BackwardADJ</th>\n",
       "      <th>BearPower</th>\n",
       "      <th>Beta120</th>\n",
       "      <th>Beta20</th>\n",
       "      <th>Beta252</th>\n",
       "      <th>Beta60</th>\n",
       "      <th>BollDown</th>\n",
       "      <th>BollUp</th>\n",
       "      <th>BondsPayableToAsset</th>\n",
       "      <th>BullPower</th>\n",
       "      <th>CCI10</th>\n",
       "      <th>CCI20</th>\n",
       "      <th>CCI5</th>\n",
       "      <th>CCI88</th>\n",
       "      <th>CETOP</th>\n",
       "      <th>CFO2EV</th>\n",
       "      <th>CMO</th>\n",
       "      <th>CMRA</th>\n",
       "      <th>CR20</th>\n",
       "      <th>CTOP</th>\n",
       "      <th>CTP5</th>\n",
       "      <th>CapitalSurplusFundPS</th>\n",
       "      <th>CashConversionCycle</th>\n",
       "      <th>CashDividendCover</th>\n",
       "      <th>CashEquivalentPS</th>\n",
       "      <th>CashFlowPS</th>\n",
       "      <th>CashRateOfSales</th>\n",
       "      <th>CashRateOfSalesLatest</th>\n",
       "      <th>CashToCurrentLiability</th>\n",
       "      <th>ChaikinOscillator</th>\n",
       "      <th>ChaikinVolatility</th>\n",
       "      <th>ChandeSD</th>\n",
       "      <th>ChandeSU</th>\n",
       "      <th>CmraCNE5</th>\n",
       "      <th>CoppockCurve</th>\n",
       "      <th>CostTTM</th>\n",
       "      <th>CurrentAssetsRatio</th>\n",
       "      <th>CurrentAssetsTRate</th>\n",
       "      <th>CurrentRatio</th>\n",
       "      <th>DA</th>\n",
       "      <th>DAREC</th>\n",
       "      <th>DAREV</th>\n",
       "      <th>DASREV</th>\n",
       "      <th>DASTD</th>\n",
       "      <th>DAVOL10</th>\n",
       "      <th>DAVOL20</th>\n",
       "      <th>DAVOL5</th>\n",
       "      <th>DBCD</th>\n",
       "      <th>DDI</th>\n",
       "      <th>DDNBT</th>\n",
       "      <th>DDNCR</th>\n",
       "      <th>DDNSR</th>\n",
       "      <th>DEA</th>\n",
       "      <th>DEGM</th>\n",
       "      <th>DHILO</th>\n",
       "      <th>DIF</th>\n",
       "      <th>DIFF</th>\n",
       "      <th>DIZ</th>\n",
       "      <th>DVRAT</th>\n",
       "      <th>DebtEquityRatio</th>\n",
       "      <th>DebtTangibleEquityRatio</th>\n",
       "      <th>DebtsAssetRatio</th>\n",
       "      <th>DilutedEPS</th>\n",
       "      <th>DividendCover</th>\n",
       "      <th>DividendPS</th>\n",
       "      <th>DividendPaidRatio</th>\n",
       "      <th>DownRVI</th>\n",
       "      <th>EARNMOM</th>\n",
       "      <th>EBIAT</th>\n",
       "      <th>EBIT</th>\n",
       "      <th>EBITDA</th>\n",
       "      <th>EBITToTOR</th>\n",
       "      <th>EGRO</th>\n",
       "      <th>EMA10</th>\n",
       "      <th>EMA12</th>\n",
       "      <th>EMA120</th>\n",
       "      <th>EMA20</th>\n",
       "      <th>EMA26</th>\n",
       "      <th>EMA5</th>\n",
       "      <th>EMA60</th>\n",
       "      <th>EMV14</th>\n",
       "      <th>EMV6</th>\n",
       "      <th>EPIBS</th>\n",
       "      <th>EPS</th>\n",
       "      <th>EPSTTM</th>\n",
       "      <th>ETOP</th>\n",
       "      <th>ETP5</th>\n",
       "      <th>EgibsLong</th>\n",
       "      <th>Elder</th>\n",
       "      <th>EnterpriseFCFPS</th>\n",
       "      <th>EquityFixedAssetRatio</th>\n",
       "      <th>EquityTRate</th>\n",
       "      <th>EquityToAsset</th>\n",
       "      <th>FCFE</th>\n",
       "      <th>FCFF</th>\n",
       "      <th>FEARNG</th>\n",
       "      <th>FSALESG</th>\n",
       "      <th>FY12P</th>\n",
       "      <th>FiftyTwoWeekHigh</th>\n",
       "      <th>FinanExpenseTTM</th>\n",
       "      <th>FinancialExpenseRate</th>\n",
       "      <th>FinancingCashGrowRate</th>\n",
       "      <th>FixAssetRatio</th>\n",
       "      <th>FixedAssetsTRate</th>\n",
       "      <th>ForwardPE</th>\n",
       "      <th>GREC</th>\n",
       "      <th>GREV</th>\n",
       "      <th>GSREV</th>\n",
       "      <th>GainLossVarianceRatio120</th>\n",
       "      <th>GainLossVarianceRatio20</th>\n",
       "      <th>GainLossVarianceRatio60</th>\n",
       "      <th>GainVariance120</th>\n",
       "      <th>GainVariance20</th>\n",
       "      <th>GainVariance60</th>\n",
       "      <th>GrossIncomeRatio</th>\n",
       "      <th>GrossProfit</th>\n",
       "      <th>GrossProfitTTM</th>\n",
       "      <th>HBETA</th>\n",
       "      <th>HSIGMA</th>\n",
       "      <th>HsigmaCNE5</th>\n",
       "      <th>Hurst</th>\n",
       "      <th>ILLIQUIDITY</th>\n",
       "      <th>InformationRatio120</th>\n",
       "      <th>InformationRatio20</th>\n",
       "      <th>InformationRatio60</th>\n",
       "      <th>IntCL</th>\n",
       "      <th>IntDebt</th>\n",
       "      <th>IntFreeCL</th>\n",
       "      <th>IntFreeNCL</th>\n",
       "      <th>IntangibleAssetRatio</th>\n",
       "      <th>InteBearDebtToTotalCapital</th>\n",
       "      <th>InterestCover</th>\n",
       "      <th>InventoryTDays</th>\n",
       "      <th>InventoryTRate</th>\n",
       "      <th>InvestCashGrowRate</th>\n",
       "      <th>InvestRAssociatesToTP</th>\n",
       "      <th>InvestRAssociatesToTPLatest</th>\n",
       "      <th>JDQS20</th>\n",
       "      <th>KDJ_D</th>\n",
       "      <th>KDJ_J</th>\n",
       "      <th>KDJ_K</th>\n",
       "      <th>KlingerOscillator</th>\n",
       "      <th>Kurtosis120</th>\n",
       "      <th>Kurtosis20</th>\n",
       "      <th>Kurtosis60</th>\n",
       "      <th>LCAP</th>\n",
       "      <th>LFLO</th>\n",
       "      <th>LongDebtToAsset</th>\n",
       "      <th>LongDebtToWorkingCapital</th>\n",
       "      <th>LongTermDebtToAsset</th>\n",
       "      <th>LossVariance120</th>\n",
       "      <th>LossVariance20</th>\n",
       "      <th>LossVariance60</th>\n",
       "      <th>MA10</th>\n",
       "      <th>MA10Close</th>\n",
       "      <th>MA10RegressCoeff12</th>\n",
       "      <th>MA10RegressCoeff6</th>\n",
       "      <th>MA120</th>\n",
       "      <th>MA20</th>\n",
       "      <th>MA5</th>\n",
       "      <th>MA60</th>\n",
       "      <th>MACD</th>\n",
       "      <th>MAWVAD</th>\n",
       "      <th>MFI</th>\n",
       "      <th>MLEV</th>\n",
       "      <th>MTM</th>\n",
       "      <th>MTMMA</th>\n",
       "      <th>MassIndex</th>\n",
       "      <th>MktValue</th>\n",
       "      <th>MoneyFlow20</th>\n",
       "      <th>NIAP</th>\n",
       "      <th>NIAPCut</th>\n",
       "      <th>NLSIZE</th>\n",
       "      <th>NOCFToInterestBearDebt</th>\n",
       "      <th>NOCFToNetDebt</th>\n",
       "      <th>NOCFToOperatingNI</th>\n",
       "      <th>NOCFToOperatingNILatest</th>\n",
       "      <th>NOCFToTLiability</th>\n",
       "      <th>NPCutToNP</th>\n",
       "      <th>NPFromOperatingTTM</th>\n",
       "      <th>NPFromValueChgTTM</th>\n",
       "      <th>NPParentCompanyCutYOY</th>\n",
       "      <th>NPParentCompanyGrowRate</th>\n",
       "      <th>NPToTOR</th>\n",
       "      <th>NRProfitLoss</th>\n",
       "      <th>NVI</th>\n",
       "      <th>NegMktValue</th>\n",
       "      <th>NetAssetGrowRate</th>\n",
       "      <th>NetAssetPS</th>\n",
       "      <th>NetCashFlowGrowRate</th>\n",
       "      <th>NetDebt</th>\n",
       "      <th>NetFinanceCFTTM</th>\n",
       "      <th>NetIntExpense</th>\n",
       "      <th>NetInvestCFTTM</th>\n",
       "      <th>NetNonOIToTP</th>\n",
       "      <th>NetNonOIToTPLatest</th>\n",
       "      <th>NetOperateCFTTM</th>\n",
       "      <th>NetProfitAPTTM</th>\n",
       "      <th>NetProfitCashCover</th>\n",
       "      <th>NetProfitGrowRate</th>\n",
       "      <th>NetProfitGrowRate3Y</th>\n",
       "      <th>NetProfitGrowRate5Y</th>\n",
       "      <th>NetProfitRatio</th>\n",
       "      <th>NetProfitTTM</th>\n",
       "      <th>NetTangibleAssets</th>\n",
       "      <th>NetWorkingCapital</th>\n",
       "      <th>NonCurrentAssetsRatio</th>\n",
       "      <th>NonOperatingNPTTM</th>\n",
       "      <th>OBV</th>\n",
       "      <th>OBV20</th>\n",
       "      <th>OBV6</th>\n",
       "      <th>OperCashFlowPS</th>\n",
       "      <th>OperCashGrowRate</th>\n",
       "      <th>OperCashInToAsset</th>\n",
       "      <th>OperCashInToCurrentLiability</th>\n",
       "      <th>OperateNetIncome</th>\n",
       "      <th>OperateProfitTTM</th>\n",
       "      <th>OperatingCycle</th>\n",
       "      <th>OperatingExpenseRate</th>\n",
       "      <th>OperatingNIToTP</th>\n",
       "      <th>OperatingNIToTPLatest</th>\n",
       "      <th>OperatingProfitGrowRate</th>\n",
       "      <th>OperatingProfitPS</th>\n",
       "      <th>OperatingProfitPSLatest</th>\n",
       "      <th>OperatingProfitRatio</th>\n",
       "      <th>OperatingProfitToTOR</th>\n",
       "      <th>OperatingRevenueGrowRate</th>\n",
       "      <th>OperatingRevenueGrowRate3Y</th>\n",
       "      <th>OperatingRevenueGrowRate5Y</th>\n",
       "      <th>OperatingRevenuePS</th>\n",
       "      <th>OperatingRevenuePSLatest</th>\n",
       "      <th>PB</th>\n",
       "      <th>PBIndu</th>\n",
       "      <th>PCF</th>\n",
       "      <th>PCFIndu</th>\n",
       "      <th>PE</th>\n",
       "      <th>PEG3Y</th>\n",
       "      <th>PEG5Y</th>\n",
       "      <th>PEHist120</th>\n",
       "      <th>PEHist20</th>\n",
       "      <th>PEHist250</th>\n",
       "      <th>PEHist60</th>\n",
       "      <th>PEIndu</th>\n",
       "      <th>PLRC12</th>\n",
       "      <th>PLRC6</th>\n",
       "      <th>PS</th>\n",
       "      <th>PSIndu</th>\n",
       "      <th>PSY</th>\n",
       "      <th>PVI</th>\n",
       "      <th>PVT</th>\n",
       "      <th>PVT12</th>\n",
       "      <th>PVT6</th>\n",
       "      <th>PeriodCostsRate</th>\n",
       "      <th>Price1M</th>\n",
       "      <th>Price1Y</th>\n",
       "      <th>Price3M</th>\n",
       "      <th>QuickRatio</th>\n",
       "      <th>RC12</th>\n",
       "      <th>RC24</th>\n",
       "      <th>REC</th>\n",
       "      <th>REVS10</th>\n",
       "      <th>REVS120</th>\n",
       "      <th>REVS20</th>\n",
       "      <th>REVS20Indu1</th>\n",
       "      <th>REVS250</th>\n",
       "      <th>REVS5</th>\n",
       "      <th>REVS5Indu1</th>\n",
       "      <th>REVS5m20</th>\n",
       "      <th>REVS5m60</th>\n",
       "      <th>REVS60</th>\n",
       "      <th>REVS750</th>\n",
       "      <th>ROA</th>\n",
       "      <th>ROA5</th>\n",
       "      <th>ROAEBIT</th>\n",
       "      <th>ROAEBITTTM</th>\n",
       "      <th>ROC20</th>\n",
       "      <th>ROC6</th>\n",
       "      <th>ROE</th>\n",
       "      <th>ROE5</th>\n",
       "      <th>ROEAvg</th>\n",
       "      <th>ROECut</th>\n",
       "      <th>ROECutWeighted</th>\n",
       "      <th>ROEDiluted</th>\n",
       "      <th>ROEWeighted</th>\n",
       "      <th>ROIC</th>\n",
       "      <th>RSI</th>\n",
       "      <th>RSTR12</th>\n",
       "      <th>RSTR24</th>\n",
       "      <th>RSTR504</th>\n",
       "      <th>RVI</th>\n",
       "      <th>Rank1M</th>\n",
       "      <th>RealizedVolatility</th>\n",
       "      <th>RetainedEarningRatio</th>\n",
       "      <th>RetainedEarnings</th>\n",
       "      <th>RetainedEarningsPS</th>\n",
       "      <th>RevenueTTM</th>\n",
       "      <th>SBM</th>\n",
       "      <th>SFY12P</th>\n",
       "      <th>SGRO</th>\n",
       "      <th>SRMI</th>\n",
       "      <th>STM</th>\n",
       "      <th>STOA</th>\n",
       "      <th>STOM</th>\n",
       "      <th>STOQ</th>\n",
       "      <th>SUE</th>\n",
       "      <th>SUOI</th>\n",
       "      <th>SaleServiceCashToOR</th>\n",
       "      <th>SaleServiceRenderCashTTM</th>\n",
       "      <th>SalesCostRatio</th>\n",
       "      <th>SalesExpenseTTM</th>\n",
       "      <th>SalesServiceCashToORLatest</th>\n",
       "      <th>ShareholderFCFPS</th>\n",
       "      <th>SharpeRatio120</th>\n",
       "      <th>SharpeRatio20</th>\n",
       "      <th>SharpeRatio60</th>\n",
       "      <th>Skewness</th>\n",
       "      <th>StaticPE</th>\n",
       "      <th>SuperQuickRatio</th>\n",
       "      <th>SurplusReserveFundPS</th>\n",
       "      <th>SwingIndex</th>\n",
       "      <th>TA2EV</th>\n",
       "      <th>TCostTTM</th>\n",
       "      <th>TEAP</th>\n",
       "      <th>TEMA10</th>\n",
       "      <th>TEMA5</th>\n",
       "      <th>TOBT</th>\n",
       "      <th>TORPS</th>\n",
       "      <th>TORPSLatest</th>\n",
       "      <th>TProfitTTM</th>\n",
       "      <th>TRIX10</th>\n",
       "      <th>TRIX5</th>\n",
       "      <th>TRevenueTTM</th>\n",
       "      <th>TSEPToInterestBearDebt</th>\n",
       "      <th>TSEPToTotalCapital</th>\n",
       "      <th>TVMA20</th>\n",
       "      <th>TVMA6</th>\n",
       "      <th>TVSTD20</th>\n",
       "      <th>TVSTD6</th>\n",
       "      <th>TangibleAToInteBearDebt</th>\n",
       "      <th>TangibleAToNetDebt</th>\n",
       "      <th>TaxRatio</th>\n",
       "      <th>TotalAssetGrowRate</th>\n",
       "      <th>TotalAssets</th>\n",
       "      <th>TotalAssetsTRate</th>\n",
       "      <th>TotalFixedAssets</th>\n",
       "      <th>TotalPaidinCapital</th>\n",
       "      <th>TotalProfitCostRatio</th>\n",
       "      <th>TotalProfitGrowRate</th>\n",
       "      <th>TreynorRatio120</th>\n",
       "      <th>TreynorRatio20</th>\n",
       "      <th>TreynorRatio60</th>\n",
       "      <th>UOS</th>\n",
       "      <th>Ulcer10</th>\n",
       "      <th>Ulcer5</th>\n",
       "      <th>UndividedProfitPS</th>\n",
       "      <th>UpRVI</th>\n",
       "      <th>VDEA</th>\n",
       "      <th>VDIFF</th>\n",
       "      <th>VEMA10</th>\n",
       "      <th>VEMA12</th>\n",
       "      <th>VEMA26</th>\n",
       "      <th>VEMA5</th>\n",
       "      <th>VMACD</th>\n",
       "      <th>VOL10</th>\n",
       "      <th>VOL120</th>\n",
       "      <th>VOL20</th>\n",
       "      <th>VOL240</th>\n",
       "      <th>VOL5</th>\n",
       "      <th>VOL60</th>\n",
       "      <th>VR</th>\n",
       "      <th>VROC12</th>\n",
       "      <th>VROC6</th>\n",
       "      <th>VSTD10</th>\n",
       "      <th>VSTD20</th>\n",
       "      <th>ValueChgProfit</th>\n",
       "      <th>Variance120</th>\n",
       "      <th>Variance20</th>\n",
       "      <th>Variance60</th>\n",
       "      <th>Volatility</th>\n",
       "      <th>Volumn1M</th>\n",
       "      <th>Volumn3M</th>\n",
       "      <th>WVAD</th>\n",
       "      <th>WorkingCapital</th>\n",
       "      <th>minusDI</th>\n",
       "      <th>plusDI</th>\n",
       "      <th>code</th>\n",
       "      <th>chgPct</th>\n",
       "      <th>alpha_1</th>\n",
       "      <th>alpha_10</th>\n",
       "      <th>alpha_100</th>\n",
       "      <th>alpha_101</th>\n",
       "      <th>alpha_102</th>\n",
       "      <th>alpha_103</th>\n",
       "      <th>alpha_104</th>\n",
       "      <th>alpha_105</th>\n",
       "      <th>alpha_106</th>\n",
       "      <th>alpha_107</th>\n",
       "      <th>alpha_108</th>\n",
       "      <th>alpha_109</th>\n",
       "      <th>alpha_11</th>\n",
       "      <th>alpha_110</th>\n",
       "      <th>alpha_111</th>\n",
       "      <th>alpha_112</th>\n",
       "      <th>alpha_113</th>\n",
       "      <th>alpha_114</th>\n",
       "      <th>alpha_115</th>\n",
       "      <th>alpha_116</th>\n",
       "      <th>alpha_117</th>\n",
       "      <th>alpha_118</th>\n",
       "      <th>alpha_119</th>\n",
       "      <th>alpha_12</th>\n",
       "      <th>alpha_120</th>\n",
       "      <th>alpha_121</th>\n",
       "      <th>alpha_122</th>\n",
       "      <th>alpha_123</th>\n",
       "      <th>alpha_124</th>\n",
       "      <th>alpha_125</th>\n",
       "      <th>alpha_126</th>\n",
       "      <th>alpha_127</th>\n",
       "      <th>alpha_128</th>\n",
       "      <th>alpha_129</th>\n",
       "      <th>alpha_13</th>\n",
       "      <th>alpha_130</th>\n",
       "      <th>alpha_131</th>\n",
       "      <th>alpha_132</th>\n",
       "      <th>alpha_133</th>\n",
       "      <th>alpha_134</th>\n",
       "      <th>alpha_135</th>\n",
       "      <th>alpha_136</th>\n",
       "      <th>alpha_137</th>\n",
       "      <th>alpha_138</th>\n",
       "      <th>alpha_139</th>\n",
       "      <th>alpha_14</th>\n",
       "      <th>alpha_140</th>\n",
       "      <th>alpha_141</th>\n",
       "      <th>alpha_142</th>\n",
       "      <th>alpha_143</th>\n",
       "      <th>alpha_144</th>\n",
       "      <th>alpha_145</th>\n",
       "      <th>alpha_146</th>\n",
       "      <th>alpha_147</th>\n",
       "      <th>alpha_148</th>\n",
       "      <th>alpha_149</th>\n",
       "      <th>alpha_15</th>\n",
       "      <th>alpha_150</th>\n",
       "      <th>alpha_151</th>\n",
       "      <th>alpha_152</th>\n",
       "      <th>alpha_153</th>\n",
       "      <th>alpha_154</th>\n",
       "      <th>alpha_155</th>\n",
       "      <th>alpha_156</th>\n",
       "      <th>alpha_157</th>\n",
       "      <th>alpha_158</th>\n",
       "      <th>alpha_159</th>\n",
       "      <th>alpha_16</th>\n",
       "      <th>alpha_160</th>\n",
       "      <th>alpha_161</th>\n",
       "      <th>alpha_162</th>\n",
       "      <th>alpha_163</th>\n",
       "      <th>alpha_164</th>\n",
       "      <th>alpha_165</th>\n",
       "      <th>alpha_166</th>\n",
       "      <th>alpha_167</th>\n",
       "      <th>alpha_168</th>\n",
       "      <th>alpha_169</th>\n",
       "      <th>alpha_17</th>\n",
       "      <th>alpha_170</th>\n",
       "      <th>alpha_171</th>\n",
       "      <th>alpha_172</th>\n",
       "      <th>alpha_173</th>\n",
       "      <th>alpha_174</th>\n",
       "      <th>alpha_175</th>\n",
       "      <th>alpha_176</th>\n",
       "      <th>alpha_177</th>\n",
       "      <th>alpha_178</th>\n",
       "      <th>alpha_179</th>\n",
       "      <th>alpha_18</th>\n",
       "      <th>alpha_180</th>\n",
       "      <th>alpha_181</th>\n",
       "      <th>alpha_182</th>\n",
       "      <th>alpha_183</th>\n",
       "      <th>alpha_184</th>\n",
       "      <th>alpha_185</th>\n",
       "      <th>alpha_186</th>\n",
       "      <th>alpha_187</th>\n",
       "      <th>alpha_188</th>\n",
       "      <th>alpha_189</th>\n",
       "      <th>alpha_19</th>\n",
       "      <th>alpha_190</th>\n",
       "      <th>alpha_191</th>\n",
       "      <th>alpha_2</th>\n",
       "      <th>alpha_20</th>\n",
       "      <th>alpha_21</th>\n",
       "      <th>alpha_22</th>\n",
       "      <th>alpha_23</th>\n",
       "      <th>alpha_24</th>\n",
       "      <th>alpha_25</th>\n",
       "      <th>alpha_26</th>\n",
       "      <th>alpha_27</th>\n",
       "      <th>alpha_28</th>\n",
       "      <th>alpha_29</th>\n",
       "      <th>alpha_3</th>\n",
       "      <th>alpha_30</th>\n",
       "      <th>alpha_31</th>\n",
       "      <th>alpha_32</th>\n",
       "      <th>alpha_33</th>\n",
       "      <th>alpha_34</th>\n",
       "      <th>alpha_35</th>\n",
       "      <th>alpha_36</th>\n",
       "      <th>alpha_37</th>\n",
       "      <th>alpha_38</th>\n",
       "      <th>alpha_39</th>\n",
       "      <th>alpha_4</th>\n",
       "      <th>alpha_40</th>\n",
       "      <th>alpha_41</th>\n",
       "      <th>alpha_42</th>\n",
       "      <th>alpha_43</th>\n",
       "      <th>alpha_44</th>\n",
       "      <th>alpha_45</th>\n",
       "      <th>alpha_46</th>\n",
       "      <th>alpha_47</th>\n",
       "      <th>alpha_48</th>\n",
       "      <th>alpha_49</th>\n",
       "      <th>alpha_5</th>\n",
       "      <th>alpha_50</th>\n",
       "      <th>alpha_51</th>\n",
       "      <th>alpha_52</th>\n",
       "      <th>alpha_53</th>\n",
       "      <th>alpha_54</th>\n",
       "      <th>alpha_55</th>\n",
       "      <th>alpha_56</th>\n",
       "      <th>alpha_57</th>\n",
       "      <th>alpha_58</th>\n",
       "      <th>alpha_59</th>\n",
       "      <th>alpha_6</th>\n",
       "      <th>alpha_60</th>\n",
       "      <th>alpha_61</th>\n",
       "      <th>alpha_62</th>\n",
       "      <th>alpha_63</th>\n",
       "      <th>alpha_64</th>\n",
       "      <th>alpha_65</th>\n",
       "      <th>alpha_66</th>\n",
       "      <th>alpha_67</th>\n",
       "      <th>alpha_68</th>\n",
       "      <th>alpha_69</th>\n",
       "      <th>alpha_7</th>\n",
       "      <th>alpha_70</th>\n",
       "      <th>alpha_71</th>\n",
       "      <th>alpha_72</th>\n",
       "      <th>alpha_73</th>\n",
       "      <th>alpha_74</th>\n",
       "      <th>alpha_75</th>\n",
       "      <th>alpha_76</th>\n",
       "      <th>alpha_77</th>\n",
       "      <th>alpha_78</th>\n",
       "      <th>alpha_79</th>\n",
       "      <th>alpha_8</th>\n",
       "      <th>alpha_80</th>\n",
       "      <th>alpha_81</th>\n",
       "      <th>alpha_82</th>\n",
       "      <th>alpha_83</th>\n",
       "      <th>alpha_84</th>\n",
       "      <th>alpha_85</th>\n",
       "      <th>alpha_86</th>\n",
       "      <th>alpha_87</th>\n",
       "      <th>alpha_88</th>\n",
       "      <th>alpha_89</th>\n",
       "      <th>alpha_9</th>\n",
       "      <th>alpha_90</th>\n",
       "      <th>alpha_91</th>\n",
       "      <th>alpha_92</th>\n",
       "      <th>alpha_93</th>\n",
       "      <th>alpha_94</th>\n",
       "      <th>alpha_95</th>\n",
       "      <th>alpha_96</th>\n",
       "      <th>alpha_97</th>\n",
       "      <th>alpha_98</th>\n",
       "      <th>alpha_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240973.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240965.000000</td>\n",
       "      <td>235908.000000</td>\n",
       "      <td>235877.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240972.000000</td>\n",
       "      <td>240972.000000</td>\n",
       "      <td>240959.000000</td>\n",
       "      <td>240243.000000</td>\n",
       "      <td>240243.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240967.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240875.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240967.000000</td>\n",
       "      <td>240967.000000</td>\n",
       "      <td>240967.000000</td>\n",
       "      <td>240967.000000</td>\n",
       "      <td>240953.000000</td>\n",
       "      <td>240972.000000</td>\n",
       "      <td>232849.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240965.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240967.000000</td>\n",
       "      <td>240967.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240967.000000</td>\n",
       "      <td>240967.000000</td>\n",
       "      <td>240967.000000</td>\n",
       "      <td>240967.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240965.000000</td>\n",
       "      <td>240184.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>230909.000000</td>\n",
       "      <td>196129.000000</td>\n",
       "      <td>240762.000000</td>\n",
       "      <td>239548.000000</td>\n",
       "      <td>222561.000000</td>\n",
       "      <td>240949.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240965.000000</td>\n",
       "      <td>240965.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240973.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240295.000000</td>\n",
       "      <td>232944.000000</td>\n",
       "      <td>238220.000000</td>\n",
       "      <td>235100.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240967.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240292.000000</td>\n",
       "      <td>240967.000000</td>\n",
       "      <td>240967.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>238206.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>237573.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240607.000000</td>\n",
       "      <td>222561.000000</td>\n",
       "      <td>222670.000000</td>\n",
       "      <td>222469.000000</td>\n",
       "      <td>240967.000000</td>\n",
       "      <td>234285.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>239813.000000</td>\n",
       "      <td>240953.000000</td>\n",
       "      <td>240950.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>238880.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>223918.000000</td>\n",
       "      <td>239140.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240515.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>238880.000000</td>\n",
       "      <td>236254.000000</td>\n",
       "      <td>238880.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>223674.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>237784.000000</td>\n",
       "      <td>239748.000000</td>\n",
       "      <td>237698.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240960.000000</td>\n",
       "      <td>240960.000000</td>\n",
       "      <td>240965.000000</td>\n",
       "      <td>240962.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240203.000000</td>\n",
       "      <td>240606.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240907.000000</td>\n",
       "      <td>239628.000000</td>\n",
       "      <td>239628.000000</td>\n",
       "      <td>125952.000000</td>\n",
       "      <td>193439.000000</td>\n",
       "      <td>195223.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240967.000000</td>\n",
       "      <td>240967.000000</td>\n",
       "      <td>240967.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>219505.000000</td>\n",
       "      <td>240953.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240972.000000</td>\n",
       "      <td>240967.000000</td>\n",
       "      <td>240953.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240973.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240893.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>236579.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240893.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>238390.000000</td>\n",
       "      <td>239556.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240823.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240714.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>234523.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240838.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>238611.000000</td>\n",
       "      <td>239514.000000</td>\n",
       "      <td>230295.000000</td>\n",
       "      <td>213185.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>236104.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>239548.000000</td>\n",
       "      <td>237715.000000</td>\n",
       "      <td>238770.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>238413.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240960.000000</td>\n",
       "      <td>234614.000000</td>\n",
       "      <td>217825.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240618.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>235969.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>230458.000000</td>\n",
       "      <td>213266.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>238270.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240618.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>235858.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>239868.000000</td>\n",
       "      <td>240969.000000</td>\n",
       "      <td>240955.000000</td>\n",
       "      <td>237872.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240918.000000</td>\n",
       "      <td>240973.000000</td>\n",
       "      <td>240973.000000</td>\n",
       "      <td>238472.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240973.000000</td>\n",
       "      <td>240965.000000</td>\n",
       "      <td>240965.000000</td>\n",
       "      <td>221209.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>232122.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240965.000000</td>\n",
       "      <td>240967.000000</td>\n",
       "      <td>240910.000000</td>\n",
       "      <td>231584.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240893.000000</td>\n",
       "      <td>240885.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240893.000000</td>\n",
       "      <td>240766.000000</td>\n",
       "      <td>240967.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240953.000000</td>\n",
       "      <td>240967.000000</td>\n",
       "      <td>240973.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>222469.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240965.000000</td>\n",
       "      <td>236258.000000</td>\n",
       "      <td>240950.000000</td>\n",
       "      <td>240967.000000</td>\n",
       "      <td>240965.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>237519.000000</td>\n",
       "      <td>237519.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>237715.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240515.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240728.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>239960.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240178.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>236816.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>236816.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240755.000000</td>\n",
       "      <td>240718.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>239757.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240967.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240973.000000</td>\n",
       "      <td>240965.000000</td>\n",
       "      <td>240972.000000</td>\n",
       "      <td>240323.000000</td>\n",
       "      <td>235908.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240955.000000</td>\n",
       "      <td>2.409750e+05</td>\n",
       "      <td>237507.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240925.000000</td>\n",
       "      <td>240925.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240959.000000</td>\n",
       "      <td>2.409750e+05</td>\n",
       "      <td>217843.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>2.409750e+05</td>\n",
       "      <td>240914.000000</td>\n",
       "      <td>2.409750e+05</td>\n",
       "      <td>240859.000000</td>\n",
       "      <td>240859.000000</td>\n",
       "      <td>240973.000000</td>\n",
       "      <td>240973.000000</td>\n",
       "      <td>240973.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240925.000000</td>\n",
       "      <td>240468.000000</td>\n",
       "      <td>240966.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>237477.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>233622.000000</td>\n",
       "      <td>240684.000000</td>\n",
       "      <td>225754.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240887.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240961.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>239743.000000</td>\n",
       "      <td>239573.000000</td>\n",
       "      <td>2.409250e+05</td>\n",
       "      <td>240925.000000</td>\n",
       "      <td>2.409690e+05</td>\n",
       "      <td>240973.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>223339.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240974.000000</td>\n",
       "      <td>235529.000000</td>\n",
       "      <td>240961.000000</td>\n",
       "      <td>2.409250e+05</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>2.409140e+05</td>\n",
       "      <td>240826.000000</td>\n",
       "      <td>240973.000000</td>\n",
       "      <td>240959.000000</td>\n",
       "      <td>236087.000000</td>\n",
       "      <td>238449.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>2.409750e+05</td>\n",
       "      <td>240973.000000</td>\n",
       "      <td>2.409670e+05</td>\n",
       "      <td>240869.000000</td>\n",
       "      <td>184375.000000</td>\n",
       "      <td>2.409750e+05</td>\n",
       "      <td>240973.000000</td>\n",
       "      <td>240962.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240861.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240961.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240925.000000</td>\n",
       "      <td>2.409730e+05</td>\n",
       "      <td>204930.000000</td>\n",
       "      <td>240847.000000</td>\n",
       "      <td>240961.000000</td>\n",
       "      <td>240925.000000</td>\n",
       "      <td>240971.000000</td>\n",
       "      <td>2.409520e+05</td>\n",
       "      <td>240925.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240925.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240973.000000</td>\n",
       "      <td>240968.000000</td>\n",
       "      <td>240925.000000</td>\n",
       "      <td>2.409750e+05</td>\n",
       "      <td>2.394080e+05</td>\n",
       "      <td>240974.000000</td>\n",
       "      <td>2.409750e+05</td>\n",
       "      <td>240965.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>235995.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240826.000000</td>\n",
       "      <td>240914.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240962.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240054.000000</td>\n",
       "      <td>240918.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240974.000000</td>\n",
       "      <td>240932.000000</td>\n",
       "      <td>240950.000000</td>\n",
       "      <td>240966.000000</td>\n",
       "      <td>238149.000000</td>\n",
       "      <td>240973.000000</td>\n",
       "      <td>240974.000000</td>\n",
       "      <td>240969.000000</td>\n",
       "      <td>2.409730e+05</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240961.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>238721.000000</td>\n",
       "      <td>240961.000000</td>\n",
       "      <td>240946.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240965.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>164578.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240973.000000</td>\n",
       "      <td>240966.000000</td>\n",
       "      <td>2.409750e+05</td>\n",
       "      <td>240946.000000</td>\n",
       "      <td>201414.000000</td>\n",
       "      <td>240869.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240925.000000</td>\n",
       "      <td>240961.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240961.000000</td>\n",
       "      <td>240961.000000</td>\n",
       "      <td>240805.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240966.000000</td>\n",
       "      <td>240914.000000</td>\n",
       "      <td>237849.000000</td>\n",
       "      <td>2.409750e+05</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240974.000000</td>\n",
       "      <td>2.409210e+05</td>\n",
       "      <td>227004.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>2.409750e+05</td>\n",
       "      <td>231783.000000</td>\n",
       "      <td>240974.000000</td>\n",
       "      <td>240974.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>2.409750e+05</td>\n",
       "      <td>240914.000000</td>\n",
       "      <td>240974.000000</td>\n",
       "      <td>2.409740e+05</td>\n",
       "      <td>240869.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240654.000000</td>\n",
       "      <td>238266.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240914.000000</td>\n",
       "      <td>239504.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240974.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>2.409750e+05</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>2.409140e+05</td>\n",
       "      <td>240925.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>240961.000000</td>\n",
       "      <td>240959.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>2.409750e+05</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>239952.000000</td>\n",
       "      <td>184604.000000</td>\n",
       "      <td>240914.000000</td>\n",
       "      <td>2.406840e+05</td>\n",
       "      <td>2.409250e+05</td>\n",
       "      <td>240975.000000</td>\n",
       "      <td>2.409750e+05</td>\n",
       "      <td>240974.000000</td>\n",
       "      <td>240975.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500645</td>\n",
       "      <td>0.504629</td>\n",
       "      <td>0.508350</td>\n",
       "      <td>0.533987</td>\n",
       "      <td>0.533960</td>\n",
       "      <td>0.533966</td>\n",
       "      <td>0.498894</td>\n",
       "      <td>0.496710</td>\n",
       "      <td>0.496818</td>\n",
       "      <td>0.522025</td>\n",
       "      <td>0.496482</td>\n",
       "      <td>0.492176</td>\n",
       "      <td>0.493097</td>\n",
       "      <td>0.511797</td>\n",
       "      <td>0.511176</td>\n",
       "      <td>0.504986</td>\n",
       "      <td>0.569467</td>\n",
       "      <td>0.524187</td>\n",
       "      <td>0.524585</td>\n",
       "      <td>0.506486</td>\n",
       "      <td>0.515315</td>\n",
       "      <td>0.562125</td>\n",
       "      <td>0.492909</td>\n",
       "      <td>0.514249</td>\n",
       "      <td>0.514019</td>\n",
       "      <td>0.514233</td>\n",
       "      <td>0.506846</td>\n",
       "      <td>0.513668</td>\n",
       "      <td>0.506616</td>\n",
       "      <td>0.525746</td>\n",
       "      <td>0.522025</td>\n",
       "      <td>0.513848</td>\n",
       "      <td>0.508839</td>\n",
       "      <td>0.506888</td>\n",
       "      <td>0.509884</td>\n",
       "      <td>0.503846</td>\n",
       "      <td>0.530963</td>\n",
       "      <td>0.507789</td>\n",
       "      <td>0.512265</td>\n",
       "      <td>0.502759</td>\n",
       "      <td>0.531191</td>\n",
       "      <td>0.525429</td>\n",
       "      <td>0.533289</td>\n",
       "      <td>0.531191</td>\n",
       "      <td>0.537718</td>\n",
       "      <td>0.534779</td>\n",
       "      <td>0.527898</td>\n",
       "      <td>0.510485</td>\n",
       "      <td>0.508292</td>\n",
       "      <td>0.506962</td>\n",
       "      <td>0.508569</td>\n",
       "      <td>0.501898</td>\n",
       "      <td>0.528463</td>\n",
       "      <td>0.528679</td>\n",
       "      <td>0.504661</td>\n",
       "      <td>0.491984</td>\n",
       "      <td>0.504969</td>\n",
       "      <td>0.490914</td>\n",
       "      <td>0.414117</td>\n",
       "      <td>0.509968</td>\n",
       "      <td>0.513315</td>\n",
       "      <td>0.457711</td>\n",
       "      <td>0.519979</td>\n",
       "      <td>0.503884</td>\n",
       "      <td>0.520511</td>\n",
       "      <td>0.515700</td>\n",
       "      <td>0.504595</td>\n",
       "      <td>0.513270</td>\n",
       "      <td>0.509314</td>\n",
       "      <td>0.526703</td>\n",
       "      <td>0.522423</td>\n",
       "      <td>0.497904</td>\n",
       "      <td>0.505715</td>\n",
       "      <td>0.556752</td>\n",
       "      <td>0.506400</td>\n",
       "      <td>0.516971</td>\n",
       "      <td>0.512742</td>\n",
       "      <td>0.553586</td>\n",
       "      <td>0.487014</td>\n",
       "      <td>0.488698</td>\n",
       "      <td>0.481352</td>\n",
       "      <td>0.498858</td>\n",
       "      <td>0.518697</td>\n",
       "      <td>0.517615</td>\n",
       "      <td>0.519090</td>\n",
       "      <td>0.510338</td>\n",
       "      <td>0.507143</td>\n",
       "      <td>0.510687</td>\n",
       "      <td>0.536385</td>\n",
       "      <td>0.485129</td>\n",
       "      <td>0.502724</td>\n",
       "      <td>0.505253</td>\n",
       "      <td>0.497644</td>\n",
       "      <td>0.514440</td>\n",
       "      <td>0.503255</td>\n",
       "      <td>0.507143</td>\n",
       "      <td>0.476293</td>\n",
       "      <td>0.518547</td>\n",
       "      <td>0.527116</td>\n",
       "      <td>0.510904</td>\n",
       "      <td>0.538601</td>\n",
       "      <td>0.471957</td>\n",
       "      <td>0.465723</td>\n",
       "      <td>0.445226</td>\n",
       "      <td>0.523212</td>\n",
       "      <td>0.494510</td>\n",
       "      <td>0.556839</td>\n",
       "      <td>0.557867</td>\n",
       "      <td>0.566727</td>\n",
       "      <td>0.528014</td>\n",
       "      <td>0.499282</td>\n",
       "      <td>0.535795</td>\n",
       "      <td>0.535891</td>\n",
       "      <td>0.539331</td>\n",
       "      <td>0.536215</td>\n",
       "      <td>0.536451</td>\n",
       "      <td>0.535573</td>\n",
       "      <td>0.537553</td>\n",
       "      <td>0.507784</td>\n",
       "      <td>0.508858</td>\n",
       "      <td>0.537789</td>\n",
       "      <td>0.539906</td>\n",
       "      <td>0.541444</td>\n",
       "      <td>0.551865</td>\n",
       "      <td>0.517714</td>\n",
       "      <td>0.503641</td>\n",
       "      <td>0.502943</td>\n",
       "      <td>0.505253</td>\n",
       "      <td>0.520836</td>\n",
       "      <td>0.516355</td>\n",
       "      <td>0.510909</td>\n",
       "      <td>0.515504</td>\n",
       "      <td>0.499226</td>\n",
       "      <td>0.487091</td>\n",
       "      <td>0.483411</td>\n",
       "      <td>0.537789</td>\n",
       "      <td>0.504503</td>\n",
       "      <td>0.532653</td>\n",
       "      <td>0.510650</td>\n",
       "      <td>0.477431</td>\n",
       "      <td>0.506990</td>\n",
       "      <td>0.521465</td>\n",
       "      <td>0.508270</td>\n",
       "      <td>0.499635</td>\n",
       "      <td>0.492621</td>\n",
       "      <td>0.486459</td>\n",
       "      <td>0.491425</td>\n",
       "      <td>0.504057</td>\n",
       "      <td>0.497261</td>\n",
       "      <td>0.486905</td>\n",
       "      <td>0.496519</td>\n",
       "      <td>0.489641</td>\n",
       "      <td>0.519993</td>\n",
       "      <td>0.563885</td>\n",
       "      <td>0.567492</td>\n",
       "      <td>0.533658</td>\n",
       "      <td>0.533658</td>\n",
       "      <td>0.477268</td>\n",
       "      <td>0.504477</td>\n",
       "      <td>0.452878</td>\n",
       "      <td>0.493683</td>\n",
       "      <td>0.503563</td>\n",
       "      <td>0.498854</td>\n",
       "      <td>0.547106</td>\n",
       "      <td>0.548612</td>\n",
       "      <td>0.553163</td>\n",
       "      <td>0.540756</td>\n",
       "      <td>0.524495</td>\n",
       "      <td>0.519087</td>\n",
       "      <td>0.531649</td>\n",
       "      <td>0.508850</td>\n",
       "      <td>0.503264</td>\n",
       "      <td>0.324063</td>\n",
       "      <td>0.411808</td>\n",
       "      <td>0.412887</td>\n",
       "      <td>0.495296</td>\n",
       "      <td>0.509041</td>\n",
       "      <td>0.509749</td>\n",
       "      <td>0.509056</td>\n",
       "      <td>0.505721</td>\n",
       "      <td>0.509900</td>\n",
       "      <td>0.512194</td>\n",
       "      <td>0.512097</td>\n",
       "      <td>0.566670</td>\n",
       "      <td>0.589273</td>\n",
       "      <td>0.529512</td>\n",
       "      <td>0.484886</td>\n",
       "      <td>0.534167</td>\n",
       "      <td>0.498108</td>\n",
       "      <td>0.499173</td>\n",
       "      <td>0.495141</td>\n",
       "      <td>0.535770</td>\n",
       "      <td>0.512861</td>\n",
       "      <td>0.506700</td>\n",
       "      <td>0.507117</td>\n",
       "      <td>0.538859</td>\n",
       "      <td>0.536170</td>\n",
       "      <td>0.535566</td>\n",
       "      <td>0.537338</td>\n",
       "      <td>0.511444</td>\n",
       "      <td>0.512910</td>\n",
       "      <td>0.506022</td>\n",
       "      <td>0.535104</td>\n",
       "      <td>0.507210</td>\n",
       "      <td>0.505818</td>\n",
       "      <td>0.511311</td>\n",
       "      <td>0.566670</td>\n",
       "      <td>0.562520</td>\n",
       "      <td>0.555206</td>\n",
       "      <td>0.552764</td>\n",
       "      <td>0.566670</td>\n",
       "      <td>0.510983</td>\n",
       "      <td>0.518368</td>\n",
       "      <td>0.528577</td>\n",
       "      <td>0.509163</td>\n",
       "      <td>0.520841</td>\n",
       "      <td>0.502264</td>\n",
       "      <td>0.548113</td>\n",
       "      <td>0.538793</td>\n",
       "      <td>0.515852</td>\n",
       "      <td>0.521146</td>\n",
       "      <td>0.528543</td>\n",
       "      <td>0.534344</td>\n",
       "      <td>0.503249</td>\n",
       "      <td>0.589273</td>\n",
       "      <td>0.520324</td>\n",
       "      <td>0.538170</td>\n",
       "      <td>0.496576</td>\n",
       "      <td>0.536407</td>\n",
       "      <td>0.506468</td>\n",
       "      <td>0.535886</td>\n",
       "      <td>0.475746</td>\n",
       "      <td>0.516370</td>\n",
       "      <td>0.518394</td>\n",
       "      <td>0.538555</td>\n",
       "      <td>0.557758</td>\n",
       "      <td>0.509734</td>\n",
       "      <td>0.521944</td>\n",
       "      <td>0.485036</td>\n",
       "      <td>0.460909</td>\n",
       "      <td>0.528577</td>\n",
       "      <td>0.557598</td>\n",
       "      <td>0.573509</td>\n",
       "      <td>0.541004</td>\n",
       "      <td>0.513824</td>\n",
       "      <td>0.526735</td>\n",
       "      <td>0.549135</td>\n",
       "      <td>0.549146</td>\n",
       "      <td>0.549140</td>\n",
       "      <td>0.527981</td>\n",
       "      <td>0.512223</td>\n",
       "      <td>0.517425</td>\n",
       "      <td>0.520362</td>\n",
       "      <td>0.547313</td>\n",
       "      <td>0.555150</td>\n",
       "      <td>0.509553</td>\n",
       "      <td>0.503174</td>\n",
       "      <td>0.515553</td>\n",
       "      <td>0.498410</td>\n",
       "      <td>0.515682</td>\n",
       "      <td>0.541722</td>\n",
       "      <td>0.540927</td>\n",
       "      <td>0.532075</td>\n",
       "      <td>0.531980</td>\n",
       "      <td>0.514277</td>\n",
       "      <td>0.505732</td>\n",
       "      <td>0.485585</td>\n",
       "      <td>0.535291</td>\n",
       "      <td>0.534322</td>\n",
       "      <td>0.489925</td>\n",
       "      <td>0.487875</td>\n",
       "      <td>0.516832</td>\n",
       "      <td>0.481298</td>\n",
       "      <td>0.503890</td>\n",
       "      <td>0.490304</td>\n",
       "      <td>0.459194</td>\n",
       "      <td>0.509016</td>\n",
       "      <td>0.507872</td>\n",
       "      <td>0.511542</td>\n",
       "      <td>0.508710</td>\n",
       "      <td>0.472297</td>\n",
       "      <td>0.507897</td>\n",
       "      <td>0.508991</td>\n",
       "      <td>0.485048</td>\n",
       "      <td>0.484785</td>\n",
       "      <td>0.508000</td>\n",
       "      <td>0.527190</td>\n",
       "      <td>0.513371</td>\n",
       "      <td>0.513565</td>\n",
       "      <td>0.513774</td>\n",
       "      <td>0.488387</td>\n",
       "      <td>0.507230</td>\n",
       "      <td>0.498093</td>\n",
       "      <td>0.504961</td>\n",
       "      <td>0.509939</td>\n",
       "      <td>0.506898</td>\n",
       "      <td>0.504297</td>\n",
       "      <td>0.519377</td>\n",
       "      <td>0.507093</td>\n",
       "      <td>0.492179</td>\n",
       "      <td>0.504897</td>\n",
       "      <td>0.504898</td>\n",
       "      <td>0.480107</td>\n",
       "      <td>0.508967</td>\n",
       "      <td>0.508253</td>\n",
       "      <td>0.513983</td>\n",
       "      <td>0.514516</td>\n",
       "      <td>0.499545</td>\n",
       "      <td>0.457340</td>\n",
       "      <td>0.508967</td>\n",
       "      <td>0.527033</td>\n",
       "      <td>0.532418</td>\n",
       "      <td>0.527375</td>\n",
       "      <td>0.504914</td>\n",
       "      <td>0.508506</td>\n",
       "      <td>0.530631</td>\n",
       "      <td>0.529767</td>\n",
       "      <td>0.535496</td>\n",
       "      <td>0.535728</td>\n",
       "      <td>0.538870</td>\n",
       "      <td>0.536361</td>\n",
       "      <td>0.533081</td>\n",
       "      <td>0.529929</td>\n",
       "      <td>0.506008</td>\n",
       "      <td>0.491286</td>\n",
       "      <td>0.497006</td>\n",
       "      <td>0.480581</td>\n",
       "      <td>0.507493</td>\n",
       "      <td>0.504692</td>\n",
       "      <td>0.503913</td>\n",
       "      <td>0.467913</td>\n",
       "      <td>0.577758</td>\n",
       "      <td>0.553884</td>\n",
       "      <td>0.562187</td>\n",
       "      <td>0.527720</td>\n",
       "      <td>0.512734</td>\n",
       "      <td>0.517636</td>\n",
       "      <td>0.507073</td>\n",
       "      <td>0.519655</td>\n",
       "      <td>0.506119</td>\n",
       "      <td>0.500501</td>\n",
       "      <td>0.499494</td>\n",
       "      <td>0.511625</td>\n",
       "      <td>0.508139</td>\n",
       "      <td>0.517746</td>\n",
       "      <td>0.563041</td>\n",
       "      <td>0.500351</td>\n",
       "      <td>0.547823</td>\n",
       "      <td>0.514588</td>\n",
       "      <td>0.518403</td>\n",
       "      <td>0.494461</td>\n",
       "      <td>0.504986</td>\n",
       "      <td>0.500043</td>\n",
       "      <td>0.508357</td>\n",
       "      <td>0.489864</td>\n",
       "      <td>0.508513</td>\n",
       "      <td>0.525484</td>\n",
       "      <td>0.508438</td>\n",
       "      <td>0.532626</td>\n",
       "      <td>0.558829</td>\n",
       "      <td>0.580989</td>\n",
       "      <td>0.535430</td>\n",
       "      <td>0.535430</td>\n",
       "      <td>0.510907</td>\n",
       "      <td>0.535433</td>\n",
       "      <td>0.534323</td>\n",
       "      <td>0.558386</td>\n",
       "      <td>0.504979</td>\n",
       "      <td>0.506818</td>\n",
       "      <td>0.561785</td>\n",
       "      <td>0.497552</td>\n",
       "      <td>0.500601</td>\n",
       "      <td>0.554286</td>\n",
       "      <td>0.552568</td>\n",
       "      <td>0.543012</td>\n",
       "      <td>0.541772</td>\n",
       "      <td>0.496838</td>\n",
       "      <td>0.523794</td>\n",
       "      <td>0.508597</td>\n",
       "      <td>0.519389</td>\n",
       "      <td>0.569464</td>\n",
       "      <td>0.517743</td>\n",
       "      <td>0.550162</td>\n",
       "      <td>0.573916</td>\n",
       "      <td>0.529695</td>\n",
       "      <td>0.521970</td>\n",
       "      <td>0.495088</td>\n",
       "      <td>0.507259</td>\n",
       "      <td>0.501650</td>\n",
       "      <td>0.504956</td>\n",
       "      <td>0.499420</td>\n",
       "      <td>0.502425</td>\n",
       "      <td>0.555228</td>\n",
       "      <td>0.521690</td>\n",
       "      <td>0.508487</td>\n",
       "      <td>0.507276</td>\n",
       "      <td>0.551897</td>\n",
       "      <td>0.552014</td>\n",
       "      <td>0.552468</td>\n",
       "      <td>0.551241</td>\n",
       "      <td>0.509486</td>\n",
       "      <td>0.501465</td>\n",
       "      <td>0.497602</td>\n",
       "      <td>0.500456</td>\n",
       "      <td>0.497955</td>\n",
       "      <td>0.501945</td>\n",
       "      <td>0.498760</td>\n",
       "      <td>0.507253</td>\n",
       "      <td>0.510281</td>\n",
       "      <td>0.510882</td>\n",
       "      <td>0.542771</td>\n",
       "      <td>0.542900</td>\n",
       "      <td>0.536571</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.498137</td>\n",
       "      <td>0.492984</td>\n",
       "      <td>0.495046</td>\n",
       "      <td>0.511774</td>\n",
       "      <td>0.510863</td>\n",
       "      <td>0.512726</td>\n",
       "      <td>0.541004</td>\n",
       "      <td>0.495342</td>\n",
       "      <td>0.500961</td>\n",
       "      <td>315624.963117</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>-0.217869</td>\n",
       "      <td>0.502160</td>\n",
       "      <td>6.787220e+06</td>\n",
       "      <td>0.009864</td>\n",
       "      <td>49.319377</td>\n",
       "      <td>48.536910</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>-0.169837</td>\n",
       "      <td>0.018995</td>\n",
       "      <td>-1.590789e-01</td>\n",
       "      <td>-0.711840</td>\n",
       "      <td>1.002601</td>\n",
       "      <td>-3.317557e+06</td>\n",
       "      <td>103.997285</td>\n",
       "      <td>-3.266880e+04</td>\n",
       "      <td>-0.048513</td>\n",
       "      <td>-0.048513</td>\n",
       "      <td>-0.001245</td>\n",
       "      <td>1.456024</td>\n",
       "      <td>1.456024</td>\n",
       "      <td>0.106584</td>\n",
       "      <td>114.715241</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>-0.250042</td>\n",
       "      <td>4.532614</td>\n",
       "      <td>-0.621322</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>0.020201</td>\n",
       "      <td>-0.013044</td>\n",
       "      <td>3.160014</td>\n",
       "      <td>10.819647</td>\n",
       "      <td>5.824981</td>\n",
       "      <td>54.840860</td>\n",
       "      <td>1.360542</td>\n",
       "      <td>-0.009794</td>\n",
       "      <td>5.231543</td>\n",
       "      <td>0.697621</td>\n",
       "      <td>1.679766e+08</td>\n",
       "      <td>1.434969</td>\n",
       "      <td>6.772992e+05</td>\n",
       "      <td>1.007457</td>\n",
       "      <td>-0.120231</td>\n",
       "      <td>-0.450663</td>\n",
       "      <td>0.171363</td>\n",
       "      <td>-0.240131</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.412638</td>\n",
       "      <td>-0.505592</td>\n",
       "      <td>-1.441562e-01</td>\n",
       "      <td>0.190296</td>\n",
       "      <td>3.468979e-10</td>\n",
       "      <td>-6.349429</td>\n",
       "      <td>1.230589</td>\n",
       "      <td>1.588466</td>\n",
       "      <td>0.009197</td>\n",
       "      <td>0.964039</td>\n",
       "      <td>-0.001265</td>\n",
       "      <td>1.440872e+08</td>\n",
       "      <td>0.017940</td>\n",
       "      <td>-9.233634e-05</td>\n",
       "      <td>10.826100</td>\n",
       "      <td>-0.601833</td>\n",
       "      <td>1.705316e+04</td>\n",
       "      <td>-0.714416</td>\n",
       "      <td>0.955786</td>\n",
       "      <td>0.038547</td>\n",
       "      <td>-2647.027205</td>\n",
       "      <td>-0.725039</td>\n",
       "      <td>0.146701</td>\n",
       "      <td>0.445936</td>\n",
       "      <td>0.499978</td>\n",
       "      <td>0.498000</td>\n",
       "      <td>1.993393e+06</td>\n",
       "      <td>69.988140</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>1.355779</td>\n",
       "      <td>-1.047639</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>2.814547e+173</td>\n",
       "      <td>-0.286147</td>\n",
       "      <td>-3.056303</td>\n",
       "      <td>35.979017</td>\n",
       "      <td>12.957392</td>\n",
       "      <td>0.148784</td>\n",
       "      <td>0.448273</td>\n",
       "      <td>0.291458</td>\n",
       "      <td>49.971879</td>\n",
       "      <td>7.755351e+04</td>\n",
       "      <td>2.518993e-01</td>\n",
       "      <td>1.001715</td>\n",
       "      <td>-7.560313e+06</td>\n",
       "      <td>-0.777249</td>\n",
       "      <td>0.673641</td>\n",
       "      <td>30.001292</td>\n",
       "      <td>1.027685</td>\n",
       "      <td>0.515235</td>\n",
       "      <td>35.931785</td>\n",
       "      <td>3.339376</td>\n",
       "      <td>0.734805</td>\n",
       "      <td>0.288957</td>\n",
       "      <td>-0.043254</td>\n",
       "      <td>-0.112718</td>\n",
       "      <td>0.155116</td>\n",
       "      <td>-0.026644</td>\n",
       "      <td>0.207667</td>\n",
       "      <td>1.042345</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>48.882372</td>\n",
       "      <td>-0.000924</td>\n",
       "      <td>-0.754872</td>\n",
       "      <td>0.879356</td>\n",
       "      <td>0.328110</td>\n",
       "      <td>48.738994</td>\n",
       "      <td>4.131160e+05</td>\n",
       "      <td>-0.034449</td>\n",
       "      <td>4.041927</td>\n",
       "      <td>0.046532</td>\n",
       "      <td>-1.429595</td>\n",
       "      <td>0.005385</td>\n",
       "      <td>1.002641</td>\n",
       "      <td>-0.340144</td>\n",
       "      <td>0.491076</td>\n",
       "      <td>-0.501472</td>\n",
       "      <td>-0.074577</td>\n",
       "      <td>-0.005304</td>\n",
       "      <td>-0.274738</td>\n",
       "      <td>1.400739</td>\n",
       "      <td>-0.489869</td>\n",
       "      <td>-0.238008</td>\n",
       "      <td>1.009143e+07</td>\n",
       "      <td>1.191480</td>\n",
       "      <td>0.256519</td>\n",
       "      <td>1.002326</td>\n",
       "      <td>50.440034</td>\n",
       "      <td>-0.128266</td>\n",
       "      <td>0.499326</td>\n",
       "      <td>-0.617461</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.500674</td>\n",
       "      <td>106.764448</td>\n",
       "      <td>47.128679</td>\n",
       "      <td>-0.490482</td>\n",
       "      <td>-8.622405</td>\n",
       "      <td>0.008868</td>\n",
       "      <td>4.938340e+01</td>\n",
       "      <td>44.953128</td>\n",
       "      <td>0.210878</td>\n",
       "      <td>-0.498392</td>\n",
       "      <td>1.210269e+07</td>\n",
       "      <td>-0.658504</td>\n",
       "      <td>-0.369486</td>\n",
       "      <td>9.967506e+11</td>\n",
       "      <td>-0.663503</td>\n",
       "      <td>1.001089</td>\n",
       "      <td>-0.108919</td>\n",
       "      <td>50.039808</td>\n",
       "      <td>-3.184871e-09</td>\n",
       "      <td>0.290258</td>\n",
       "      <td>0.483799</td>\n",
       "      <td>5.914137e+07</td>\n",
       "      <td>0.185763</td>\n",
       "      <td>50.449628</td>\n",
       "      <td>-0.115513</td>\n",
       "      <td>1.007942</td>\n",
       "      <td>0.243162</td>\n",
       "      <td>0.759373</td>\n",
       "      <td>0.331662</td>\n",
       "      <td>0.609077</td>\n",
       "      <td>49.199715</td>\n",
       "      <td>-0.500902</td>\n",
       "      <td>30.954515</td>\n",
       "      <td>1.455383e+07</td>\n",
       "      <td>50.393842</td>\n",
       "      <td>-0.488100</td>\n",
       "      <td>3.676714e+07</td>\n",
       "      <td>0.280926</td>\n",
       "      <td>0.409037</td>\n",
       "      <td>-1.077064</td>\n",
       "      <td>0.746239</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>-2.915563e-09</td>\n",
       "      <td>-0.493496</td>\n",
       "      <td>-0.261731</td>\n",
       "      <td>-0.710929</td>\n",
       "      <td>2.021408</td>\n",
       "      <td>5.635388e+07</td>\n",
       "      <td>7.837346e+07</td>\n",
       "      <td>49.402508</td>\n",
       "      <td>5.831073e+06</td>\n",
       "      <td>-0.391971</td>\n",
       "      <td>-0.487776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.280616</td>\n",
       "      <td>0.287521</td>\n",
       "      <td>0.287922</td>\n",
       "      <td>0.279544</td>\n",
       "      <td>0.279479</td>\n",
       "      <td>0.279546</td>\n",
       "      <td>0.285094</td>\n",
       "      <td>0.281948</td>\n",
       "      <td>0.280955</td>\n",
       "      <td>0.283733</td>\n",
       "      <td>0.284231</td>\n",
       "      <td>0.281365</td>\n",
       "      <td>0.279859</td>\n",
       "      <td>0.283832</td>\n",
       "      <td>0.282161</td>\n",
       "      <td>0.287499</td>\n",
       "      <td>0.248713</td>\n",
       "      <td>0.283041</td>\n",
       "      <td>0.283621</td>\n",
       "      <td>0.282584</td>\n",
       "      <td>0.284114</td>\n",
       "      <td>0.262105</td>\n",
       "      <td>0.283463</td>\n",
       "      <td>0.279703</td>\n",
       "      <td>0.282508</td>\n",
       "      <td>0.281604</td>\n",
       "      <td>0.283753</td>\n",
       "      <td>0.268928</td>\n",
       "      <td>0.278000</td>\n",
       "      <td>0.284065</td>\n",
       "      <td>0.283733</td>\n",
       "      <td>0.282938</td>\n",
       "      <td>0.282864</td>\n",
       "      <td>0.282921</td>\n",
       "      <td>0.283047</td>\n",
       "      <td>0.283377</td>\n",
       "      <td>0.279450</td>\n",
       "      <td>0.285527</td>\n",
       "      <td>0.276477</td>\n",
       "      <td>0.287558</td>\n",
       "      <td>0.281320</td>\n",
       "      <td>0.282607</td>\n",
       "      <td>0.278896</td>\n",
       "      <td>0.281320</td>\n",
       "      <td>0.282481</td>\n",
       "      <td>0.282715</td>\n",
       "      <td>0.214687</td>\n",
       "      <td>0.287737</td>\n",
       "      <td>0.285473</td>\n",
       "      <td>0.285675</td>\n",
       "      <td>0.285031</td>\n",
       "      <td>0.286271</td>\n",
       "      <td>0.287094</td>\n",
       "      <td>0.286680</td>\n",
       "      <td>0.284221</td>\n",
       "      <td>0.276641</td>\n",
       "      <td>0.285172</td>\n",
       "      <td>0.266469</td>\n",
       "      <td>0.243442</td>\n",
       "      <td>0.287395</td>\n",
       "      <td>0.279656</td>\n",
       "      <td>0.260295</td>\n",
       "      <td>0.279814</td>\n",
       "      <td>0.289319</td>\n",
       "      <td>0.281462</td>\n",
       "      <td>0.282252</td>\n",
       "      <td>0.278537</td>\n",
       "      <td>0.292695</td>\n",
       "      <td>0.283499</td>\n",
       "      <td>0.282085</td>\n",
       "      <td>0.283327</td>\n",
       "      <td>0.279313</td>\n",
       "      <td>0.282970</td>\n",
       "      <td>0.268991</td>\n",
       "      <td>0.284399</td>\n",
       "      <td>0.280397</td>\n",
       "      <td>0.280898</td>\n",
       "      <td>0.268009</td>\n",
       "      <td>0.275578</td>\n",
       "      <td>0.288196</td>\n",
       "      <td>0.281112</td>\n",
       "      <td>0.274419</td>\n",
       "      <td>0.277719</td>\n",
       "      <td>0.277797</td>\n",
       "      <td>0.277784</td>\n",
       "      <td>0.282438</td>\n",
       "      <td>0.285541</td>\n",
       "      <td>0.274599</td>\n",
       "      <td>0.280263</td>\n",
       "      <td>0.274226</td>\n",
       "      <td>0.288175</td>\n",
       "      <td>0.277972</td>\n",
       "      <td>0.281345</td>\n",
       "      <td>0.285495</td>\n",
       "      <td>0.288076</td>\n",
       "      <td>0.285541</td>\n",
       "      <td>0.273135</td>\n",
       "      <td>0.276758</td>\n",
       "      <td>0.277270</td>\n",
       "      <td>0.275978</td>\n",
       "      <td>0.276921</td>\n",
       "      <td>0.260151</td>\n",
       "      <td>0.263748</td>\n",
       "      <td>0.260686</td>\n",
       "      <td>0.282622</td>\n",
       "      <td>0.272721</td>\n",
       "      <td>0.267021</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>0.254090</td>\n",
       "      <td>0.278071</td>\n",
       "      <td>0.269587</td>\n",
       "      <td>0.282819</td>\n",
       "      <td>0.282805</td>\n",
       "      <td>0.282327</td>\n",
       "      <td>0.282754</td>\n",
       "      <td>0.282709</td>\n",
       "      <td>0.282848</td>\n",
       "      <td>0.282574</td>\n",
       "      <td>0.283430</td>\n",
       "      <td>0.283521</td>\n",
       "      <td>0.277707</td>\n",
       "      <td>0.277874</td>\n",
       "      <td>0.279933</td>\n",
       "      <td>0.280881</td>\n",
       "      <td>0.255937</td>\n",
       "      <td>0.277619</td>\n",
       "      <td>0.283867</td>\n",
       "      <td>0.290236</td>\n",
       "      <td>0.281537</td>\n",
       "      <td>0.277262</td>\n",
       "      <td>0.276386</td>\n",
       "      <td>0.297061</td>\n",
       "      <td>0.295741</td>\n",
       "      <td>0.278927</td>\n",
       "      <td>0.272996</td>\n",
       "      <td>0.277707</td>\n",
       "      <td>0.283391</td>\n",
       "      <td>0.289927</td>\n",
       "      <td>0.279385</td>\n",
       "      <td>0.276297</td>\n",
       "      <td>0.286139</td>\n",
       "      <td>0.284354</td>\n",
       "      <td>0.274460</td>\n",
       "      <td>0.293991</td>\n",
       "      <td>0.298060</td>\n",
       "      <td>0.294630</td>\n",
       "      <td>0.278675</td>\n",
       "      <td>0.286019</td>\n",
       "      <td>0.282093</td>\n",
       "      <td>0.274487</td>\n",
       "      <td>0.283217</td>\n",
       "      <td>0.278127</td>\n",
       "      <td>0.285076</td>\n",
       "      <td>0.259603</td>\n",
       "      <td>0.256559</td>\n",
       "      <td>0.277274</td>\n",
       "      <td>0.277274</td>\n",
       "      <td>0.276006</td>\n",
       "      <td>0.281799</td>\n",
       "      <td>0.259071</td>\n",
       "      <td>0.283102</td>\n",
       "      <td>0.285554</td>\n",
       "      <td>0.284804</td>\n",
       "      <td>0.280797</td>\n",
       "      <td>0.279429</td>\n",
       "      <td>0.264359</td>\n",
       "      <td>0.281526</td>\n",
       "      <td>0.282576</td>\n",
       "      <td>0.278976</td>\n",
       "      <td>0.282652</td>\n",
       "      <td>0.281039</td>\n",
       "      <td>0.281100</td>\n",
       "      <td>0.238720</td>\n",
       "      <td>0.236241</td>\n",
       "      <td>0.237809</td>\n",
       "      <td>0.277623</td>\n",
       "      <td>0.284713</td>\n",
       "      <td>0.284828</td>\n",
       "      <td>0.284471</td>\n",
       "      <td>0.293389</td>\n",
       "      <td>0.280403</td>\n",
       "      <td>0.286388</td>\n",
       "      <td>0.283484</td>\n",
       "      <td>0.227620</td>\n",
       "      <td>0.239152</td>\n",
       "      <td>0.276148</td>\n",
       "      <td>0.269425</td>\n",
       "      <td>0.283483</td>\n",
       "      <td>0.278264</td>\n",
       "      <td>0.283646</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.282795</td>\n",
       "      <td>0.282839</td>\n",
       "      <td>0.287799</td>\n",
       "      <td>0.287862</td>\n",
       "      <td>0.282105</td>\n",
       "      <td>0.282728</td>\n",
       "      <td>0.282843</td>\n",
       "      <td>0.282447</td>\n",
       "      <td>0.287670</td>\n",
       "      <td>0.291975</td>\n",
       "      <td>0.285368</td>\n",
       "      <td>0.281940</td>\n",
       "      <td>0.287465</td>\n",
       "      <td>0.287523</td>\n",
       "      <td>0.283244</td>\n",
       "      <td>0.227620</td>\n",
       "      <td>0.263303</td>\n",
       "      <td>0.266878</td>\n",
       "      <td>0.271885</td>\n",
       "      <td>0.227620</td>\n",
       "      <td>0.275401</td>\n",
       "      <td>0.289141</td>\n",
       "      <td>0.278170</td>\n",
       "      <td>0.285318</td>\n",
       "      <td>0.281347</td>\n",
       "      <td>0.278127</td>\n",
       "      <td>0.275714</td>\n",
       "      <td>0.287912</td>\n",
       "      <td>0.270107</td>\n",
       "      <td>0.272812</td>\n",
       "      <td>0.278037</td>\n",
       "      <td>0.283747</td>\n",
       "      <td>0.267908</td>\n",
       "      <td>0.239153</td>\n",
       "      <td>0.273867</td>\n",
       "      <td>0.278693</td>\n",
       "      <td>0.284718</td>\n",
       "      <td>0.290790</td>\n",
       "      <td>0.296847</td>\n",
       "      <td>0.289999</td>\n",
       "      <td>0.286112</td>\n",
       "      <td>0.277793</td>\n",
       "      <td>0.278367</td>\n",
       "      <td>0.287267</td>\n",
       "      <td>0.264091</td>\n",
       "      <td>0.278120</td>\n",
       "      <td>0.273062</td>\n",
       "      <td>0.266760</td>\n",
       "      <td>0.256789</td>\n",
       "      <td>0.278170</td>\n",
       "      <td>0.264985</td>\n",
       "      <td>0.251019</td>\n",
       "      <td>0.284026</td>\n",
       "      <td>0.283965</td>\n",
       "      <td>0.283083</td>\n",
       "      <td>0.279727</td>\n",
       "      <td>0.279630</td>\n",
       "      <td>0.279692</td>\n",
       "      <td>0.288430</td>\n",
       "      <td>0.281189</td>\n",
       "      <td>0.283658</td>\n",
       "      <td>0.282442</td>\n",
       "      <td>0.277743</td>\n",
       "      <td>0.268366</td>\n",
       "      <td>0.282796</td>\n",
       "      <td>0.280295</td>\n",
       "      <td>0.275072</td>\n",
       "      <td>0.280302</td>\n",
       "      <td>0.272662</td>\n",
       "      <td>0.279747</td>\n",
       "      <td>0.279155</td>\n",
       "      <td>0.277770</td>\n",
       "      <td>0.277673</td>\n",
       "      <td>0.278694</td>\n",
       "      <td>0.269683</td>\n",
       "      <td>0.259308</td>\n",
       "      <td>0.281553</td>\n",
       "      <td>0.281506</td>\n",
       "      <td>0.279862</td>\n",
       "      <td>0.278360</td>\n",
       "      <td>0.278216</td>\n",
       "      <td>0.279353</td>\n",
       "      <td>0.277994</td>\n",
       "      <td>0.270367</td>\n",
       "      <td>0.256100</td>\n",
       "      <td>0.276908</td>\n",
       "      <td>0.282378</td>\n",
       "      <td>0.275475</td>\n",
       "      <td>0.279597</td>\n",
       "      <td>0.280984</td>\n",
       "      <td>0.287558</td>\n",
       "      <td>0.287537</td>\n",
       "      <td>0.281106</td>\n",
       "      <td>0.281027</td>\n",
       "      <td>0.276443</td>\n",
       "      <td>0.271164</td>\n",
       "      <td>0.291159</td>\n",
       "      <td>0.292505</td>\n",
       "      <td>0.291832</td>\n",
       "      <td>0.277398</td>\n",
       "      <td>0.282949</td>\n",
       "      <td>0.283915</td>\n",
       "      <td>0.283814</td>\n",
       "      <td>0.278319</td>\n",
       "      <td>0.282996</td>\n",
       "      <td>0.282878</td>\n",
       "      <td>0.268183</td>\n",
       "      <td>0.282717</td>\n",
       "      <td>0.279658</td>\n",
       "      <td>0.282632</td>\n",
       "      <td>0.282756</td>\n",
       "      <td>0.275431</td>\n",
       "      <td>0.282837</td>\n",
       "      <td>0.283164</td>\n",
       "      <td>0.282894</td>\n",
       "      <td>0.281511</td>\n",
       "      <td>0.281492</td>\n",
       "      <td>0.263891</td>\n",
       "      <td>0.282837</td>\n",
       "      <td>0.262871</td>\n",
       "      <td>0.276294</td>\n",
       "      <td>0.276352</td>\n",
       "      <td>0.282698</td>\n",
       "      <td>0.282605</td>\n",
       "      <td>0.274739</td>\n",
       "      <td>0.260386</td>\n",
       "      <td>0.275910</td>\n",
       "      <td>0.276735</td>\n",
       "      <td>0.275056</td>\n",
       "      <td>0.276407</td>\n",
       "      <td>0.275234</td>\n",
       "      <td>0.275048</td>\n",
       "      <td>0.284046</td>\n",
       "      <td>0.282002</td>\n",
       "      <td>0.282532</td>\n",
       "      <td>0.275131</td>\n",
       "      <td>0.285146</td>\n",
       "      <td>0.282853</td>\n",
       "      <td>0.282813</td>\n",
       "      <td>0.261306</td>\n",
       "      <td>0.256189</td>\n",
       "      <td>0.276614</td>\n",
       "      <td>0.262618</td>\n",
       "      <td>0.282443</td>\n",
       "      <td>0.276551</td>\n",
       "      <td>0.275453</td>\n",
       "      <td>0.282733</td>\n",
       "      <td>0.283812</td>\n",
       "      <td>0.278039</td>\n",
       "      <td>0.276181</td>\n",
       "      <td>0.275870</td>\n",
       "      <td>0.282596</td>\n",
       "      <td>0.280907</td>\n",
       "      <td>0.281187</td>\n",
       "      <td>0.262463</td>\n",
       "      <td>0.284423</td>\n",
       "      <td>0.269923</td>\n",
       "      <td>0.284763</td>\n",
       "      <td>0.290508</td>\n",
       "      <td>0.281607</td>\n",
       "      <td>0.284069</td>\n",
       "      <td>0.283202</td>\n",
       "      <td>0.286517</td>\n",
       "      <td>0.271747</td>\n",
       "      <td>0.277145</td>\n",
       "      <td>0.285942</td>\n",
       "      <td>0.287841</td>\n",
       "      <td>0.280940</td>\n",
       "      <td>0.266132</td>\n",
       "      <td>0.239645</td>\n",
       "      <td>0.282843</td>\n",
       "      <td>0.282857</td>\n",
       "      <td>0.273201</td>\n",
       "      <td>0.281030</td>\n",
       "      <td>0.281060</td>\n",
       "      <td>0.264489</td>\n",
       "      <td>0.283338</td>\n",
       "      <td>0.282956</td>\n",
       "      <td>0.262349</td>\n",
       "      <td>0.276139</td>\n",
       "      <td>0.278937</td>\n",
       "      <td>0.264451</td>\n",
       "      <td>0.265726</td>\n",
       "      <td>0.270956</td>\n",
       "      <td>0.273480</td>\n",
       "      <td>0.277037</td>\n",
       "      <td>0.286589</td>\n",
       "      <td>0.283462</td>\n",
       "      <td>0.275543</td>\n",
       "      <td>0.248716</td>\n",
       "      <td>0.279660</td>\n",
       "      <td>0.273085</td>\n",
       "      <td>0.248405</td>\n",
       "      <td>0.278344</td>\n",
       "      <td>0.273504</td>\n",
       "      <td>0.277922</td>\n",
       "      <td>0.281278</td>\n",
       "      <td>0.279772</td>\n",
       "      <td>0.283628</td>\n",
       "      <td>0.284325</td>\n",
       "      <td>0.283965</td>\n",
       "      <td>0.275742</td>\n",
       "      <td>0.284317</td>\n",
       "      <td>0.291401</td>\n",
       "      <td>0.291815</td>\n",
       "      <td>0.275162</td>\n",
       "      <td>0.275173</td>\n",
       "      <td>0.275396</td>\n",
       "      <td>0.275216</td>\n",
       "      <td>0.292839</td>\n",
       "      <td>0.276680</td>\n",
       "      <td>0.274512</td>\n",
       "      <td>0.276102</td>\n",
       "      <td>0.274798</td>\n",
       "      <td>0.277240</td>\n",
       "      <td>0.275435</td>\n",
       "      <td>0.285019</td>\n",
       "      <td>0.283094</td>\n",
       "      <td>0.283519</td>\n",
       "      <td>0.277751</td>\n",
       "      <td>0.277850</td>\n",
       "      <td>0.291530</td>\n",
       "      <td>0.276843</td>\n",
       "      <td>0.281987</td>\n",
       "      <td>0.279816</td>\n",
       "      <td>0.286348</td>\n",
       "      <td>0.282612</td>\n",
       "      <td>0.280333</td>\n",
       "      <td>0.291946</td>\n",
       "      <td>0.284026</td>\n",
       "      <td>0.280474</td>\n",
       "      <td>0.284575</td>\n",
       "      <td>294946.072604</td>\n",
       "      <td>0.027588</td>\n",
       "      <td>0.501355</td>\n",
       "      <td>0.285796</td>\n",
       "      <td>1.007634e+07</td>\n",
       "      <td>0.436854</td>\n",
       "      <td>24.523371</td>\n",
       "      <td>34.073537</td>\n",
       "      <td>0.361012</td>\n",
       "      <td>0.423113</td>\n",
       "      <td>2.228564</td>\n",
       "      <td>1.917752e-01</td>\n",
       "      <td>0.226502</td>\n",
       "      <td>0.159938</td>\n",
       "      <td>3.758588e+07</td>\n",
       "      <td>42.780168</td>\n",
       "      <td>6.866809e+06</td>\n",
       "      <td>0.327249</td>\n",
       "      <td>0.327249</td>\n",
       "      <td>2.510209</td>\n",
       "      <td>4.701915</td>\n",
       "      <td>4.701915</td>\n",
       "      <td>0.145086</td>\n",
       "      <td>45.555559</td>\n",
       "      <td>0.407859</td>\n",
       "      <td>0.236466</td>\n",
       "      <td>41.549432</td>\n",
       "      <td>0.264249</td>\n",
       "      <td>0.008653</td>\n",
       "      <td>0.435249</td>\n",
       "      <td>1.479433</td>\n",
       "      <td>24.945268</td>\n",
       "      <td>10.410254</td>\n",
       "      <td>4.606471</td>\n",
       "      <td>18.320454</td>\n",
       "      <td>2.025688</td>\n",
       "      <td>0.094080</td>\n",
       "      <td>54.049711</td>\n",
       "      <td>0.273453</td>\n",
       "      <td>2.412641e+08</td>\n",
       "      <td>60.136028</td>\n",
       "      <td>5.961999e+06</td>\n",
       "      <td>0.104143</td>\n",
       "      <td>0.245590</td>\n",
       "      <td>9.296278</td>\n",
       "      <td>0.431807</td>\n",
       "      <td>0.412699</td>\n",
       "      <td>1.119579</td>\n",
       "      <td>0.245858</td>\n",
       "      <td>0.296845</td>\n",
       "      <td>1.833326e-01</td>\n",
       "      <td>0.115226</td>\n",
       "      <td>5.209246e-10</td>\n",
       "      <td>32.000689</td>\n",
       "      <td>12.047502</td>\n",
       "      <td>4.860489</td>\n",
       "      <td>0.397186</td>\n",
       "      <td>0.302216</td>\n",
       "      <td>0.011237</td>\n",
       "      <td>2.502416e+08</td>\n",
       "      <td>1.752965</td>\n",
       "      <td>2.020973e-02</td>\n",
       "      <td>10.347183</td>\n",
       "      <td>1.589973</td>\n",
       "      <td>1.819403e+06</td>\n",
       "      <td>0.179449</td>\n",
       "      <td>0.376467</td>\n",
       "      <td>0.023840</td>\n",
       "      <td>1142.587105</td>\n",
       "      <td>0.228052</td>\n",
       "      <td>0.220396</td>\n",
       "      <td>0.549876</td>\n",
       "      <td>0.380183</td>\n",
       "      <td>0.298284</td>\n",
       "      <td>2.096202e+08</td>\n",
       "      <td>68.383088</td>\n",
       "      <td>0.005870</td>\n",
       "      <td>1.941630</td>\n",
       "      <td>0.612612</td>\n",
       "      <td>0.055371</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.352034</td>\n",
       "      <td>7.557312</td>\n",
       "      <td>19.884465</td>\n",
       "      <td>11.061382</td>\n",
       "      <td>0.221431</td>\n",
       "      <td>0.564735</td>\n",
       "      <td>0.512424</td>\n",
       "      <td>34.600474</td>\n",
       "      <td>1.355707e+06</td>\n",
       "      <td>2.311085e-01</td>\n",
       "      <td>0.065763</td>\n",
       "      <td>1.414257e+07</td>\n",
       "      <td>56.657688</td>\n",
       "      <td>0.169722</td>\n",
       "      <td>30.394664</td>\n",
       "      <td>0.391332</td>\n",
       "      <td>0.282178</td>\n",
       "      <td>16.623761</td>\n",
       "      <td>4.467129</td>\n",
       "      <td>38.910087</td>\n",
       "      <td>0.432321</td>\n",
       "      <td>0.047686</td>\n",
       "      <td>0.795230</td>\n",
       "      <td>0.710875</td>\n",
       "      <td>0.875550</td>\n",
       "      <td>7.043155</td>\n",
       "      <td>4.107713</td>\n",
       "      <td>0.015821</td>\n",
       "      <td>12.810531</td>\n",
       "      <td>0.895208</td>\n",
       "      <td>0.482055</td>\n",
       "      <td>0.676291</td>\n",
       "      <td>7.316449</td>\n",
       "      <td>69.622534</td>\n",
       "      <td>3.991377e+06</td>\n",
       "      <td>1.009495</td>\n",
       "      <td>4.553998</td>\n",
       "      <td>5.497908</td>\n",
       "      <td>0.657339</td>\n",
       "      <td>0.425148</td>\n",
       "      <td>0.057211</td>\n",
       "      <td>0.234413</td>\n",
       "      <td>0.288362</td>\n",
       "      <td>0.282603</td>\n",
       "      <td>0.455767</td>\n",
       "      <td>0.403006</td>\n",
       "      <td>0.961521</td>\n",
       "      <td>13.231738</td>\n",
       "      <td>0.285265</td>\n",
       "      <td>0.259150</td>\n",
       "      <td>6.410925e+07</td>\n",
       "      <td>0.509358</td>\n",
       "      <td>0.232961</td>\n",
       "      <td>0.046933</td>\n",
       "      <td>17.869932</td>\n",
       "      <td>0.089054</td>\n",
       "      <td>0.199480</td>\n",
       "      <td>0.403665</td>\n",
       "      <td>0.397871</td>\n",
       "      <td>0.199480</td>\n",
       "      <td>44.495613</td>\n",
       "      <td>16.127809</td>\n",
       "      <td>0.284351</td>\n",
       "      <td>55.793806</td>\n",
       "      <td>0.392233</td>\n",
       "      <td>2.287423e+01</td>\n",
       "      <td>13.192576</td>\n",
       "      <td>2.426699</td>\n",
       "      <td>0.204989</td>\n",
       "      <td>6.866239e+07</td>\n",
       "      <td>0.238425</td>\n",
       "      <td>0.524144</td>\n",
       "      <td>4.892969e+14</td>\n",
       "      <td>0.240565</td>\n",
       "      <td>0.036543</td>\n",
       "      <td>3.654281</td>\n",
       "      <td>22.801326</td>\n",
       "      <td>3.814653e-07</td>\n",
       "      <td>0.427406</td>\n",
       "      <td>0.355527</td>\n",
       "      <td>9.546036e+07</td>\n",
       "      <td>8.140531</td>\n",
       "      <td>15.603696</td>\n",
       "      <td>0.445954</td>\n",
       "      <td>0.427054</td>\n",
       "      <td>0.116788</td>\n",
       "      <td>0.185281</td>\n",
       "      <td>0.227638</td>\n",
       "      <td>100.583653</td>\n",
       "      <td>24.818349</td>\n",
       "      <td>0.282431</td>\n",
       "      <td>530.272278</td>\n",
       "      <td>1.940660e+07</td>\n",
       "      <td>15.319835</td>\n",
       "      <td>0.278556</td>\n",
       "      <td>1.245679e+08</td>\n",
       "      <td>0.238676</td>\n",
       "      <td>0.629594</td>\n",
       "      <td>0.434089</td>\n",
       "      <td>13.158720</td>\n",
       "      <td>0.328509</td>\n",
       "      <td>5.507456e-07</td>\n",
       "      <td>0.288568</td>\n",
       "      <td>0.217823</td>\n",
       "      <td>0.256234</td>\n",
       "      <td>2.832495</td>\n",
       "      <td>1.582944e+08</td>\n",
       "      <td>1.108456e+08</td>\n",
       "      <td>20.310674</td>\n",
       "      <td>9.322911e+06</td>\n",
       "      <td>1.554516</td>\n",
       "      <td>0.280986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.009709</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.009009</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.009009</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.009009</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.009009</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.009009</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.009009</td>\n",
       "      <td>0.009009</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-0.104500</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>3.684226e+04</td>\n",
       "      <td>-0.998479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-1.932000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-67.420000</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.082629</td>\n",
       "      <td>-1.717333e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.264836e+08</td>\n",
       "      <td>-0.994187</td>\n",
       "      <td>-0.994187</td>\n",
       "      <td>-126.708393</td>\n",
       "      <td>-4.336789</td>\n",
       "      <td>-4.336789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.197970</td>\n",
       "      <td>-0.988011</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.635722</td>\n",
       "      <td>-0.998412</td>\n",
       "      <td>-222.743588</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.665852</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>7.116736e+05</td>\n",
       "      <td>-95.000000</td>\n",
       "      <td>-1.322888e+08</td>\n",
       "      <td>0.358132</td>\n",
       "      <td>-0.988153</td>\n",
       "      <td>-307.455821</td>\n",
       "      <td>-0.856300</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-50.610000</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-9.567258e-01</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>3.868936e-12</td>\n",
       "      <td>-330.023568</td>\n",
       "      <td>-151.644060</td>\n",
       "      <td>-4.620615</td>\n",
       "      <td>-0.993368</td>\n",
       "      <td>-0.443523</td>\n",
       "      <td>-0.275309</td>\n",
       "      <td>1.559092e+05</td>\n",
       "      <td>-45.610559</td>\n",
       "      <td>-4.714748e-01</td>\n",
       "      <td>0.252917</td>\n",
       "      <td>-60.917557</td>\n",
       "      <td>-1.057028e+08</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.200281</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-14477.870518</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-640.835341</td>\n",
       "      <td>-0.039300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-17.523487</td>\n",
       "      <td>-1.477141</td>\n",
       "      <td>2.479430e-56</td>\n",
       "      <td>-0.999982</td>\n",
       "      <td>-464.128134</td>\n",
       "      <td>1.335389</td>\n",
       "      <td>-0.916975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010762</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-5.985623e+07</td>\n",
       "      <td>4.128967e-07</td>\n",
       "      <td>0.535885</td>\n",
       "      <td>-7.672276e+08</td>\n",
       "      <td>-1152.865522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-289.303474</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>3.730972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>-0.617021</td>\n",
       "      <td>-5.083281</td>\n",
       "      <td>-12.300213</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-46.861925</td>\n",
       "      <td>-7.283429</td>\n",
       "      <td>-0.120330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-35.056320</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-33.912013</td>\n",
       "      <td>-59.188854</td>\n",
       "      <td>-148.862760</td>\n",
       "      <td>-2.769467e+08</td>\n",
       "      <td>-35.170000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-40.387293</td>\n",
       "      <td>-2.999030</td>\n",
       "      <td>-19.471768</td>\n",
       "      <td>0.624556</td>\n",
       "      <td>-0.997873</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-31.980000</td>\n",
       "      <td>-0.992915</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.011860</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.990058</td>\n",
       "      <td>-1.586135e+09</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.732932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.886676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1724.149728</td>\n",
       "      <td>-0.999518</td>\n",
       "      <td>6.702527e-21</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-86.607696</td>\n",
       "      <td>-0.995742</td>\n",
       "      <td>-1.132213e+09</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-4.955000e+06</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.797368</td>\n",
       "      <td>-32.283465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.393257e-05</td>\n",
       "      <td>-0.984802</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>1.645243e+05</td>\n",
       "      <td>-53.325438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.999723</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269248</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>-393.811533</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-98.131314</td>\n",
       "      <td>8.200000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-2.010123e+09</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>-8.096986</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-64.213564</td>\n",
       "      <td>-11.150667</td>\n",
       "      <td>-5.542381e-05</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.981646</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.698386e+09</td>\n",
       "      <td>4.162544e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.041156e+04</td>\n",
       "      <td>-69.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.258929</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.303922</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.252874</td>\n",
       "      <td>0.254902</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.267857</td>\n",
       "      <td>0.267857</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.276316</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.288090</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.286335</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.290909</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.252874</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.298969</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.390805</td>\n",
       "      <td>0.261364</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.281690</td>\n",
       "      <td>0.284091</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.253521</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.265957</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.278481</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.256410</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>0.257353</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.271028</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.241935</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.230675</td>\n",
       "      <td>0.264368</td>\n",
       "      <td>0.284211</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.284091</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.261364</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.267857</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.261364</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.290909</td>\n",
       "      <td>0.275229</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.252874</td>\n",
       "      <td>0.244318</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.283019</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.273684</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.297297</td>\n",
       "      <td>0.297872</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.298851</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.306818</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.320755</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>0.258621</td>\n",
       "      <td>0.252174</td>\n",
       "      <td>0.276316</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.276190</td>\n",
       "      <td>0.247367</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.306818</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.280899</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.283019</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.226316</td>\n",
       "      <td>0.215116</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.252336</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.252874</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.274194</td>\n",
       "      <td>0.364706</td>\n",
       "      <td>0.369932</td>\n",
       "      <td>0.302752</td>\n",
       "      <td>0.302752</td>\n",
       "      <td>0.239583</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.257576</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.311927</td>\n",
       "      <td>0.319588</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.297297</td>\n",
       "      <td>0.267857</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.213115</td>\n",
       "      <td>0.256250</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>0.267857</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.267442</td>\n",
       "      <td>0.390805</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.258621</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.253626</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.256410</td>\n",
       "      <td>0.257576</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.296875</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.298851</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.256637</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.294737</td>\n",
       "      <td>0.258427</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.267442</td>\n",
       "      <td>0.390805</td>\n",
       "      <td>0.354167</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>0.390805</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.290698</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.281818</td>\n",
       "      <td>0.266055</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>0.295775</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.290909</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.236364</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.229885</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.359223</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.295082</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.290698</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.319149</td>\n",
       "      <td>0.319149</td>\n",
       "      <td>0.319149</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>0.270588</td>\n",
       "      <td>0.274194</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.281818</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.294690</td>\n",
       "      <td>0.278689</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.269663</td>\n",
       "      <td>0.298851</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.285099</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.267857</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.280488</td>\n",
       "      <td>0.270270</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.258621</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.242105</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.252525</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.252336</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.273585</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.311966</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.293103</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.317460</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.298246</td>\n",
       "      <td>0.261682</td>\n",
       "      <td>0.247706</td>\n",
       "      <td>0.252632</td>\n",
       "      <td>0.243119</td>\n",
       "      <td>0.261682</td>\n",
       "      <td>0.261682</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.320755</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.280374</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.329114</td>\n",
       "      <td>0.268817</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.273684</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.397849</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.284211</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.261682</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.350877</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.263736</td>\n",
       "      <td>0.340426</td>\n",
       "      <td>0.336842</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.261364</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.288462</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.280374</td>\n",
       "      <td>0.327586</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.295082</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.261682</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.258621</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>0.278689</td>\n",
       "      <td>0.252632</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.266092</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.266055</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.266006</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.295918</td>\n",
       "      <td>0.253165</td>\n",
       "      <td>0.256410</td>\n",
       "      <td>0.252874</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.269663</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.254902</td>\n",
       "      <td>2056.000000</td>\n",
       "      <td>-0.013000</td>\n",
       "      <td>-0.634667</td>\n",
       "      <td>0.253371</td>\n",
       "      <td>1.840736e+06</td>\n",
       "      <td>-0.301256</td>\n",
       "      <td>29.338233</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>-0.122688</td>\n",
       "      <td>-0.505352</td>\n",
       "      <td>-0.540000</td>\n",
       "      <td>-2.135434e-01</td>\n",
       "      <td>-0.918832</td>\n",
       "      <td>0.912681</td>\n",
       "      <td>-8.894561e+06</td>\n",
       "      <td>73.385013</td>\n",
       "      <td>-1.284391e+06</td>\n",
       "      <td>-0.241972</td>\n",
       "      <td>-0.241972</td>\n",
       "      <td>-0.158779</td>\n",
       "      <td>-0.021376</td>\n",
       "      <td>-0.021376</td>\n",
       "      <td>0.015121</td>\n",
       "      <td>82.051282</td>\n",
       "      <td>-0.284206</td>\n",
       "      <td>-0.362031</td>\n",
       "      <td>0.549622</td>\n",
       "      <td>-0.851163</td>\n",
       "      <td>-0.001338</td>\n",
       "      <td>-0.290442</td>\n",
       "      <td>-0.127633</td>\n",
       "      <td>0.516303</td>\n",
       "      <td>5.316616</td>\n",
       "      <td>2.704623</td>\n",
       "      <td>42.655768</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>-0.022878</td>\n",
       "      <td>0.509570</td>\n",
       "      <td>0.506694</td>\n",
       "      <td>4.668008e+07</td>\n",
       "      <td>-55.000000</td>\n",
       "      <td>-2.679149e+05</td>\n",
       "      <td>0.947339</td>\n",
       "      <td>-0.262650</td>\n",
       "      <td>-2.202214</td>\n",
       "      <td>-0.124093</td>\n",
       "      <td>-0.560181</td>\n",
       "      <td>-0.261953</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>-0.764758</td>\n",
       "      <td>-1.852866e-01</td>\n",
       "      <td>0.111523</td>\n",
       "      <td>1.129987e-10</td>\n",
       "      <td>-24.511799</td>\n",
       "      <td>-0.359421</td>\n",
       "      <td>-0.021065</td>\n",
       "      <td>-0.264296</td>\n",
       "      <td>0.787256</td>\n",
       "      <td>-0.004980</td>\n",
       "      <td>2.867418e+07</td>\n",
       "      <td>-0.435896</td>\n",
       "      <td>-1.050869e-02</td>\n",
       "      <td>5.339610</td>\n",
       "      <td>-0.972999</td>\n",
       "      <td>-3.248809e+05</td>\n",
       "      <td>-0.861652</td>\n",
       "      <td>0.670781</td>\n",
       "      <td>0.022059</td>\n",
       "      <td>-3192.392645</td>\n",
       "      <td>-0.914838</td>\n",
       "      <td>0.043823</td>\n",
       "      <td>0.168435</td>\n",
       "      <td>0.108062</td>\n",
       "      <td>0.229227</td>\n",
       "      <td>8.214782e+02</td>\n",
       "      <td>31.131695</td>\n",
       "      <td>-0.003030</td>\n",
       "      <td>0.417991</td>\n",
       "      <td>-1.247281</td>\n",
       "      <td>-0.011536</td>\n",
       "      <td>9.083300e-01</td>\n",
       "      <td>-0.532839</td>\n",
       "      <td>-2.775993</td>\n",
       "      <td>19.861183</td>\n",
       "      <td>6.961465</td>\n",
       "      <td>0.042157</td>\n",
       "      <td>0.164366</td>\n",
       "      <td>-0.068123</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>-8.686767e+04</td>\n",
       "      <td>6.055760e-02</td>\n",
       "      <td>0.967026</td>\n",
       "      <td>-8.710874e+06</td>\n",
       "      <td>-2.604084</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>12.116926</td>\n",
       "      <td>0.756651</td>\n",
       "      <td>0.275966</td>\n",
       "      <td>23.059354</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>-26.560031</td>\n",
       "      <td>0.088990</td>\n",
       "      <td>-0.060858</td>\n",
       "      <td>-0.643006</td>\n",
       "      <td>-0.480450</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>-3.480278</td>\n",
       "      <td>-0.032391</td>\n",
       "      <td>-0.008554</td>\n",
       "      <td>41.374340</td>\n",
       "      <td>-0.200754</td>\n",
       "      <td>-1.078191</td>\n",
       "      <td>0.704268</td>\n",
       "      <td>-3.445754</td>\n",
       "      <td>-6.595143</td>\n",
       "      <td>-1.904197e+05</td>\n",
       "      <td>-0.253904</td>\n",
       "      <td>1.275985</td>\n",
       "      <td>-2.694462</td>\n",
       "      <td>-1.919321</td>\n",
       "      <td>-0.040430</td>\n",
       "      <td>0.972631</td>\n",
       "      <td>-0.502144</td>\n",
       "      <td>0.241323</td>\n",
       "      <td>-0.743676</td>\n",
       "      <td>-0.029877</td>\n",
       "      <td>-0.289031</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.839073</td>\n",
       "      <td>-0.734996</td>\n",
       "      <td>-0.413066</td>\n",
       "      <td>-6.962079e+06</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.064896</td>\n",
       "      <td>0.978197</td>\n",
       "      <td>37.497602</td>\n",
       "      <td>-0.174892</td>\n",
       "      <td>0.346369</td>\n",
       "      <td>-0.917942</td>\n",
       "      <td>-0.302317</td>\n",
       "      <td>0.348377</td>\n",
       "      <td>74.106749</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>-0.733598</td>\n",
       "      <td>-20.405192</td>\n",
       "      <td>-0.265145</td>\n",
       "      <td>2.920154e+01</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>-0.471209</td>\n",
       "      <td>-0.643299</td>\n",
       "      <td>-7.891093e+06</td>\n",
       "      <td>-0.861775</td>\n",
       "      <td>-0.810775</td>\n",
       "      <td>-5.033621e+01</td>\n",
       "      <td>-0.868094</td>\n",
       "      <td>0.981504</td>\n",
       "      <td>-1.858974</td>\n",
       "      <td>30.667156</td>\n",
       "      <td>-2.387963e-09</td>\n",
       "      <td>-0.012195</td>\n",
       "      <td>0.178835</td>\n",
       "      <td>1.310954e+07</td>\n",
       "      <td>-4.064555</td>\n",
       "      <td>39.429423</td>\n",
       "      <td>-0.444342</td>\n",
       "      <td>0.707101</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.654237</td>\n",
       "      <td>0.140452</td>\n",
       "      <td>-57.858311</td>\n",
       "      <td>28.044940</td>\n",
       "      <td>-0.743186</td>\n",
       "      <td>-33.273201</td>\n",
       "      <td>4.416000e+06</td>\n",
       "      <td>39.265986</td>\n",
       "      <td>-0.720375</td>\n",
       "      <td>-7.025896e+06</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-0.039129</td>\n",
       "      <td>-1.390691</td>\n",
       "      <td>-6.716418</td>\n",
       "      <td>-0.072608</td>\n",
       "      <td>-2.441408e-09</td>\n",
       "      <td>-0.741728</td>\n",
       "      <td>-0.396303</td>\n",
       "      <td>-0.938311</td>\n",
       "      <td>0.666508</td>\n",
       "      <td>-4.254197e+06</td>\n",
       "      <td>2.099584e+07</td>\n",
       "      <td>32.337113</td>\n",
       "      <td>1.458137e+06</td>\n",
       "      <td>-0.525177</td>\n",
       "      <td>-0.723499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.505618</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.495327</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.490566</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.485981</td>\n",
       "      <td>0.508772</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.539683</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.508772</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.507042</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.537037</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.537037</td>\n",
       "      <td>0.552083</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.445455</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.505263</td>\n",
       "      <td>0.515789</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.508772</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.528302</td>\n",
       "      <td>0.494253</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.575221</td>\n",
       "      <td>0.507042</td>\n",
       "      <td>0.516854</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.570093</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.504854</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.467290</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.532258</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.549550</td>\n",
       "      <td>0.467391</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.435484</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.494253</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.550562</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.506494</td>\n",
       "      <td>0.508772</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.550562</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.474747</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>0.505618</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.492063</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.467742</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.435185</td>\n",
       "      <td>0.485981</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.495327</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.546392</td>\n",
       "      <td>0.510870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.402062</td>\n",
       "      <td>0.403226</td>\n",
       "      <td>0.486111</td>\n",
       "      <td>0.508197</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.507692</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.512500</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.494253</td>\n",
       "      <td>0.494949</td>\n",
       "      <td>0.487500</td>\n",
       "      <td>0.549451</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.511364</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.544643</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.584270</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.508197</td>\n",
       "      <td>0.525424</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.550459</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.491803</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.505495</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.520408</td>\n",
       "      <td>0.568966</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.513158</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.559140</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.559322</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.585859</td>\n",
       "      <td>0.511364</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.552083</td>\n",
       "      <td>0.537037</td>\n",
       "      <td>0.537037</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.508772</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.542056</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.482143</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0.467742</td>\n",
       "      <td>0.494118</td>\n",
       "      <td>0.495413</td>\n",
       "      <td>0.456140</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.504902</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.456140</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.474747</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.505155</td>\n",
       "      <td>0.535211</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.482456</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.491803</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.525316</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.516484</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.535354</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.505618</td>\n",
       "      <td>0.537634</td>\n",
       "      <td>0.536364</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.537037</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.473214</td>\n",
       "      <td>0.504587</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.580247</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.524272</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.508772</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.505495</td>\n",
       "      <td>0.537037</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.603448</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.542169</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.495146</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.508772</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.532258</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.569767</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>0.508772</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.495327</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.494253</td>\n",
       "      <td>0.485981</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>600037.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.291389</td>\n",
       "      <td>0.505663</td>\n",
       "      <td>3.707158e+06</td>\n",
       "      <td>0.004552</td>\n",
       "      <td>50.738646</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.205367</td>\n",
       "      <td>-0.010704</td>\n",
       "      <td>-8.885107e-02</td>\n",
       "      <td>-0.758957</td>\n",
       "      <td>0.986817</td>\n",
       "      <td>-6.312875e+05</td>\n",
       "      <td>96.955031</td>\n",
       "      <td>7.581923e+03</td>\n",
       "      <td>-0.043106</td>\n",
       "      <td>-0.043106</td>\n",
       "      <td>-0.003764</td>\n",
       "      <td>0.007285</td>\n",
       "      <td>0.007285</td>\n",
       "      <td>0.050364</td>\n",
       "      <td>107.541522</td>\n",
       "      <td>0.008712</td>\n",
       "      <td>-0.166654</td>\n",
       "      <td>1.095669</td>\n",
       "      <td>-0.666243</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.011361</td>\n",
       "      <td>0.006198</td>\n",
       "      <td>0.993902</td>\n",
       "      <td>8.218404</td>\n",
       "      <td>4.686986</td>\n",
       "      <td>55.938441</td>\n",
       "      <td>0.811983</td>\n",
       "      <td>-0.001564</td>\n",
       "      <td>1.019622</td>\n",
       "      <td>0.787801</td>\n",
       "      <td>9.199479e+07</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-3.516476e+03</td>\n",
       "      <td>0.998986</td>\n",
       "      <td>-0.089466</td>\n",
       "      <td>0.030137</td>\n",
       "      <td>0.179219</td>\n",
       "      <td>-0.290112</td>\n",
       "      <td>0.004805</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.506995</td>\n",
       "      <td>-6.581047e-02</td>\n",
       "      <td>0.165794</td>\n",
       "      <td>2.149803e-10</td>\n",
       "      <td>-3.317600</td>\n",
       "      <td>0.019002</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.992346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.632041e+07</td>\n",
       "      <td>-0.020012</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>8.240918</td>\n",
       "      <td>-0.334926</td>\n",
       "      <td>-9.523693e+02</td>\n",
       "      <td>-0.731159</td>\n",
       "      <td>0.958741</td>\n",
       "      <td>0.032186</td>\n",
       "      <td>-2446.004940</td>\n",
       "      <td>-0.782121</td>\n",
       "      <td>0.087857</td>\n",
       "      <td>0.291460</td>\n",
       "      <td>0.499494</td>\n",
       "      <td>0.498864</td>\n",
       "      <td>3.135596e+03</td>\n",
       "      <td>56.380612</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>0.802333</td>\n",
       "      <td>-0.898512</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>9.989138e-01</td>\n",
       "      <td>-0.280382</td>\n",
       "      <td>-0.968317</td>\n",
       "      <td>32.300023</td>\n",
       "      <td>10.289687</td>\n",
       "      <td>0.088637</td>\n",
       "      <td>0.288374</td>\n",
       "      <td>0.399593</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.826856e-01</td>\n",
       "      <td>1.000637</td>\n",
       "      <td>-3.356330e+06</td>\n",
       "      <td>-0.637430</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>24.605664</td>\n",
       "      <td>1.035074</td>\n",
       "      <td>0.523281</td>\n",
       "      <td>33.242744</td>\n",
       "      <td>2.067185</td>\n",
       "      <td>-6.476253</td>\n",
       "      <td>0.168916</td>\n",
       "      <td>-0.032346</td>\n",
       "      <td>-0.112240</td>\n",
       "      <td>0.295214</td>\n",
       "      <td>-0.006325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007769</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>49.173788</td>\n",
       "      <td>0.005741</td>\n",
       "      <td>-0.687572</td>\n",
       "      <td>0.881072</td>\n",
       "      <td>0.070719</td>\n",
       "      <td>49.102802</td>\n",
       "      <td>4.030977e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.555362</td>\n",
       "      <td>-0.015699</td>\n",
       "      <td>-1.453215</td>\n",
       "      <td>-0.001442</td>\n",
       "      <td>1.000157</td>\n",
       "      <td>-0.300664</td>\n",
       "      <td>0.487313</td>\n",
       "      <td>-0.502417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.009652</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.147926</td>\n",
       "      <td>-0.485955</td>\n",
       "      <td>-0.200070</td>\n",
       "      <td>1.896121e+06</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.189556</td>\n",
       "      <td>1.000346</td>\n",
       "      <td>50.377444</td>\n",
       "      <td>-0.111476</td>\n",
       "      <td>0.497797</td>\n",
       "      <td>-0.766965</td>\n",
       "      <td>0.004405</td>\n",
       "      <td>0.502203</td>\n",
       "      <td>99.342723</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>-0.485845</td>\n",
       "      <td>-4.312457</td>\n",
       "      <td>0.015003</td>\n",
       "      <td>4.990231e+01</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>-0.509112</td>\n",
       "      <td>4.115050e+06</td>\n",
       "      <td>-0.695672</td>\n",
       "      <td>-0.522710</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>-0.704583</td>\n",
       "      <td>0.999608</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>50.964690</td>\n",
       "      <td>-2.036053e-10</td>\n",
       "      <td>0.393701</td>\n",
       "      <td>0.422114</td>\n",
       "      <td>2.929459e+07</td>\n",
       "      <td>-0.063412</td>\n",
       "      <td>50.445407</td>\n",
       "      <td>-0.109785</td>\n",
       "      <td>1.009127</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.735868</td>\n",
       "      <td>0.295416</td>\n",
       "      <td>-1.012261</td>\n",
       "      <td>49.036951</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-2.352515</td>\n",
       "      <td>8.518502e+06</td>\n",
       "      <td>50.619396</td>\n",
       "      <td>-0.492156</td>\n",
       "      <td>1.212199e+07</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>-1.076210</td>\n",
       "      <td>-0.208238</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>-1.014994e-10</td>\n",
       "      <td>-0.490413</td>\n",
       "      <td>-0.206149</td>\n",
       "      <td>-0.751275</td>\n",
       "      <td>1.253019</td>\n",
       "      <td>2.068090e+07</td>\n",
       "      <td>4.320630e+07</td>\n",
       "      <td>49.833020</td>\n",
       "      <td>3.032624e+06</td>\n",
       "      <td>-0.110000</td>\n",
       "      <td>-0.493511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.752294</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.738318</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.753086</td>\n",
       "      <td>0.771930</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.752315</td>\n",
       "      <td>0.755814</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.768421</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.752809</td>\n",
       "      <td>0.747368</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.773333</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.773333</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.752381</td>\n",
       "      <td>0.747475</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.601942</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>0.672414</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.754545</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.771739</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.736111</td>\n",
       "      <td>0.747664</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>0.753014</td>\n",
       "      <td>0.779817</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.728571</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.754545</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.712644</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.741573</td>\n",
       "      <td>0.736264</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>0.765766</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.773684</td>\n",
       "      <td>0.688679</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.655738</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.778947</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.780952</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.779817</td>\n",
       "      <td>0.781609</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.726316</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.726316</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.715686</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.781609</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.771930</td>\n",
       "      <td>0.771930</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.785047</td>\n",
       "      <td>0.770115</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.769912</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.754098</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>0.752577</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>0.757143</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.706522</td>\n",
       "      <td>0.784091</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>0.755814</td>\n",
       "      <td>0.757009</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.761364</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.754545</td>\n",
       "      <td>0.754242</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.778947</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.770115</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.736364</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.741573</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.788732</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.754098</td>\n",
       "      <td>0.755814</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.771930</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.778947</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.778947</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.732673</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.770492</td>\n",
       "      <td>0.770492</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.730435</td>\n",
       "      <td>0.693182</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.717391</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.712121</td>\n",
       "      <td>0.655738</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.747126</td>\n",
       "      <td>0.704708</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.758242</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.752809</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.770492</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.721154</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.747664</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.746032</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.757009</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.672566</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.770567</td>\n",
       "      <td>0.771739</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.771584</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.760563</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.746032</td>\n",
       "      <td>0.744898</td>\n",
       "      <td>0.685185</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.747664</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.752941</td>\n",
       "      <td>0.746032</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.743119</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.779070</td>\n",
       "      <td>0.779070</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.783076</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.770642</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.771930</td>\n",
       "      <td>0.752294</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.747057</td>\n",
       "      <td>0.738636</td>\n",
       "      <td>0.747475</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.732143</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.752577</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.754098</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.735632</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>600575.000000</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.143569</td>\n",
       "      <td>0.748879</td>\n",
       "      <td>7.641959e+06</td>\n",
       "      <td>0.323373</td>\n",
       "      <td>69.508491</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.125154</td>\n",
       "      <td>0.134415</td>\n",
       "      <td>0.550096</td>\n",
       "      <td>-2.759954e-02</td>\n",
       "      <td>-0.524935</td>\n",
       "      <td>1.065071</td>\n",
       "      <td>5.255963e+06</td>\n",
       "      <td>126.821192</td>\n",
       "      <td>1.288679e+06</td>\n",
       "      <td>0.128293</td>\n",
       "      <td>0.128293</td>\n",
       "      <td>0.138173</td>\n",
       "      <td>0.063795</td>\n",
       "      <td>0.063795</td>\n",
       "      <td>0.136089</td>\n",
       "      <td>139.944904</td>\n",
       "      <td>0.303113</td>\n",
       "      <td>-0.069285</td>\n",
       "      <td>2.015101</td>\n",
       "      <td>-0.417722</td>\n",
       "      <td>0.001249</td>\n",
       "      <td>0.334366</td>\n",
       "      <td>0.130702</td>\n",
       "      <td>1.858979</td>\n",
       "      <td>12.863333</td>\n",
       "      <td>7.561210</td>\n",
       "      <td>67.935606</td>\n",
       "      <td>1.533945</td>\n",
       "      <td>0.013527</td>\n",
       "      <td>2.079642</td>\n",
       "      <td>0.933818</td>\n",
       "      <td>1.877442e+08</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>4.715645e+05</td>\n",
       "      <td>1.058600</td>\n",
       "      <td>0.008559</td>\n",
       "      <td>1.711422</td>\n",
       "      <td>0.497576</td>\n",
       "      <td>0.035099</td>\n",
       "      <td>0.268254</td>\n",
       "      <td>0.608206</td>\n",
       "      <td>-0.247197</td>\n",
       "      <td>-2.292205e-02</td>\n",
       "      <td>0.240285</td>\n",
       "      <td>4.199825e-10</td>\n",
       "      <td>15.671387</td>\n",
       "      <td>0.870834</td>\n",
       "      <td>0.071578</td>\n",
       "      <td>0.287114</td>\n",
       "      <td>1.170273</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>1.548913e+08</td>\n",
       "      <td>0.428405</td>\n",
       "      <td>1.063909e-02</td>\n",
       "      <td>12.871455</td>\n",
       "      <td>0.249936</td>\n",
       "      <td>2.976798e+05</td>\n",
       "      <td>-0.589023</td>\n",
       "      <td>1.220547</td>\n",
       "      <td>0.047935</td>\n",
       "      <td>-1864.520885</td>\n",
       "      <td>-0.581166</td>\n",
       "      <td>0.169591</td>\n",
       "      <td>0.515833</td>\n",
       "      <td>0.892883</td>\n",
       "      <td>0.765541</td>\n",
       "      <td>1.005314e+04</td>\n",
       "      <td>93.033166</td>\n",
       "      <td>0.003009</td>\n",
       "      <td>1.560000</td>\n",
       "      <td>-0.662176</td>\n",
       "      <td>0.013337</td>\n",
       "      <td>1.235508e+00</td>\n",
       "      <td>-0.055636</td>\n",
       "      <td>-0.316373</td>\n",
       "      <td>49.458636</td>\n",
       "      <td>15.398042</td>\n",
       "      <td>0.173207</td>\n",
       "      <td>0.518751</td>\n",
       "      <td>0.719792</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>1.062787e+05</td>\n",
       "      <td>3.882571e-01</td>\n",
       "      <td>1.034795</td>\n",
       "      <td>-8.833333e-01</td>\n",
       "      <td>2.249876</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>41.784454</td>\n",
       "      <td>1.308907</td>\n",
       "      <td>0.757537</td>\n",
       "      <td>46.202922</td>\n",
       "      <td>3.892354</td>\n",
       "      <td>20.309119</td>\n",
       "      <td>0.324045</td>\n",
       "      <td>-0.013857</td>\n",
       "      <td>0.427397</td>\n",
       "      <td>0.775543</td>\n",
       "      <td>0.618827</td>\n",
       "      <td>3.698630</td>\n",
       "      <td>0.067087</td>\n",
       "      <td>0.009339</td>\n",
       "      <td>56.864947</td>\n",
       "      <td>0.211200</td>\n",
       "      <td>-0.364315</td>\n",
       "      <td>1.050854</td>\n",
       "      <td>3.898714</td>\n",
       "      <td>103.122907</td>\n",
       "      <td>3.233235e+05</td>\n",
       "      <td>0.197360</td>\n",
       "      <td>5.041505</td>\n",
       "      <td>2.813918</td>\n",
       "      <td>-0.984669</td>\n",
       "      <td>0.031300</td>\n",
       "      <td>1.027691</td>\n",
       "      <td>-0.145206</td>\n",
       "      <td>0.739283</td>\n",
       "      <td>-0.258404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277632</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.547627</td>\n",
       "      <td>-0.240984</td>\n",
       "      <td>-0.052920</td>\n",
       "      <td>1.778677e+07</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.389131</td>\n",
       "      <td>1.022386</td>\n",
       "      <td>63.256163</td>\n",
       "      <td>-0.061928</td>\n",
       "      <td>0.651623</td>\n",
       "      <td>-0.449467</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>0.653631</td>\n",
       "      <td>131.535010</td>\n",
       "      <td>58.333333</td>\n",
       "      <td>-0.245248</td>\n",
       "      <td>8.377322</td>\n",
       "      <td>0.286898</td>\n",
       "      <td>6.955538e+01</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>0.783679</td>\n",
       "      <td>-0.361731</td>\n",
       "      <td>2.238889e+07</td>\n",
       "      <td>-0.489370</td>\n",
       "      <td>-0.016943</td>\n",
       "      <td>1.521970e+02</td>\n",
       "      <td>-0.491604</td>\n",
       "      <td>1.018590</td>\n",
       "      <td>1.849614</td>\n",
       "      <td>69.059890</td>\n",
       "      <td>4.606029e-10</td>\n",
       "      <td>0.637500</td>\n",
       "      <td>0.738043</td>\n",
       "      <td>6.592281e+07</td>\n",
       "      <td>4.263357</td>\n",
       "      <td>61.341227</td>\n",
       "      <td>0.206776</td>\n",
       "      <td>1.310316</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.828583</td>\n",
       "      <td>0.492537</td>\n",
       "      <td>60.147538</td>\n",
       "      <td>70.199823</td>\n",
       "      <td>-0.259543</td>\n",
       "      <td>45.009806</td>\n",
       "      <td>1.681707e+07</td>\n",
       "      <td>61.507459</td>\n",
       "      <td>-0.252923</td>\n",
       "      <td>5.119516e+07</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.761277</td>\n",
       "      <td>7.256686</td>\n",
       "      <td>0.080714</td>\n",
       "      <td>9.509501e-10</td>\n",
       "      <td>-0.244149</td>\n",
       "      <td>-0.081056</td>\n",
       "      <td>-0.533333</td>\n",
       "      <td>2.328216</td>\n",
       "      <td>7.397858e+07</td>\n",
       "      <td>8.965824e+07</td>\n",
       "      <td>66.556920</td>\n",
       "      <td>6.444395e+06</td>\n",
       "      <td>0.101709</td>\n",
       "      <td>-0.244013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>603939.000000</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.225546e+08</td>\n",
       "      <td>0.997735</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.884350</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>-2.381007e-11</td>\n",
       "      <td>-0.020614</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.442828e+09</td>\n",
       "      <td>795.833333</td>\n",
       "      <td>2.296963e+08</td>\n",
       "      <td>0.998553</td>\n",
       "      <td>0.998553</td>\n",
       "      <td>298.744269</td>\n",
       "      <td>101.729106</td>\n",
       "      <td>101.729106</td>\n",
       "      <td>0.907258</td>\n",
       "      <td>4300.000000</td>\n",
       "      <td>0.992730</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>3051.000000</td>\n",
       "      <td>-0.001572</td>\n",
       "      <td>1.988943</td>\n",
       "      <td>0.998372</td>\n",
       "      <td>145.194255</td>\n",
       "      <td>2417.495840</td>\n",
       "      <td>310.810000</td>\n",
       "      <td>51.246855</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>80.020000</td>\n",
       "      <td>4.531358</td>\n",
       "      <td>3498.608550</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.188162e+09</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>5.363861e+08</td>\n",
       "      <td>3.582492</td>\n",
       "      <td>0.996212</td>\n",
       "      <td>548.706802</td>\n",
       "      <td>0.999528</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000274</td>\n",
       "      <td>-6.170000e-07</td>\n",
       "      <td>1.533616</td>\n",
       "      <td>9.255523e-08</td>\n",
       "      <td>83.713656</td>\n",
       "      <td>1269.686946</td>\n",
       "      <td>99.024538</td>\n",
       "      <td>0.997656</td>\n",
       "      <td>2.395575</td>\n",
       "      <td>0.101538</td>\n",
       "      <td>8.924972e+09</td>\n",
       "      <td>51.395536</td>\n",
       "      <td>2.207100e-01</td>\n",
       "      <td>298.787812</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.317682e+07</td>\n",
       "      <td>-0.017817</td>\n",
       "      <td>1.999665</td>\n",
       "      <td>0.222484</td>\n",
       "      <td>-543.847012</td>\n",
       "      <td>-0.000280</td>\n",
       "      <td>8.302637</td>\n",
       "      <td>15.123333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.961918e+10</td>\n",
       "      <td>1458.492815</td>\n",
       "      <td>0.050941</td>\n",
       "      <td>68.740000</td>\n",
       "      <td>-0.041625</td>\n",
       "      <td>1.784002</td>\n",
       "      <td>6.781707e+178</td>\n",
       "      <td>6.304742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>332.005744</td>\n",
       "      <td>8.563506</td>\n",
       "      <td>18.650000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>5.630381e+07</td>\n",
       "      <td>9.982312e-01</td>\n",
       "      <td>1.617021</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>565.086314</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>628.778412</td>\n",
       "      <td>1.994113</td>\n",
       "      <td>0.999409</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>138.870000</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>17.708333</td>\n",
       "      <td>0.343915</td>\n",
       "      <td>5.086490</td>\n",
       "      <td>7.314052</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>77.335640</td>\n",
       "      <td>100.187884</td>\n",
       "      <td>0.119379</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>34.768560</td>\n",
       "      <td>-0.000547</td>\n",
       "      <td>35.575706</td>\n",
       "      <td>78.364758</td>\n",
       "      <td>261.632745</td>\n",
       "      <td>3.184020e+08</td>\n",
       "      <td>53.220000</td>\n",
       "      <td>86.460011</td>\n",
       "      <td>60.113677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.900891</td>\n",
       "      <td>1.677495</td>\n",
       "      <td>-0.000269</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000275</td>\n",
       "      <td>14.239576</td>\n",
       "      <td>0.997625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3836.973726</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>0.948055</td>\n",
       "      <td>2.556015e+09</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.995753</td>\n",
       "      <td>1.530215</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-0.000193</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>657.948718</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-0.000281</td>\n",
       "      <td>1363.517840</td>\n",
       "      <td>0.988932</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>78.015406</td>\n",
       "      <td>-0.004754</td>\n",
       "      <td>2.947028e+09</td>\n",
       "      <td>-0.001398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.401920e+17</td>\n",
       "      <td>-0.001182</td>\n",
       "      <td>1.322835</td>\n",
       "      <td>20.263158</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>3.972884e-05</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.969380</td>\n",
       "      <td>4.220538e+09</td>\n",
       "      <td>90.154991</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.997964</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>4.347976</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28533.333333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-0.000279</td>\n",
       "      <td>115753.915293</td>\n",
       "      <td>5.002194e+08</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-0.000273</td>\n",
       "      <td>3.904067e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.020000</td>\n",
       "      <td>-0.143145</td>\n",
       "      <td>250.966851</td>\n",
       "      <td>10.167177</td>\n",
       "      <td>7.199519e-05</td>\n",
       "      <td>-0.000279</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>91.560000</td>\n",
       "      <td>3.868521e+09</td>\n",
       "      <td>3.932279e+09</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>3.373454e+08</td>\n",
       "      <td>48.410000</td>\n",
       "      <td>-0.000273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ACCA          ACD20           ACD6             AD  \\\n",
       "count  240975.000000  240973.000000  240975.000000  240975.000000   \n",
       "mean        0.500645       0.504629       0.508350       0.533987   \n",
       "std         0.280616       0.287521       0.287922       0.279544   \n",
       "min         0.008850       0.008547       0.008547       0.008547   \n",
       "25%         0.258065       0.254545       0.258929       0.303571   \n",
       "50%         0.500000       0.500000       0.505618       0.543478   \n",
       "75%         0.739130       0.752294       0.758621       0.769231   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                AD20            AD6           ADTM            ADX  \\\n",
       "count  240975.000000  240975.000000  240965.000000  235908.000000   \n",
       "mean        0.533960       0.533966       0.498894       0.496710   \n",
       "std         0.279479       0.279546       0.285094       0.281948   \n",
       "min         0.009091       0.009091       0.008547       0.008547   \n",
       "25%         0.303922       0.303571       0.250000       0.252874   \n",
       "50%         0.543478       0.543478       0.495327       0.500000   \n",
       "75%         0.769231       0.769231       0.741935       0.737705   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                ADXR          APBMA             AR           ARBR  \\\n",
       "count  235877.000000  240975.000000  240972.000000  240972.000000   \n",
       "mean        0.496818       0.522025       0.496482       0.492176   \n",
       "std         0.280955       0.283733       0.284231       0.281365   \n",
       "min         0.008547       0.008547       0.008547       0.008547   \n",
       "25%         0.254902       0.280000       0.250000       0.250000   \n",
       "50%         0.500000       0.527778       0.490566       0.482759   \n",
       "75%         0.736842       0.766667       0.738318       0.731707   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                 ARC        ARTDays        ARTRate            ASI  \\\n",
       "count  240959.000000  240243.000000  240243.000000  240975.000000   \n",
       "mean        0.493097       0.511797       0.511176       0.504986   \n",
       "std         0.279859       0.283832       0.282161       0.287499   \n",
       "min         0.008696       0.008547       0.008547       0.008547   \n",
       "25%         0.250000       0.267857       0.267857       0.255814   \n",
       "50%         0.485981       0.508772       0.514286       0.500000   \n",
       "75%         0.731707       0.757576       0.750000       0.753086   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                ASSI          ATR14           ATR6  AccountsPayablesTDays  \\\n",
       "count  240975.000000  240975.000000  240967.000000          240323.000000   \n",
       "mean        0.569467       0.524187       0.524585               0.506486   \n",
       "std         0.248713       0.283041       0.283621               0.282584   \n",
       "min         0.015385       0.009091       0.008850               0.008547   \n",
       "25%         0.379310       0.282051       0.282609               0.264151   \n",
       "50%         0.580000       0.531915       0.531915               0.500000   \n",
       "75%         0.771930       0.769231       0.769231               0.750000   \n",
       "max         1.000000       1.000000       1.000000               1.000000   \n",
       "\n",
       "       AccountsPayablesTRate  AdminExpenseTTM  AdminiExpenseRate  \\\n",
       "count          240323.000000    240323.000000      240323.000000   \n",
       "mean                0.515315         0.562125           0.492909   \n",
       "std                 0.284114         0.262105           0.283463   \n",
       "min                 0.008547         0.009091           0.009091   \n",
       "25%                 0.272727         0.352941           0.250000   \n",
       "50%                 0.521739         0.580645           0.491228   \n",
       "75%                 0.760000         0.782609           0.733333   \n",
       "max                 1.000000         1.000000           1.000000   \n",
       "\n",
       "            Alpha120        Alpha20        Alpha60          Aroon  \\\n",
       "count  240975.000000  240975.000000  240975.000000  240975.000000   \n",
       "mean        0.514249       0.514019       0.514233       0.506846   \n",
       "std         0.279703       0.282508       0.281604       0.283753   \n",
       "min         0.008696       0.008547       0.008696       0.008547   \n",
       "25%         0.276316       0.272727       0.272727       0.261905   \n",
       "50%         0.517241       0.517241       0.517241       0.500000   \n",
       "75%         0.752315       0.755814       0.755102       0.750000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "           AroonDown        AroonUp  AssetImpairLossTTM            BBI  \\\n",
       "count  240975.000000  240975.000000       240875.000000  240975.000000   \n",
       "mean        0.513668       0.506616            0.525746       0.522025   \n",
       "std         0.268928       0.278000            0.284065       0.283733   \n",
       "min         0.008621       0.008547            0.008547       0.008547   \n",
       "25%         0.288090       0.263158            0.286335       0.280000   \n",
       "50%         0.520833       0.500000            0.539683       0.527778   \n",
       "75%         0.736842       0.750000            0.768421       0.766667   \n",
       "max         1.000000       1.000000            1.000000       1.000000   \n",
       "\n",
       "                BBIC         BIAS10         BIAS20          BIAS5  \\\n",
       "count  240975.000000  240967.000000  240967.000000  240967.000000   \n",
       "mean        0.513848       0.508839       0.506888       0.509884   \n",
       "std         0.282938       0.282864       0.282921       0.283047   \n",
       "min         0.008547       0.008547       0.008547       0.008547   \n",
       "25%         0.270833       0.265306       0.263158       0.266667   \n",
       "50%         0.516129       0.505747       0.500000       0.508772   \n",
       "75%         0.757576       0.750000       0.750000       0.750000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              BIAS60           BLEV             BR    BackwardADJ  \\\n",
       "count  240967.000000  240953.000000  240972.000000  232849.000000   \n",
       "mean        0.503846       0.530963       0.507789       0.512265   \n",
       "std         0.283377       0.279450       0.285527       0.276477   \n",
       "min         0.008547       0.009901       0.008547       0.008547   \n",
       "25%         0.260870       0.290909       0.262295       0.275000   \n",
       "50%         0.500000       0.538462       0.507042       0.527778   \n",
       "75%         0.745455       0.772727       0.752809       0.747368   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "           BearPower        Beta120         Beta20        Beta252  \\\n",
       "count  240975.000000  240975.000000  240975.000000  240965.000000   \n",
       "mean        0.502759       0.531191       0.525429       0.533289   \n",
       "std         0.287558       0.281320       0.282607       0.278896   \n",
       "min         0.008547       0.008696       0.008547       0.008696   \n",
       "25%         0.252874       0.294118       0.285714       0.300000   \n",
       "50%         0.500000       0.537037       0.530000       0.540541   \n",
       "75%         0.750000       0.773333       0.769231       0.772727   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              Beta60       BollDown         BollUp  BondsPayableToAsset  \\\n",
       "count  240975.000000  240967.000000  240967.000000        240975.000000   \n",
       "mean        0.531191       0.537718       0.534779             0.527898   \n",
       "std         0.281320       0.282481       0.282715             0.214687   \n",
       "min         0.008696       0.009091       0.009091             0.119048   \n",
       "25%         0.294118       0.298969       0.295455             0.390805   \n",
       "50%         0.537037       0.552083       0.547619             0.445455   \n",
       "75%         0.773333       0.781818       0.777778             0.631579   \n",
       "max         1.000000       1.000000       1.000000             1.000000   \n",
       "\n",
       "           BullPower          CCI10          CCI20           CCI5  \\\n",
       "count  240975.000000  240967.000000  240967.000000  240967.000000   \n",
       "mean        0.510485       0.508292       0.506962       0.508569   \n",
       "std         0.287737       0.285473       0.285675       0.285031   \n",
       "min         0.008547       0.008547       0.008547       0.008547   \n",
       "25%         0.261364       0.263158       0.260870       0.263158   \n",
       "50%         0.509804       0.505747       0.500000       0.505747   \n",
       "75%         0.760000       0.750000       0.750000       0.752381   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "               CCI88          CETOP         CFO2EV            CMO  \\\n",
       "count  240967.000000  240975.000000  240975.000000  240965.000000   \n",
       "mean        0.501898       0.528463       0.528679       0.504661   \n",
       "std         0.286271       0.287094       0.286680       0.284221   \n",
       "min         0.008547       0.008547       0.008547       0.008547   \n",
       "25%         0.254545       0.281690       0.284091       0.260000   \n",
       "50%         0.500000       0.540000       0.538462       0.500000   \n",
       "75%         0.747475       0.777778       0.777778       0.750000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                CMRA           CR20           CTOP           CTP5  \\\n",
       "count  240184.000000  240975.000000  230909.000000  196129.000000   \n",
       "mean        0.491984       0.504969       0.490914       0.414117   \n",
       "std         0.276641       0.285172       0.266469       0.243442   \n",
       "min         0.008547       0.008547       0.012048       0.009709   \n",
       "25%         0.253521       0.259259       0.265957       0.206897   \n",
       "50%         0.491228       0.500000       0.487805       0.400000   \n",
       "75%         0.727273       0.750000       0.714286       0.601942   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "       CapitalSurplusFundPS  CashConversionCycle  CashDividendCover  \\\n",
       "count         240762.000000        239548.000000      222561.000000   \n",
       "mean               0.509968             0.513315           0.457711   \n",
       "std                0.287395             0.279656           0.260295   \n",
       "min                0.008547             0.009091           0.008850   \n",
       "25%                0.263158             0.277778           0.232558   \n",
       "50%                0.505263             0.515789           0.457143   \n",
       "75%                0.758621             0.754386           0.672414   \n",
       "max                1.000000             1.000000           1.000000   \n",
       "\n",
       "       CashEquivalentPS     CashFlowPS  CashRateOfSales  \\\n",
       "count     240949.000000  240975.000000    240975.000000   \n",
       "mean           0.519979       0.503884         0.520511   \n",
       "std            0.279814       0.289319         0.281462   \n",
       "min            0.008696       0.008850         0.008696   \n",
       "25%            0.282051       0.250000         0.278481   \n",
       "50%            0.521739       0.500000         0.526316   \n",
       "75%            0.756757       0.754545         0.760000   \n",
       "max            1.000000       1.000000         1.000000   \n",
       "\n",
       "       CashRateOfSalesLatest  CashToCurrentLiability  ChaikinOscillator  \\\n",
       "count          240975.000000           240323.000000      240975.000000   \n",
       "mean                0.515700                0.504595           0.513270   \n",
       "std                 0.282252                0.278537           0.292695   \n",
       "min                 0.008696                0.008696           0.008547   \n",
       "25%                 0.272727                0.266667           0.256410   \n",
       "50%                 0.517241                0.500000           0.513514   \n",
       "75%                 0.759259                0.740000           0.771739   \n",
       "max                 1.000000                1.000000           1.000000   \n",
       "\n",
       "       ChaikinVolatility       ChandeSD       ChandeSU       CmraCNE5  \\\n",
       "count      240975.000000  240965.000000  240965.000000  240975.000000   \n",
       "mean            0.509314       0.526703       0.522423       0.497904   \n",
       "std             0.283499       0.282085       0.283327       0.279313   \n",
       "min             0.008547       0.008696       0.008547       0.008696   \n",
       "25%             0.265306       0.285714       0.280702       0.257353   \n",
       "50%             0.508772       0.534884       0.528302       0.494253   \n",
       "75%             0.750000       0.769231       0.767857       0.736111   \n",
       "max             1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "        CoppockCurve        CostTTM  CurrentAssetsRatio  CurrentAssetsTRate  \\\n",
       "count  240973.000000  240975.000000       240323.000000       240323.000000   \n",
       "mean        0.505715       0.556752            0.506400            0.516971   \n",
       "std         0.282970       0.268991            0.284399            0.280397   \n",
       "min         0.008547       0.009804            0.008547            0.008696   \n",
       "25%         0.263158       0.333333            0.259259            0.277778   \n",
       "50%         0.500000       0.575221            0.507042            0.516854   \n",
       "75%         0.747664       0.785714            0.754386            0.759259   \n",
       "max         1.000000       1.000000            1.000000            1.000000   \n",
       "\n",
       "        CurrentRatio             DA          DAREC          DAREV  \\\n",
       "count  240323.000000  240295.000000  232944.000000  238220.000000   \n",
       "mean        0.512742       0.553586       0.487014       0.488698   \n",
       "std         0.280898       0.268009       0.275578       0.288196   \n",
       "min         0.008696       0.010526       0.008696       0.008547   \n",
       "25%         0.271028       0.333333       0.241935       0.230769   \n",
       "50%         0.510638       0.570093       0.500000       0.500000   \n",
       "75%         0.753014       0.779817       0.721311       0.736842   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              DASREV          DASTD        DAVOL10        DAVOL20  \\\n",
       "count  235100.000000  240975.000000  240975.000000  240975.000000   \n",
       "mean        0.481352       0.498858       0.518697       0.517615   \n",
       "std         0.281112       0.274419       0.277719       0.277797   \n",
       "min         0.008547       0.008850       0.008696       0.008696   \n",
       "25%         0.230675       0.264368       0.284211       0.282609   \n",
       "50%         0.500000       0.500000       0.517857       0.517241   \n",
       "75%         0.710526       0.728571       0.756098       0.754545   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              DAVOL5           DBCD            DDI          DDNBT  \\\n",
       "count  240975.000000  240967.000000  240975.000000  240292.000000   \n",
       "mean        0.519090       0.510338       0.507143       0.510687   \n",
       "std         0.277784       0.282438       0.285541       0.274599   \n",
       "min         0.008696       0.008547       0.008547       0.008696   \n",
       "25%         0.284091       0.266667       0.261364       0.280702   \n",
       "50%         0.518519       0.509434       0.500000       0.516129   \n",
       "75%         0.756098       0.750000       0.750000       0.740741   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "               DDNCR          DDNSR            DEA           DEGM  \\\n",
       "count  240967.000000  240967.000000  240975.000000  238206.000000   \n",
       "mean        0.536385       0.485129       0.502724       0.505253   \n",
       "std         0.280263       0.274226       0.288175       0.277972   \n",
       "min         0.008696       0.008547       0.008547       0.008696   \n",
       "25%         0.301887       0.250000       0.250000       0.263158   \n",
       "50%         0.545455       0.480000       0.500000       0.504854   \n",
       "75%         0.777778       0.712644       0.750000       0.741573   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "               DHILO            DIF           DIFF            DIZ  \\\n",
       "count  240975.000000  240975.000000  240975.000000  240975.000000   \n",
       "mean        0.497644       0.514440       0.503255       0.507143   \n",
       "std         0.281345       0.285495       0.288076       0.285541   \n",
       "min         0.008547       0.008547       0.008547       0.008547   \n",
       "25%         0.257143       0.267857       0.250000       0.261364   \n",
       "50%         0.500000       0.517241       0.500000       0.500000   \n",
       "75%         0.736264       0.760870       0.750000       0.750000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "               DVRAT  DebtEquityRatio  DebtTangibleEquityRatio  \\\n",
       "count  237573.000000    240975.000000            240975.000000   \n",
       "mean        0.476293         0.518547                 0.527116   \n",
       "std         0.273135         0.276758                 0.277270   \n",
       "min         0.008696         0.009091                 0.008547   \n",
       "25%         0.241379         0.281250                 0.290909   \n",
       "50%         0.467290         0.523810                 0.532258   \n",
       "75%         0.703704         0.754386                 0.765766   \n",
       "max         1.000000         1.000000                 1.000000   \n",
       "\n",
       "       DebtsAssetRatio     DilutedEPS  DividendCover     DividendPS  \\\n",
       "count    240975.000000  240607.000000  222561.000000  222670.000000   \n",
       "mean          0.510904       0.538601       0.471957       0.465723   \n",
       "std           0.275978       0.276921       0.260151       0.263748   \n",
       "min           0.008547       0.008696       0.008547       0.008547   \n",
       "25%           0.275229       0.307692       0.252874       0.244318   \n",
       "50%           0.517241       0.549550       0.467391       0.454545   \n",
       "75%           0.744186       0.773684       0.688679       0.684211   \n",
       "max           1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "       DividendPaidRatio        DownRVI        EARNMOM          EBIAT  \\\n",
       "count      222469.000000  240967.000000  234285.000000  240323.000000   \n",
       "mean            0.445226       0.523212       0.494510       0.556839   \n",
       "std             0.260686       0.282622       0.272721       0.267021   \n",
       "min             0.008696       0.008547       0.008850       0.008547   \n",
       "25%             0.220000       0.283019       0.260870       0.352941   \n",
       "50%             0.435484       0.529412       0.494253       0.583333   \n",
       "75%             0.655738       0.766667       0.722222       0.777778   \n",
       "max             1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                EBIT         EBITDA      EBITToTOR           EGRO  \\\n",
       "count  240323.000000  239813.000000  240953.000000  240950.000000   \n",
       "mean        0.557867       0.566727       0.528014       0.499282   \n",
       "std         0.266700       0.254090       0.278071       0.269587   \n",
       "min         0.008547       0.008547       0.008696       0.008696   \n",
       "25%         0.354839       0.375000       0.291667       0.273684   \n",
       "50%         0.583333       0.588235       0.533333       0.500000   \n",
       "75%         0.778947       0.772727       0.766667       0.722222   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "               EMA10          EMA12         EMA120          EMA20  \\\n",
       "count  240975.000000  240975.000000  240975.000000  240975.000000   \n",
       "mean        0.535795       0.535891       0.539331       0.536215   \n",
       "std         0.282819       0.282805       0.282327       0.282754   \n",
       "min         0.009091       0.009091       0.009091       0.009091   \n",
       "25%         0.296296       0.296296       0.300000       0.297297   \n",
       "50%         0.550000       0.550000       0.555556       0.550000   \n",
       "75%         0.780000       0.780000       0.782609       0.780952   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "               EMA26           EMA5          EMA60          EMV14  \\\n",
       "count  240975.000000  240975.000000  240975.000000  240975.000000   \n",
       "mean        0.536451       0.535573       0.537553       0.507784   \n",
       "std         0.282709       0.282848       0.282574       0.283430   \n",
       "min         0.009091       0.009091       0.009091       0.008547   \n",
       "25%         0.297872       0.296296       0.298851       0.263889   \n",
       "50%         0.550562       0.549020       0.553191       0.506494   \n",
       "75%         0.781250       0.779817       0.781609       0.750000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                EMV6          EPIBS            EPS         EPSTTM  \\\n",
       "count  240975.000000  238880.000000  240975.000000  240975.000000   \n",
       "mean        0.508858       0.537789       0.539906       0.541444   \n",
       "std         0.283521       0.277707       0.277874       0.279933   \n",
       "min         0.008547       0.008547       0.008696       0.008547   \n",
       "25%         0.264706       0.306818       0.307692       0.307692   \n",
       "50%         0.508772       0.551724       0.550562       0.553571   \n",
       "75%         0.750000       0.777778       0.777778       0.783333   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                ETOP           ETP5      EgibsLong          Elder  \\\n",
       "count  240975.000000  223918.000000  239140.000000  240975.000000   \n",
       "mean        0.551865       0.517714       0.503641       0.502943   \n",
       "std         0.280881       0.255937       0.277619       0.283867   \n",
       "min         0.008547       0.008696       0.008850       0.008547   \n",
       "25%         0.320755       0.307692       0.265625       0.258621   \n",
       "50%         0.566667       0.523810       0.500000       0.500000   \n",
       "75%         0.800000       0.729412       0.740741       0.745098   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "       EnterpriseFCFPS  EquityFixedAssetRatio    EquityTRate  EquityToAsset  \\\n",
       "count    240323.000000          240975.000000  240975.000000  240975.000000   \n",
       "mean          0.505253               0.520836       0.516355       0.510909   \n",
       "std           0.290236               0.281537       0.277262       0.276386   \n",
       "min           0.008547               0.008696       0.008850       0.009091   \n",
       "25%           0.252174               0.276316       0.282051       0.276190   \n",
       "50%           0.500000               0.524590       0.519231       0.500000   \n",
       "75%           0.759036               0.766667       0.750000       0.745455   \n",
       "max           1.000000               1.000000       1.000000       1.000000   \n",
       "\n",
       "                FCFE           FCFF         FEARNG        FSALESG  \\\n",
       "count  240515.000000  240323.000000  238880.000000  236254.000000   \n",
       "mean        0.515504       0.499226       0.487091       0.483411   \n",
       "std         0.297061       0.295741       0.278927       0.272996   \n",
       "min         0.008850       0.008850       0.008547       0.008696   \n",
       "25%         0.247367       0.233333       0.250000       0.250000   \n",
       "50%         0.523810       0.483871       0.476190       0.474747   \n",
       "75%         0.782609       0.766667       0.726316       0.708333   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "               FY12P  FiftyTwoWeekHigh  FinanExpenseTTM  FinancialExpenseRate  \\\n",
       "count  238880.000000     240975.000000    240323.000000         240323.000000   \n",
       "mean        0.537789          0.504503         0.532653              0.510650   \n",
       "std         0.277707          0.283391         0.289927              0.279385   \n",
       "min         0.008547          0.008547         0.008547              0.008929   \n",
       "25%         0.306818          0.260000         0.280899              0.272727   \n",
       "50%         0.551724          0.500000         0.551724              0.516129   \n",
       "75%         0.777778          0.750000         0.784314              0.750000   \n",
       "max         1.000000          1.000000         1.000000              1.000000   \n",
       "\n",
       "       FinancingCashGrowRate  FixAssetRatio  FixedAssetsTRate      ForwardPE  \\\n",
       "count          223674.000000  240975.000000     240975.000000  240975.000000   \n",
       "mean                0.477431       0.506990          0.521465       0.508270   \n",
       "std                 0.276297       0.286139          0.284354       0.274460   \n",
       "min                 0.008547       0.009091          0.008696       0.008547   \n",
       "25%                 0.238095       0.259259          0.283019       0.277778   \n",
       "50%                 0.471429       0.505618          0.521739       0.500000   \n",
       "75%                 0.709677       0.750000          0.767442       0.739130   \n",
       "max                 1.000000       1.000000          1.000000       1.000000   \n",
       "\n",
       "                GREC           GREV          GSREV  GainLossVarianceRatio120  \\\n",
       "count  237784.000000  239748.000000  237698.000000             240975.000000   \n",
       "mean        0.499635       0.492621       0.486459                  0.491425   \n",
       "std         0.293991       0.298060       0.294630                  0.278675   \n",
       "min         0.008547       0.009804       0.009091                  0.008547   \n",
       "25%         0.226316       0.215116       0.215686                  0.252336   \n",
       "50%         0.500000       0.488372       0.486486                  0.485714   \n",
       "75%         0.769231       0.765625       0.750000                  0.726316   \n",
       "max         1.000000       1.000000       1.000000                  1.000000   \n",
       "\n",
       "       GainLossVarianceRatio20  GainLossVarianceRatio60  GainVariance120  \\\n",
       "count            240975.000000            240975.000000    240975.000000   \n",
       "mean                  0.504057                 0.497261         0.486905   \n",
       "std                   0.286019                 0.282093         0.274487   \n",
       "min                   0.008547                 0.008696         0.008547   \n",
       "25%                   0.258065                 0.254545         0.252874   \n",
       "50%                   0.500000                 0.492063         0.480000   \n",
       "75%                   0.750000                 0.737500         0.715686   \n",
       "max                   1.000000                 1.000000         1.000000   \n",
       "\n",
       "       GainVariance20  GainVariance60  GrossIncomeRatio    GrossProfit  \\\n",
       "count   240975.000000   240975.000000     240975.000000  240323.000000   \n",
       "mean         0.496519        0.489641          0.519993       0.563885   \n",
       "std          0.283217        0.278127          0.285076       0.259603   \n",
       "min          0.008547        0.008547          0.008696       0.008547   \n",
       "25%          0.250000        0.250000          0.274194       0.364706   \n",
       "50%          0.490909        0.482759          0.521739       0.583333   \n",
       "75%          0.737705        0.724138          0.766667       0.781609   \n",
       "max          1.000000        1.000000          1.000000       1.000000   \n",
       "\n",
       "       GrossProfitTTM          HBETA         HSIGMA     HsigmaCNE5  \\\n",
       "count   240323.000000  240960.000000  240960.000000  240965.000000   \n",
       "mean         0.567492       0.533658       0.533658       0.477268   \n",
       "std          0.256559       0.277274       0.277274       0.276006   \n",
       "min          0.008547       0.008696       0.008696       0.008547   \n",
       "25%          0.369932       0.302752       0.302752       0.239583   \n",
       "50%          0.588889       0.541667       0.541667       0.467742   \n",
       "75%          0.781818       0.771930       0.771930       0.705882   \n",
       "max          1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "               Hurst    ILLIQUIDITY  InformationRatio120  InformationRatio20  \\\n",
       "count  240962.000000  240975.000000        240975.000000       240975.000000   \n",
       "mean        0.504477       0.452878             0.493683            0.503563   \n",
       "std         0.281799       0.259071             0.283102            0.285554   \n",
       "min         0.008696       0.008547             0.008696            0.008547   \n",
       "25%         0.262295       0.235294             0.250000            0.257576   \n",
       "50%         0.500000       0.435185             0.485981            0.500000   \n",
       "75%         0.744186       0.655172             0.735849            0.750000   \n",
       "max         1.000000       1.000000             1.000000            1.000000   \n",
       "\n",
       "       InformationRatio60          IntCL        IntDebt      IntFreeCL  \\\n",
       "count       240975.000000  240323.000000  240323.000000  240323.000000   \n",
       "mean             0.498854       0.547106       0.548612       0.553163   \n",
       "std              0.284804       0.280797       0.279429       0.264359   \n",
       "min              0.008696       0.009091       0.009091       0.008547   \n",
       "25%              0.250000       0.311927       0.319588       0.339623   \n",
       "50%              0.495327       0.566667       0.566667       0.565217   \n",
       "75%              0.741379       0.787500       0.785714       0.775862   \n",
       "max              1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "          IntFreeNCL  IntangibleAssetRatio  InteBearDebtToTotalCapital  \\\n",
       "count  240203.000000         240606.000000               240323.000000   \n",
       "mean        0.540756              0.524495                    0.519087   \n",
       "std         0.281526              0.282576                    0.278976   \n",
       "min         0.009091              0.008696                    0.009091   \n",
       "25%         0.304348              0.285714                    0.280000   \n",
       "50%         0.558140              0.529412                    0.526316   \n",
       "75%         0.785047              0.770115                    0.756098   \n",
       "max         1.000000              1.000000                    1.000000   \n",
       "\n",
       "       InterestCover  InventoryTDays  InventoryTRate  InvestCashGrowRate  \\\n",
       "count  240907.000000   239628.000000   239628.000000       125952.000000   \n",
       "mean        0.531649        0.508850        0.503264            0.324063   \n",
       "std         0.282652        0.281039        0.281100            0.238720   \n",
       "min         0.008547        0.008547        0.009091            0.008547   \n",
       "25%         0.297297        0.267857        0.262295            0.131148   \n",
       "50%         0.546392        0.510870        0.500000            0.269231   \n",
       "75%         0.769912        0.750000        0.741379            0.466667   \n",
       "max         1.000000        1.000000        1.000000            1.000000   \n",
       "\n",
       "       InvestRAssociatesToTP  InvestRAssociatesToTPLatest         JDQS20  \\\n",
       "count          193439.000000                195223.000000  240975.000000   \n",
       "mean                0.411808                     0.412887       0.495296   \n",
       "std                 0.236241                     0.237809       0.277623   \n",
       "min                 0.008547                     0.008696       0.008547   \n",
       "25%                 0.214286                     0.213115       0.256250   \n",
       "50%                 0.402062                     0.403226       0.486111   \n",
       "75%                 0.592593                     0.594595       0.729167   \n",
       "max                 1.000000                     1.000000       1.000000   \n",
       "\n",
       "               KDJ_D          KDJ_J          KDJ_K  KlingerOscillator  \\\n",
       "count  240967.000000  240967.000000  240967.000000      240975.000000   \n",
       "mean        0.509041       0.509749       0.509056           0.505721   \n",
       "std         0.284713       0.284828       0.284471           0.293389   \n",
       "min         0.008547       0.008547       0.008547           0.008696   \n",
       "25%         0.263158       0.263158       0.263158           0.244898   \n",
       "50%         0.508197       0.509091       0.507692           0.500000   \n",
       "75%         0.754098       0.754386       0.752577           0.766667   \n",
       "max         1.000000       1.000000       1.000000           1.000000   \n",
       "\n",
       "         Kurtosis120     Kurtosis20     Kurtosis60           LCAP  \\\n",
       "count  240975.000000  240975.000000  240975.000000  240975.000000   \n",
       "mean        0.509900       0.512194       0.512097       0.566670   \n",
       "std         0.280403       0.286388       0.283484       0.227620   \n",
       "min         0.008696       0.008696       0.008696       0.010870   \n",
       "25%         0.267857       0.264151       0.267442       0.390805   \n",
       "50%         0.509091       0.512500       0.510204       0.576923   \n",
       "75%         0.750000       0.759259       0.757143       0.750000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                LFLO  LongDebtToAsset  LongDebtToWorkingCapital  \\\n",
       "count  240975.000000    240975.000000             219505.000000   \n",
       "mean        0.589273         0.529512                  0.484886   \n",
       "std         0.239152         0.276148                  0.269425   \n",
       "min         0.009434         0.033333                  0.009346   \n",
       "25%         0.423077         0.254545                  0.258621   \n",
       "50%         0.615385         0.520000                  0.478261   \n",
       "75%         0.782609         0.782609                  0.706522   \n",
       "max         1.000000         1.000000                  1.000000   \n",
       "\n",
       "       LongTermDebtToAsset  LossVariance120  LossVariance20  LossVariance60  \\\n",
       "count        240953.000000    240975.000000   240975.000000   240975.000000   \n",
       "mean              0.534167         0.498108        0.499173        0.495141   \n",
       "std               0.283483         0.278264        0.283646        0.280656   \n",
       "min               0.009901         0.008547        0.008547        0.008547   \n",
       "25%               0.290323         0.260870        0.255000        0.253626   \n",
       "50%               0.545455         0.494253        0.494949        0.487500   \n",
       "75%               0.784091         0.733333        0.739130        0.733333   \n",
       "max               1.000000         1.000000        1.000000        1.000000   \n",
       "\n",
       "                MA10      MA10Close  MA10RegressCoeff12  MA10RegressCoeff6  \\\n",
       "count  240975.000000  240975.000000       240975.000000      240975.000000   \n",
       "mean        0.535770       0.512861            0.506700           0.507117   \n",
       "std         0.282795       0.282839            0.287799           0.287862   \n",
       "min         0.009091       0.008547            0.008547           0.008547   \n",
       "25%         0.296296       0.269231            0.256410           0.257576   \n",
       "50%         0.549451       0.515152            0.500000           0.500000   \n",
       "75%         0.780000       0.754717            0.755814           0.757009   \n",
       "max         1.000000       1.000000            1.000000           1.000000   \n",
       "\n",
       "               MA120           MA20            MA5           MA60  \\\n",
       "count  240975.000000  240975.000000  240975.000000  240975.000000   \n",
       "mean        0.538859       0.536170       0.535566       0.537338   \n",
       "std         0.282105       0.282728       0.282843       0.282447   \n",
       "min         0.009091       0.009091       0.009091       0.009091   \n",
       "25%         0.300000       0.296875       0.296296       0.298851   \n",
       "50%         0.555556       0.550000       0.549020       0.552632   \n",
       "75%         0.781818       0.780488       0.780000       0.781250   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                MACD         MAWVAD            MFI           MLEV  \\\n",
       "count  240975.000000  240972.000000  240967.000000  240953.000000   \n",
       "mean        0.511444       0.512910       0.506022       0.535104   \n",
       "std         0.287670       0.291975       0.285368       0.281940   \n",
       "min         0.008547       0.008547       0.008547       0.010309   \n",
       "25%         0.262295       0.256637       0.260000       0.294737   \n",
       "50%         0.511364       0.517241       0.500000       0.544643   \n",
       "75%         0.761364       0.769231       0.750000       0.781250   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                 MTM          MTMMA      MassIndex       MktValue  \\\n",
       "count  240975.000000  240973.000000  240975.000000  240975.000000   \n",
       "mean        0.507210       0.505818       0.511311       0.566670   \n",
       "std         0.287465       0.287523       0.283244       0.227620   \n",
       "min         0.008547       0.008547       0.008547       0.010870   \n",
       "25%         0.258427       0.255814       0.267442       0.390805   \n",
       "50%         0.500000       0.500000       0.509804       0.576923   \n",
       "75%         0.756757       0.754545       0.754242       0.750000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "         MoneyFlow20           NIAP        NIAPCut         NLSIZE  \\\n",
       "count  240975.000000  240975.000000  240893.000000  240975.000000   \n",
       "mean        0.562520       0.555206       0.552764       0.566670   \n",
       "std         0.263303       0.266878       0.271885       0.227620   \n",
       "min         0.008850       0.008547       0.008547       0.010870   \n",
       "25%         0.354167       0.351852       0.342105       0.390805   \n",
       "50%         0.576923       0.583333       0.584270       0.576923   \n",
       "75%         0.784314       0.777778       0.778947       0.750000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "       NOCFToInterestBearDebt  NOCFToNetDebt  NOCFToOperatingNI  \\\n",
       "count           236579.000000  240323.000000      240975.000000   \n",
       "mean                 0.510983       0.518368           0.528577   \n",
       "std                  0.275401       0.289141           0.278170   \n",
       "min                  0.008850       0.008696           0.008696   \n",
       "25%                  0.277778       0.266667           0.290698   \n",
       "50%                  0.511628       0.526316           0.529412   \n",
       "75%                  0.743590       0.770115           0.767442   \n",
       "max                  1.000000       1.000000           1.000000   \n",
       "\n",
       "       NOCFToOperatingNILatest  NOCFToTLiability      NPCutToNP  \\\n",
       "count            240975.000000     240975.000000  240893.000000   \n",
       "mean                  0.509163          0.520841       0.502264   \n",
       "std                   0.285318          0.281347       0.278127   \n",
       "min                   0.008547          0.009009       0.008547   \n",
       "25%                   0.263158          0.281818       0.266055   \n",
       "50%                   0.508197          0.525424       0.500000   \n",
       "75%                   0.755102          0.758621       0.736364   \n",
       "max                   1.000000          1.000000       1.000000   \n",
       "\n",
       "       NPFromOperatingTTM  NPFromValueChgTTM  NPParentCompanyCutYOY  \\\n",
       "count       240975.000000      240975.000000          238390.000000   \n",
       "mean             0.548113           0.538793               0.515852   \n",
       "std              0.275714           0.287912               0.270107   \n",
       "min              0.008547           0.008547               0.008696   \n",
       "25%              0.325581           0.295775               0.291667   \n",
       "50%              0.580000           0.560976               0.517857   \n",
       "75%              0.781250           0.786885               0.741935   \n",
       "max              1.000000           1.000000               1.000000   \n",
       "\n",
       "       NPParentCompanyGrowRate        NPToTOR   NRProfitLoss            NVI  \\\n",
       "count            239556.000000  240975.000000  240823.000000  240975.000000   \n",
       "mean                  0.521146       0.528543       0.534344       0.503249   \n",
       "std                   0.272812       0.278037       0.283747       0.267908   \n",
       "min                   0.008929       0.008696       0.008547       0.008547   \n",
       "25%                   0.294118       0.290909       0.300000       0.283333   \n",
       "50%                   0.524590       0.529412       0.550459       0.500000   \n",
       "75%                   0.750000       0.767442       0.777778       0.725000   \n",
       "max                   1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "         NegMktValue  NetAssetGrowRate     NetAssetPS  NetCashFlowGrowRate  \\\n",
       "count  240975.000000     240714.000000  240975.000000        234523.000000   \n",
       "mean        0.589273          0.520324       0.538170             0.496576   \n",
       "std         0.239153          0.273867       0.278693             0.284718   \n",
       "min         0.009434          0.008696       0.010870             0.008696   \n",
       "25%         0.423077          0.290323       0.302326             0.250000   \n",
       "50%         0.615385          0.526316       0.541667             0.491803   \n",
       "75%         0.782609          0.750000       0.780000             0.741573   \n",
       "max         1.000000          1.000000       1.000000             1.000000   \n",
       "\n",
       "             NetDebt  NetFinanceCFTTM  NetIntExpense  NetInvestCFTTM  \\\n",
       "count  240323.000000    240975.000000  240323.000000   240975.000000   \n",
       "mean        0.536407         0.506468       0.535886        0.475746   \n",
       "std         0.290790         0.296847       0.289999        0.286112   \n",
       "min         0.008547         0.008547       0.008547        0.009174   \n",
       "25%         0.282051         0.236364       0.285714        0.229885   \n",
       "50%         0.555556         0.505495       0.555556        0.448276   \n",
       "75%         0.789474         0.772727       0.788732        0.711111   \n",
       "max         1.000000         1.000000       1.000000        1.000000   \n",
       "\n",
       "        NetNonOIToTP  NetNonOIToTPLatest  NetOperateCFTTM  NetProfitAPTTM  \\\n",
       "count  240838.000000       240975.000000    240975.000000   240975.000000   \n",
       "mean        0.516370            0.518394         0.538555        0.557758   \n",
       "std         0.277793            0.278367         0.287267        0.264091   \n",
       "min         0.008696            0.008696         0.008696        0.008547   \n",
       "25%         0.280702            0.285714         0.296296        0.359223   \n",
       "50%         0.518519            0.520408         0.568966        0.586957   \n",
       "75%         0.754098            0.755814         0.783784        0.777778   \n",
       "max         1.000000            1.000000         1.000000        1.000000   \n",
       "\n",
       "       NetProfitCashCover  NetProfitGrowRate  NetProfitGrowRate3Y  \\\n",
       "count       238611.000000      239514.000000        230295.000000   \n",
       "mean             0.509734           0.521944             0.485036   \n",
       "std              0.278120           0.273062             0.266760   \n",
       "min              0.008696           0.008547             0.009009   \n",
       "25%              0.272727           0.295082             0.263158   \n",
       "50%              0.512821           0.523810             0.480769   \n",
       "75%              0.743590           0.750000             0.700000   \n",
       "max              1.000000           1.000000             1.000000   \n",
       "\n",
       "       NetProfitGrowRate5Y  NetProfitRatio   NetProfitTTM  NetTangibleAssets  \\\n",
       "count        213185.000000   240975.000000  240975.000000      240975.000000   \n",
       "mean              0.460909        0.528577       0.557598           0.573509   \n",
       "std               0.256789        0.278170       0.264985           0.251019   \n",
       "min               0.009091        0.008696       0.008547           0.008696   \n",
       "25%               0.250000        0.290698       0.354839           0.384615   \n",
       "50%               0.450000        0.529412       0.586207           0.600000   \n",
       "75%               0.666667        0.767442       0.777778           0.777778   \n",
       "max               1.000000        1.000000       1.000000           1.000000   \n",
       "\n",
       "       NetWorkingCapital  NonCurrentAssetsRatio  NonOperatingNPTTM  \\\n",
       "count      240323.000000          240323.000000      240975.000000   \n",
       "mean            0.541004               0.513824           0.526735   \n",
       "std             0.284026               0.283965           0.283083   \n",
       "min             0.008547               0.009009           0.008547   \n",
       "25%             0.302326               0.265625           0.285714   \n",
       "50%             0.571429               0.513158           0.540000   \n",
       "75%             0.785714               0.760870           0.771930   \n",
       "max             1.000000               1.000000           1.000000   \n",
       "\n",
       "                 OBV          OBV20           OBV6  OperCashFlowPS  \\\n",
       "count  240975.000000  240975.000000  240975.000000   240975.000000   \n",
       "mean        0.549135       0.549146       0.549140        0.527981   \n",
       "std         0.279727       0.279630       0.279692        0.288430   \n",
       "min         0.008547       0.008547       0.008547        0.008621   \n",
       "25%         0.319149       0.319149       0.319149        0.280702   \n",
       "50%         0.559140       0.558824       0.559322        0.535714   \n",
       "75%         0.789474       0.789474       0.789474        0.778947   \n",
       "max         1.000000       1.000000       1.000000        1.000000   \n",
       "\n",
       "       OperCashGrowRate  OperCashInToAsset  OperCashInToCurrentLiability  \\\n",
       "count     236104.000000      240975.000000                 240323.000000   \n",
       "mean           0.512223           0.517425                      0.520362   \n",
       "std            0.281189           0.283658                      0.282442   \n",
       "min            0.008696           0.008696                      0.009009   \n",
       "25%            0.270588           0.274194                      0.280000   \n",
       "50%            0.517857           0.520000                      0.526316   \n",
       "75%            0.750000           0.762500                      0.761905   \n",
       "max            1.000000           1.000000                      1.000000   \n",
       "\n",
       "       OperateNetIncome  OperateProfitTTM  OperatingCycle  \\\n",
       "count     240975.000000     240975.000000   239548.000000   \n",
       "mean           0.547313          0.555150        0.509553   \n",
       "std            0.277743          0.268366        0.282796   \n",
       "min            0.008547          0.008547        0.008547   \n",
       "25%            0.321429          0.348837        0.261905   \n",
       "50%            0.580000          0.585859        0.511364   \n",
       "75%            0.781818          0.778947        0.758065   \n",
       "max            1.000000          1.000000        1.000000   \n",
       "\n",
       "       OperatingExpenseRate  OperatingNIToTP  OperatingNIToTPLatest  \\\n",
       "count         237715.000000    238770.000000          240975.000000   \n",
       "mean               0.503174         0.515553               0.498410   \n",
       "std                0.280295         0.275072               0.280302   \n",
       "min                0.008929         0.008547               0.008547   \n",
       "25%                0.264706         0.281818               0.259259   \n",
       "50%                0.500000         0.518519               0.500000   \n",
       "75%                0.741935         0.750000               0.732673   \n",
       "max                1.000000         1.000000               1.000000   \n",
       "\n",
       "       OperatingProfitGrowRate  OperatingProfitPS  OperatingProfitPSLatest  \\\n",
       "count            238413.000000      240975.000000            240975.000000   \n",
       "mean                  0.515682           0.541722                 0.540927   \n",
       "std                   0.272662           0.279747                 0.279155   \n",
       "min                   0.009009           0.008547                 0.008547   \n",
       "25%                   0.285714           0.307692                 0.307692   \n",
       "50%                   0.517241           0.553571                 0.552083   \n",
       "75%                   0.744186           0.782609                 0.781818   \n",
       "max                   1.000000           1.000000                 1.000000   \n",
       "\n",
       "       OperatingProfitRatio  OperatingProfitToTOR  OperatingRevenueGrowRate  \\\n",
       "count         240975.000000         240975.000000             240960.000000   \n",
       "mean               0.532075              0.531980                  0.514277   \n",
       "std                0.277770              0.277673                  0.278694   \n",
       "min                0.008696              0.008696                  0.008696   \n",
       "25%                0.294118              0.294690                  0.278689   \n",
       "50%                0.537037              0.537037                  0.515152   \n",
       "75%                0.770492              0.770492                  0.750000   \n",
       "max                1.000000              1.000000                  1.000000   \n",
       "\n",
       "       OperatingRevenueGrowRate3Y  OperatingRevenueGrowRate5Y  \\\n",
       "count               234614.000000               217825.000000   \n",
       "mean                     0.505732                    0.485585   \n",
       "std                      0.269683                    0.259308   \n",
       "min                      0.009091                    0.009901   \n",
       "25%                      0.281250                    0.269663   \n",
       "50%                      0.508772                    0.483871   \n",
       "75%                      0.730435                    0.693182   \n",
       "max                      1.000000                    1.000000   \n",
       "\n",
       "       OperatingRevenuePS  OperatingRevenuePSLatest             PB  \\\n",
       "count       240975.000000             240975.000000  240975.000000   \n",
       "mean             0.535291                  0.534322       0.489925   \n",
       "std              0.281553                  0.281506       0.279862   \n",
       "min              0.008696                  0.008850       0.009259   \n",
       "25%              0.298851                  0.300000       0.250000   \n",
       "50%              0.542056                  0.540000       0.482143   \n",
       "75%              0.777778                  0.777778       0.727273   \n",
       "max              1.000000                  1.000000       1.000000   \n",
       "\n",
       "              PBIndu            PCF        PCFIndu             PE  \\\n",
       "count  240618.000000  240975.000000  235969.000000  240975.000000   \n",
       "mean        0.487875       0.516832       0.481298       0.503890   \n",
       "std         0.278360       0.278216       0.279353       0.277994   \n",
       "min         0.009901       0.008547       0.008547       0.008547   \n",
       "25%         0.250000       0.285099       0.241379       0.267857   \n",
       "50%         0.481481       0.519231       0.467742       0.494118   \n",
       "75%         0.723404       0.750000       0.717391       0.738095   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "               PEG3Y          PEG5Y      PEHist120       PEHist20  \\\n",
       "count  230458.000000  213266.000000  240975.000000  240975.000000   \n",
       "mean        0.490304       0.459194       0.509016       0.507872   \n",
       "std         0.270367       0.256100       0.276908       0.282378   \n",
       "min         0.008547       0.008547       0.008696       0.008547   \n",
       "25%         0.260870       0.245283       0.275862       0.264706   \n",
       "50%         0.495413       0.456140       0.505747       0.504902   \n",
       "75%         0.712121       0.655738       0.741935       0.750000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "           PEHist250       PEHist60         PEIndu         PLRC12  \\\n",
       "count  240975.000000  240975.000000  238270.000000  240975.000000   \n",
       "mean        0.511542       0.508710       0.472297       0.507897   \n",
       "std         0.275475       0.279597       0.280984       0.287558   \n",
       "min         0.008696       0.008621       0.008850       0.008547   \n",
       "25%         0.280488       0.270270       0.227273       0.258621   \n",
       "50%         0.510204       0.505747       0.456140       0.500000   \n",
       "75%         0.742857       0.747126       0.704708       0.757576   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "               PLRC6             PS         PSIndu            PSY  \\\n",
       "count  240975.000000  240975.000000  240618.000000  240975.000000   \n",
       "mean        0.508991       0.485048       0.484785       0.508000   \n",
       "std         0.287537       0.281106       0.281027       0.276443   \n",
       "min         0.008547       0.009091       0.011494       0.008621   \n",
       "25%         0.259259       0.242424       0.242105       0.275000   \n",
       "50%         0.506329       0.474747       0.473684       0.505155   \n",
       "75%         0.758242       0.722222       0.722222       0.741379   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                 PVI            PVT          PVT12           PVT6  \\\n",
       "count  240975.000000  240975.000000  240975.000000  240975.000000   \n",
       "mean        0.527190       0.513371       0.513565       0.513774   \n",
       "std         0.271164       0.291159       0.292505       0.291832   \n",
       "min         0.008547       0.008547       0.008547       0.008547   \n",
       "25%         0.303571       0.259259       0.258065       0.259259   \n",
       "50%         0.535211       0.515152       0.516667       0.516129   \n",
       "75%         0.752809       0.769231       0.770492       0.769231   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "       PeriodCostsRate        Price1M        Price1Y        Price3M  \\\n",
       "count    235858.000000  240975.000000  240975.000000  240975.000000   \n",
       "mean          0.488387       0.507230       0.498093       0.504961   \n",
       "std           0.277398       0.282949       0.283915       0.283814   \n",
       "min           0.009091       0.008547       0.008547       0.008547   \n",
       "25%           0.252525       0.263158       0.252336       0.260870   \n",
       "50%           0.482456       0.500000       0.491803       0.500000   \n",
       "75%           0.721154       0.750000       0.739130       0.747664   \n",
       "max           1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "          QuickRatio           RC12           RC24            REC  \\\n",
       "count  239868.000000  240969.000000  240955.000000  237872.000000   \n",
       "mean        0.509939       0.506898       0.504297       0.519377   \n",
       "std         0.278319       0.282996       0.282878       0.268183   \n",
       "min         0.008850       0.008547       0.008547       0.008696   \n",
       "25%         0.272727       0.263158       0.262295       0.291667   \n",
       "50%         0.505747       0.500000       0.500000       0.525316   \n",
       "75%         0.745455       0.750000       0.745098       0.760000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              REVS10        REVS120         REVS20    REVS20Indu1  \\\n",
       "count  240975.000000  240918.000000  240973.000000  240973.000000   \n",
       "mean        0.507093       0.492179       0.504897       0.504898   \n",
       "std         0.282717       0.279658       0.282632       0.282756   \n",
       "min         0.008547       0.008547       0.008547       0.008547   \n",
       "25%         0.263158       0.250000       0.262295       0.262295   \n",
       "50%         0.500000       0.486486       0.500000       0.500000   \n",
       "75%         0.750000       0.730769       0.746032       0.746988   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "             REVS250          REVS5     REVS5Indu1       REVS5m20  \\\n",
       "count  238472.000000  240975.000000  240975.000000  240973.000000   \n",
       "mean        0.480107       0.508967       0.508253       0.513983   \n",
       "std         0.275431       0.282837       0.283164       0.282894   \n",
       "min         0.008547       0.008547       0.008547       0.008547   \n",
       "25%         0.241379       0.264706       0.264151       0.270833   \n",
       "50%         0.473684       0.507937       0.505747       0.516484   \n",
       "75%         0.710843       0.750000       0.750000       0.757009   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "            REVS5m60         REVS60        REVS750            ROA  \\\n",
       "count  240965.000000  240965.000000  221209.000000  240975.000000   \n",
       "mean        0.514516       0.499545       0.457340       0.508967   \n",
       "std         0.281511       0.281492       0.263891       0.282837   \n",
       "min         0.008547       0.008547       0.008547       0.008547   \n",
       "25%         0.273585       0.258065       0.230769       0.264706   \n",
       "50%         0.517857       0.500000       0.451613       0.507937   \n",
       "75%         0.756098       0.739130       0.672566       0.750000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                ROA5        ROAEBIT     ROAEBITTTM          ROC20  \\\n",
       "count  232122.000000  240323.000000  240323.000000  240965.000000   \n",
       "mean        0.527033       0.532418       0.527375       0.504914   \n",
       "std         0.262871       0.276294       0.276352       0.282698   \n",
       "min         0.008696       0.008696       0.008696       0.008547   \n",
       "25%         0.311966       0.300000       0.293103       0.262295   \n",
       "50%         0.531250       0.538462       0.535354       0.500000   \n",
       "75%         0.744186       0.767857       0.759259       0.746988   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                ROC6            ROE           ROE5         ROEAvg  \\\n",
       "count  240967.000000  240910.000000  231584.000000  240975.000000   \n",
       "mean        0.508506       0.530631       0.529767       0.535496   \n",
       "std         0.282605       0.274739       0.260386       0.275910   \n",
       "min         0.008547       0.008929       0.008696       0.008696   \n",
       "25%         0.264706       0.300000       0.317460       0.304348   \n",
       "50%         0.505618       0.537634       0.536364       0.545455   \n",
       "75%         0.750000       0.766667       0.742857       0.770567   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              ROECut  ROECutWeighted     ROEDiluted    ROEWeighted  \\\n",
       "count  240893.000000   240885.000000  240975.000000  240893.000000   \n",
       "mean        0.535728        0.538870       0.536361       0.533081   \n",
       "std         0.276735        0.275056       0.276407       0.275234   \n",
       "min         0.008547        0.008696       0.008547       0.008547   \n",
       "25%         0.304348        0.310345       0.304348       0.303571   \n",
       "50%         0.546512        0.550000       0.545455       0.542857   \n",
       "75%         0.771739        0.775000       0.771584       0.767857   \n",
       "max         1.000000        1.000000       1.000000       1.000000   \n",
       "\n",
       "                ROIC            RSI         RSTR12         RSTR24  \\\n",
       "count  240766.000000  240967.000000  240975.000000  240975.000000   \n",
       "mean        0.529929       0.506008       0.491286       0.497006   \n",
       "std         0.275048       0.284046       0.282002       0.282532   \n",
       "min         0.008547       0.008547       0.008547       0.008547   \n",
       "25%         0.298246       0.261682       0.247706       0.252632   \n",
       "50%         0.537037       0.500000       0.483871       0.500000   \n",
       "75%         0.760563       0.750000       0.730769       0.736842   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "             RSTR504            RVI         Rank1M  RealizedVolatility  \\\n",
       "count  240953.000000  240967.000000  240973.000000       240975.000000   \n",
       "mean        0.480581       0.507493       0.504692            0.503913   \n",
       "std         0.275131       0.285146       0.282853            0.282813   \n",
       "min         0.008547       0.008621       0.008547            0.008696   \n",
       "25%         0.243119       0.261682       0.261682            0.260870   \n",
       "50%         0.473214       0.504587       0.500000            0.500000   \n",
       "75%         0.709091       0.750000       0.746032            0.744898   \n",
       "max         1.000000       1.000000       1.000000            1.000000   \n",
       "\n",
       "       RetainedEarningRatio  RetainedEarnings  RetainedEarningsPS  \\\n",
       "count         222469.000000     240975.000000       240975.000000   \n",
       "mean               0.467913          0.577758            0.553884   \n",
       "std                0.261306          0.256189            0.276614   \n",
       "min                0.008547          0.008696            0.008696   \n",
       "25%                0.250000          0.388889            0.320755   \n",
       "50%                0.464286          0.607143            0.567568   \n",
       "75%                0.685185          0.788462            0.793103   \n",
       "max                1.000000          1.000000            1.000000   \n",
       "\n",
       "          RevenueTTM            SBM         SFY12P           SGRO  \\\n",
       "count  240975.000000  240965.000000  236258.000000  240950.000000   \n",
       "mean        0.562187       0.527720       0.512734       0.517636   \n",
       "std         0.262618       0.282443       0.276551       0.275453   \n",
       "min         0.010870       0.008696       0.008547       0.009804   \n",
       "25%         0.351351       0.288136       0.280374       0.285714   \n",
       "50%         0.578947       0.535714       0.517241       0.526316   \n",
       "75%         0.784314       0.770833       0.747664       0.750000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                SRMI            STM           STOA           STOM  \\\n",
       "count  240967.000000  240965.000000  240975.000000  240975.000000   \n",
       "mean        0.507073       0.519655       0.506119       0.500501   \n",
       "std         0.282733       0.283812       0.278039       0.276181   \n",
       "min         0.008547       0.008547       0.008547       0.008547   \n",
       "25%         0.263158       0.275862       0.269231       0.265625   \n",
       "50%         0.500000       0.526316       0.500000       0.500000   \n",
       "75%         0.750000       0.765625       0.738095       0.733333   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                STOQ            SUE           SUOI  SaleServiceCashToOR  \\\n",
       "count  240975.000000  237519.000000  237519.000000        240975.000000   \n",
       "mean        0.499494       0.511625       0.508139             0.517746   \n",
       "std         0.275870       0.282596       0.280907             0.281187   \n",
       "min         0.008547       0.009009       0.009009             0.008850   \n",
       "25%         0.264706       0.266667       0.269231             0.277778   \n",
       "50%         0.500000       0.515152       0.509091             0.523810   \n",
       "75%         0.733333       0.752941       0.746032             0.758065   \n",
       "max         1.000000       1.000000       1.000000             1.000000   \n",
       "\n",
       "       SaleServiceRenderCashTTM  SalesCostRatio  SalesExpenseTTM  \\\n",
       "count             240975.000000   240975.000000    237715.000000   \n",
       "mean                   0.563041        0.500351         0.547823   \n",
       "std                    0.262463        0.284423         0.269923   \n",
       "min                    0.010417        0.009091         0.009259   \n",
       "25%                    0.350000        0.254545         0.329114   \n",
       "50%                    0.580247        0.500000         0.568627   \n",
       "75%                    0.785714        0.745098         0.777778   \n",
       "max                    1.000000        1.000000         1.000000   \n",
       "\n",
       "       SalesServiceCashToORLatest  ShareholderFCFPS  SharpeRatio120  \\\n",
       "count               240975.000000     240515.000000   240975.000000   \n",
       "mean                     0.514588          0.518403        0.494461   \n",
       "std                      0.284763          0.290508        0.281607   \n",
       "min                      0.008547          0.008696        0.008547   \n",
       "25%                      0.268817          0.264151        0.250000   \n",
       "50%                      0.517857          0.524272        0.488372   \n",
       "75%                      0.760870          0.771429        0.735294   \n",
       "max                      1.000000          1.000000        1.000000   \n",
       "\n",
       "       SharpeRatio20  SharpeRatio60       Skewness       StaticPE  \\\n",
       "count  240975.000000  240975.000000  240975.000000  240728.000000   \n",
       "mean        0.504986       0.500043       0.508357       0.489864   \n",
       "std         0.284069       0.283202       0.286517       0.271747   \n",
       "min         0.008547       0.008547       0.008547       0.008547   \n",
       "25%         0.260870       0.255814       0.260870       0.260000   \n",
       "50%         0.500000       0.500000       0.508772       0.473684   \n",
       "75%         0.750000       0.740741       0.754386       0.716667   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "       SuperQuickRatio  SurplusReserveFundPS     SwingIndex          TA2EV  \\\n",
       "count    240323.000000         239960.000000  240975.000000  240975.000000   \n",
       "mean          0.508513              0.525484       0.508438       0.532626   \n",
       "std           0.277145              0.285942       0.287841       0.280940   \n",
       "min           0.008850              0.008547       0.008547       0.008850   \n",
       "25%           0.273684              0.281250       0.259259       0.296296   \n",
       "50%           0.500000              0.536585       0.505495       0.537037   \n",
       "75%           0.743119              0.770833       0.758065       0.775000   \n",
       "max           1.000000              1.000000       1.000000       1.000000   \n",
       "\n",
       "            TCostTTM           TEAP         TEMA10          TEMA5  \\\n",
       "count  240975.000000  240975.000000  240975.000000  240975.000000   \n",
       "mean        0.558829       0.580989       0.535430       0.535430   \n",
       "std         0.266132       0.239645       0.282843       0.282857   \n",
       "min         0.010417       0.010753       0.009091       0.009091   \n",
       "25%         0.342857       0.397849       0.296296       0.296296   \n",
       "50%         0.575758       0.603448       0.549020       0.549020   \n",
       "75%         0.785714       0.775000       0.779070       0.779070   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                TOBT          TORPS    TORPSLatest     TProfitTTM  \\\n",
       "count  240178.000000  240975.000000  240975.000000  240975.000000   \n",
       "mean        0.510907       0.535433       0.534323       0.558386   \n",
       "std         0.273201       0.281030       0.281060       0.264489   \n",
       "min         0.008696       0.008696       0.008850       0.008547   \n",
       "25%         0.284211       0.300000       0.300000       0.357143   \n",
       "50%         0.517857       0.542169       0.540000       0.586957   \n",
       "75%         0.740741       0.777778       0.775862       0.777778   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              TRIX10          TRIX5    TRevenueTTM  TSEPToInterestBearDebt  \\\n",
       "count  240975.000000  240975.000000  240975.000000           236816.000000   \n",
       "mean        0.504979       0.506818       0.561785                0.497552   \n",
       "std         0.283338       0.282956       0.262349                0.276139   \n",
       "min         0.008547       0.008547       0.010870                0.008696   \n",
       "25%         0.261682       0.263158       0.350877                0.262295   \n",
       "50%         0.500000       0.500000       0.578947                0.491228   \n",
       "75%         0.750000       0.750000       0.783076                0.730769   \n",
       "max         1.000000       1.000000       1.000000                1.000000   \n",
       "\n",
       "       TSEPToTotalCapital         TVMA20          TVMA6        TVSTD20  \\\n",
       "count       240323.000000  240975.000000  240975.000000  240975.000000   \n",
       "mean             0.500601       0.554286       0.552568       0.543012   \n",
       "std              0.278937       0.264451       0.265726       0.270956   \n",
       "min              0.008696       0.009091       0.008850       0.008547   \n",
       "25%              0.263736       0.340426       0.336842       0.320000   \n",
       "50%              0.495146       0.565217       0.562500       0.551724   \n",
       "75%              0.737705       0.777778       0.777778       0.772727   \n",
       "max              1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              TVSTD6  TangibleAToInteBearDebt  TangibleAToNetDebt  \\\n",
       "count  240975.000000            236816.000000       240323.000000   \n",
       "mean        0.541772                 0.496838            0.523794   \n",
       "std         0.273480                 0.277037            0.286589   \n",
       "min         0.008696                 0.008696            0.008696   \n",
       "25%         0.315789                 0.261364            0.280000   \n",
       "50%         0.550000                 0.491228            0.533333   \n",
       "75%         0.775000                 0.727273            0.770642   \n",
       "max         1.000000                 1.000000            1.000000   \n",
       "\n",
       "            TaxRatio  TotalAssetGrowRate    TotalAssets  TotalAssetsTRate  \\\n",
       "count  240755.000000       240718.000000  240975.000000     240975.000000   \n",
       "mean        0.508597            0.519389       0.569464          0.517743   \n",
       "std         0.283462            0.275543       0.248716          0.279660   \n",
       "min         0.008696            0.008696       0.015385          0.008696   \n",
       "25%         0.266667            0.288462       0.379310          0.280374   \n",
       "50%         0.508772            0.524590       0.580000          0.521739   \n",
       "75%         0.750000            0.750000       0.771930          0.752294   \n",
       "max         1.000000            1.000000       1.000000          1.000000   \n",
       "\n",
       "       TotalFixedAssets  TotalPaidinCapital  TotalProfitCostRatio  \\\n",
       "count     240323.000000       240323.000000         240975.000000   \n",
       "mean           0.550162            0.573916              0.529695   \n",
       "std            0.273085            0.248405              0.278344   \n",
       "min            0.010417            0.010870              0.008696   \n",
       "25%            0.327586            0.384615              0.294118   \n",
       "50%            0.566038            0.586207              0.532258   \n",
       "75%            0.782609            0.777778              0.769231   \n",
       "max            1.000000            1.000000              1.000000   \n",
       "\n",
       "       TotalProfitGrowRate  TreynorRatio120  TreynorRatio20  TreynorRatio60  \\\n",
       "count        239757.000000    240975.000000   240975.000000   240975.000000   \n",
       "mean              0.521970         0.495088        0.507259        0.501650   \n",
       "std               0.273504         0.277922        0.281278        0.279772   \n",
       "min               0.008850         0.008547        0.008547        0.008696   \n",
       "25%               0.295082         0.257143        0.265306        0.261682   \n",
       "50%               0.523810         0.490196        0.500000        0.500000   \n",
       "75%               0.750000         0.730769        0.747057        0.738636   \n",
       "max               1.000000         1.000000        1.000000        1.000000   \n",
       "\n",
       "                 UOS        Ulcer10         Ulcer5  UndividedProfitPS  \\\n",
       "count  240975.000000  240975.000000  240975.000000      240975.000000   \n",
       "mean        0.504956       0.499420       0.502425           0.555228   \n",
       "std         0.283628       0.284325       0.283965           0.275742   \n",
       "min         0.008547       0.008547       0.008547           0.008696   \n",
       "25%         0.261905       0.254237       0.258621           0.325581   \n",
       "50%         0.500000       0.500000       0.500000           0.571429   \n",
       "75%         0.747475       0.740741       0.744186           0.793651   \n",
       "max         1.000000       1.000000       1.000000           1.000000   \n",
       "\n",
       "               UpRVI           VDEA          VDIFF         VEMA10  \\\n",
       "count  240967.000000  240975.000000  240975.000000  240975.000000   \n",
       "mean        0.521690       0.508487       0.507276       0.551897   \n",
       "std         0.284317       0.291401       0.291815       0.275162   \n",
       "min         0.008696       0.008547       0.008547       0.008696   \n",
       "25%         0.278689       0.252632       0.250000       0.333333   \n",
       "50%         0.527778       0.500000       0.500000       0.568627   \n",
       "75%         0.767857       0.764706       0.764706       0.786885   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              VEMA12         VEMA26          VEMA5          VMACD  \\\n",
       "count  240975.000000  240975.000000  240975.000000  240975.000000   \n",
       "mean        0.552014       0.552468       0.551241       0.509486   \n",
       "std         0.275173       0.275396       0.275216       0.292839   \n",
       "min         0.008696       0.008696       0.008696       0.008547   \n",
       "25%         0.333333       0.333333       0.327869       0.250000   \n",
       "50%         0.569767       0.571429       0.567568       0.508772   \n",
       "75%         0.786885       0.787234       0.785714       0.767442   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "               VOL10         VOL120          VOL20         VOL240  \\\n",
       "count  240975.000000  240975.000000  240975.000000  240975.000000   \n",
       "mean        0.501465       0.497602       0.500456       0.497955   \n",
       "std         0.276680       0.274512       0.276102       0.274798   \n",
       "min         0.008547       0.008547       0.008547       0.008547   \n",
       "25%         0.266092       0.264706       0.266055       0.264706   \n",
       "50%         0.500000       0.500000       0.500000       0.495327   \n",
       "75%         0.734940       0.729412       0.733333       0.727273   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                VOL5          VOL60             VR         VROC12  \\\n",
       "count  240975.000000  240975.000000  240975.000000  240975.000000   \n",
       "mean        0.501945       0.498760       0.507253       0.510281   \n",
       "std         0.277240       0.275435       0.285019       0.283094   \n",
       "min         0.008547       0.008547       0.008547       0.008547   \n",
       "25%         0.266006       0.264151       0.263158       0.266667   \n",
       "50%         0.500000       0.500000       0.500000       0.510000   \n",
       "75%         0.735849       0.732143       0.750000       0.752577   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "               VROC6         VSTD10         VSTD20  ValueChgProfit  \\\n",
       "count  240975.000000  240975.000000  240975.000000   240975.000000   \n",
       "mean        0.510882       0.542771       0.542900        0.536571   \n",
       "std         0.283519       0.277751       0.277850        0.291530   \n",
       "min         0.008547       0.008621       0.008696        0.008547   \n",
       "25%         0.266667       0.314815       0.314815        0.295918   \n",
       "50%         0.510638       0.555556       0.555556        0.558824   \n",
       "75%         0.754386       0.781250       0.781250        0.789474   \n",
       "max         1.000000       1.000000       1.000000        1.000000   \n",
       "\n",
       "         Variance120     Variance20     Variance60     Volatility  \\\n",
       "count  240975.000000  240975.000000  240975.000000  240975.000000   \n",
       "mean        0.490196       0.498137       0.492984       0.495046   \n",
       "std         0.276843       0.281987       0.279816       0.286348   \n",
       "min         0.008547       0.008547       0.008547       0.008547   \n",
       "25%         0.253165       0.256410       0.252874       0.245902   \n",
       "50%         0.483333       0.494253       0.485981       0.489362   \n",
       "75%         0.722222       0.736842       0.730159       0.739130   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "            Volumn1M       Volumn3M           WVAD  WorkingCapital  \\\n",
       "count  240973.000000  240965.000000  240972.000000   240323.000000   \n",
       "mean        0.511774       0.510863       0.512726        0.541004   \n",
       "std         0.282612       0.280333       0.291946        0.284026   \n",
       "min         0.008547       0.008547       0.008547        0.008547   \n",
       "25%         0.269231       0.269663       0.255814        0.302326   \n",
       "50%         0.511628       0.510638       0.516667        0.571429   \n",
       "75%         0.754098       0.750000       0.769231        0.785714   \n",
       "max         1.000000       1.000000       1.000000        1.000000   \n",
       "\n",
       "             minusDI         plusDI           code         chgPct  \\\n",
       "count  235908.000000  240975.000000  240975.000000  240975.000000   \n",
       "mean        0.495342       0.500961  315624.963117       0.000356   \n",
       "std         0.280474       0.284575  294946.072604       0.027588   \n",
       "min         0.008547       0.008547       5.000000      -0.104500   \n",
       "25%         0.254545       0.254902    2056.000000      -0.013000   \n",
       "50%         0.490909       0.500000  600037.000000       0.000000   \n",
       "75%         0.735632       0.744186  600575.000000       0.013300   \n",
       "max         1.000000       1.000000  603939.000000       0.102000   \n",
       "\n",
       "             alpha_1       alpha_10     alpha_100      alpha_101  \\\n",
       "count  240975.000000  240955.000000  2.409750e+05  237507.000000   \n",
       "mean       -0.217869       0.502160  6.787220e+06       0.009864   \n",
       "std         0.501355       0.285796  1.007634e+07       0.436854   \n",
       "min        -1.000000       0.000273  3.684226e+04      -0.998479   \n",
       "25%        -0.634667       0.253371  1.840736e+06      -0.301256   \n",
       "50%        -0.291389       0.505663  3.707158e+06       0.004552   \n",
       "75%         0.143569       0.748879  7.641959e+06       0.323373   \n",
       "max         1.000000       1.000000  3.225546e+08       0.997735   \n",
       "\n",
       "           alpha_102      alpha_103      alpha_104      alpha_105  \\\n",
       "count  240975.000000  240925.000000  240925.000000  240975.000000   \n",
       "mean       49.319377      48.536910       0.001951      -0.169837   \n",
       "std        24.523371      34.073537       0.361012       0.423113   \n",
       "min         0.000000       5.000000      -1.932000      -1.000000   \n",
       "25%        29.338233      15.000000      -0.122688      -0.505352   \n",
       "50%        50.738646      45.000000      -0.000028      -0.205367   \n",
       "75%        69.508491      80.000000       0.125154       0.134415   \n",
       "max       100.000000     100.000000       1.884350       1.000000   \n",
       "\n",
       "           alpha_106     alpha_107      alpha_108      alpha_109  \\\n",
       "count  240959.000000  2.409750e+05  217843.000000  240975.000000   \n",
       "mean        0.018995 -1.590789e-01      -0.711840       1.002601   \n",
       "std         2.228564  1.917752e-01       0.226502       0.159938   \n",
       "min       -67.420000 -1.000000e+00      -1.000000       0.082629   \n",
       "25%        -0.540000 -2.135434e-01      -0.918832       0.912681   \n",
       "50%        -0.010704 -8.885107e-02      -0.758957       0.986817   \n",
       "75%         0.550096 -2.759954e-02      -0.524935       1.065071   \n",
       "max        60.500000 -2.381007e-11      -0.020614       5.000000   \n",
       "\n",
       "           alpha_11      alpha_110     alpha_111      alpha_112  \\\n",
       "count  2.409750e+05  240914.000000  2.409750e+05  240859.000000   \n",
       "mean  -3.317557e+06     103.997285 -3.266880e+04      -0.048513   \n",
       "std    3.758588e+07      42.780168  6.866809e+06       0.327249   \n",
       "min   -1.717333e+09       0.000000 -3.264836e+08      -0.994187   \n",
       "25%   -8.894561e+06      73.385013 -1.284391e+06      -0.241972   \n",
       "50%   -6.312875e+05      96.955031  7.581923e+03      -0.043106   \n",
       "75%    5.255963e+06     126.821192  1.288679e+06       0.128293   \n",
       "max    1.442828e+09     795.833333  2.296963e+08       0.998553   \n",
       "\n",
       "           alpha_113      alpha_114      alpha_115      alpha_116  \\\n",
       "count  240859.000000  240973.000000  240973.000000  240973.000000   \n",
       "mean       -0.048513      -0.001245       1.456024       1.456024   \n",
       "std         0.327249       2.510209       4.701915       4.701915   \n",
       "min        -0.994187    -126.708393      -4.336789      -4.336789   \n",
       "25%        -0.241972      -0.158779      -0.021376      -0.021376   \n",
       "50%        -0.043106      -0.003764       0.007285       0.007285   \n",
       "75%         0.128293       0.138173       0.063795       0.063795   \n",
       "max         0.998553     298.744269     101.729106     101.729106   \n",
       "\n",
       "           alpha_117      alpha_118      alpha_119       alpha_12  \\\n",
       "count  240975.000000  240925.000000  240468.000000  240966.000000   \n",
       "mean        0.106584     114.715241       0.006696      -0.250042   \n",
       "std         0.145086      45.555559       0.407859       0.236466   \n",
       "min         0.000000      13.197970      -0.988011      -1.000000   \n",
       "25%         0.015121      82.051282      -0.284206      -0.362031   \n",
       "50%         0.050364     107.541522       0.008712      -0.166654   \n",
       "75%         0.136089     139.944904       0.303113      -0.069285   \n",
       "max         0.907258    4300.000000       0.992730      -0.000003   \n",
       "\n",
       "           alpha_120      alpha_121      alpha_122      alpha_123  \\\n",
       "count  240975.000000  237477.000000  240975.000000  233622.000000   \n",
       "mean        4.532614      -0.621322      -0.000048       0.020201   \n",
       "std        41.549432       0.264249       0.008653       0.435249   \n",
       "min         0.000270      -1.000000      -1.635722      -0.998412   \n",
       "25%         0.549622      -0.851163      -0.001338      -0.290442   \n",
       "50%         1.095669      -0.666243      -0.000021       0.011361   \n",
       "75%         2.015101      -0.417722       0.001249       0.334366   \n",
       "max      3051.000000      -0.001572       1.988943       0.998372   \n",
       "\n",
       "           alpha_124      alpha_125      alpha_126      alpha_127  \\\n",
       "count  240684.000000  225754.000000  240975.000000  240887.000000   \n",
       "mean       -0.013044       3.160014      10.819647       5.824981   \n",
       "std         1.479433      24.945268      10.410254       4.606471   \n",
       "min      -222.743588       0.000319       0.233333       0.000000   \n",
       "25%        -0.127633       0.516303       5.316616       2.704623   \n",
       "50%         0.006198       0.993902       8.218404       4.686986   \n",
       "75%         0.130702       1.858979      12.863333       7.561210   \n",
       "max       145.194255    2417.495840     310.810000      51.246855   \n",
       "\n",
       "           alpha_128      alpha_129       alpha_13      alpha_130  \\\n",
       "count  240975.000000  240961.000000  240975.000000  239743.000000   \n",
       "mean       54.840860       1.360542      -0.009794       5.231543   \n",
       "std        18.320454       2.025688       0.094080      54.049711   \n",
       "min         0.000000       0.000000      -6.665852       0.000410   \n",
       "25%        42.655768       0.430000      -0.022878       0.509570   \n",
       "50%        55.938441       0.811983      -0.001564       1.019622   \n",
       "75%        67.935606       1.533945       0.013527       2.079642   \n",
       "max       100.000000      80.020000       4.531358    3498.608550   \n",
       "\n",
       "           alpha_131     alpha_132      alpha_133     alpha_134  \\\n",
       "count  239573.000000  2.409250e+05  240925.000000  2.409690e+05   \n",
       "mean        0.697621  1.679766e+08       1.434969  6.772992e+05   \n",
       "std         0.273453  2.412641e+08      60.136028  5.961999e+06   \n",
       "min         0.000331  7.116736e+05     -95.000000 -1.322888e+08   \n",
       "25%         0.506694  4.668008e+07     -55.000000 -2.679149e+05   \n",
       "50%         0.787801  9.199479e+07       5.000000 -3.516476e+03   \n",
       "75%         0.933818  1.877442e+08      60.000000  4.715645e+05   \n",
       "max         1.000000  7.188162e+09      95.000000  5.363861e+08   \n",
       "\n",
       "           alpha_135      alpha_136      alpha_137      alpha_138  \\\n",
       "count  240973.000000  240975.000000  240975.000000  223339.000000   \n",
       "mean        1.007457      -0.120231      -0.450663       0.171363   \n",
       "std         0.104143       0.245590       9.296278       0.431807   \n",
       "min         0.358132      -0.988153    -307.455821      -0.856300   \n",
       "25%         0.947339      -0.262650      -2.202214      -0.124093   \n",
       "50%         0.998986      -0.089466       0.030137       0.179219   \n",
       "75%         1.058600       0.008559       1.711422       0.497576   \n",
       "max         3.582492       0.996212     548.706802       0.999528   \n",
       "\n",
       "           alpha_139       alpha_14      alpha_140      alpha_141  \\\n",
       "count  240975.000000  240974.000000  235529.000000  240961.000000   \n",
       "mean       -0.240131       0.000492       0.412638      -0.505592   \n",
       "std         0.412699       1.119579       0.245858       0.296845   \n",
       "min        -1.000000     -50.610000       0.000279      -1.000000   \n",
       "25%        -0.560181      -0.261953       0.265000      -0.764758   \n",
       "50%        -0.290112       0.004805       0.333333      -0.506995   \n",
       "75%         0.035099       0.268254       0.608206      -0.247197   \n",
       "max         1.000000      49.600000       1.000000      -0.000274   \n",
       "\n",
       "          alpha_142      alpha_143     alpha_144      alpha_145  \\\n",
       "count  2.409250e+05  240975.000000  2.409140e+05  240826.000000   \n",
       "mean  -1.441562e-01       0.190296  3.468979e-10      -6.349429   \n",
       "std    1.833326e-01       0.115226  5.209246e-10      32.000689   \n",
       "min   -9.567258e-01       0.000405  3.868936e-12    -330.023568   \n",
       "25%   -1.852866e-01       0.111523  1.129987e-10     -24.511799   \n",
       "50%   -6.581047e-02       0.165794  2.149803e-10      -3.317600   \n",
       "75%   -2.292205e-02       0.240285  4.199825e-10      15.671387   \n",
       "max   -6.170000e-07       1.533616  9.255523e-08      83.713656   \n",
       "\n",
       "           alpha_146      alpha_147      alpha_148      alpha_149  \\\n",
       "count  240973.000000  240959.000000  236087.000000  238449.000000   \n",
       "mean        1.230589       1.588466       0.009197       0.964039   \n",
       "std        12.047502       4.860489       0.397186       0.302216   \n",
       "min      -151.644060      -4.620615      -0.993368      -0.443523   \n",
       "25%        -0.359421      -0.021065      -0.264296       0.787256   \n",
       "50%         0.019002       0.008400       0.000934       0.992346   \n",
       "75%         0.870834       0.071578       0.287114       1.170273   \n",
       "max      1269.686946      99.024538       0.997656       2.395575   \n",
       "\n",
       "            alpha_15     alpha_150      alpha_151     alpha_152  \\\n",
       "count  240975.000000  2.409750e+05  240973.000000  2.409670e+05   \n",
       "mean       -0.001265  1.440872e+08       0.017940 -9.233634e-05   \n",
       "std         0.011237  2.502416e+08       1.752965  2.020973e-02   \n",
       "min        -0.275309  1.559092e+05     -45.610559 -4.714748e-01   \n",
       "25%        -0.004980  2.867418e+07      -0.435896 -1.050869e-02   \n",
       "50%         0.000000  6.632041e+07      -0.020012  3.330669e-16   \n",
       "75%         0.002396  1.548913e+08       0.428405  1.063909e-02   \n",
       "max         0.101538  8.924972e+09      51.395536  2.207100e-01   \n",
       "\n",
       "           alpha_153      alpha_154     alpha_155      alpha_156  \\\n",
       "count  240869.000000  184375.000000  2.409750e+05  240973.000000   \n",
       "mean       10.826100      -0.601833  1.705316e+04      -0.714416   \n",
       "std        10.347183       1.589973  1.819403e+06       0.179449   \n",
       "min         0.252917     -60.917557 -1.057028e+08      -1.000000   \n",
       "25%         5.339610      -0.972999 -3.248809e+05      -0.861652   \n",
       "50%         8.240918      -0.334926 -9.523693e+02      -0.731159   \n",
       "75%        12.871455       0.249936  2.976798e+05      -0.589023   \n",
       "max       298.787812       1.000000  7.317682e+07      -0.017817   \n",
       "\n",
       "           alpha_157      alpha_158      alpha_159       alpha_16  \\\n",
       "count  240962.000000  240975.000000  240861.000000  240975.000000   \n",
       "mean        0.955786       0.038547   -2647.027205      -0.725039   \n",
       "std         0.376467       0.023840    1142.587105       0.228052   \n",
       "min         0.200281       0.000000  -14477.870518      -1.000000   \n",
       "25%         0.670781       0.022059   -3192.392645      -0.914838   \n",
       "50%         0.958741       0.032186   -2446.004940      -0.782121   \n",
       "75%         1.220547       0.047935   -1864.520885      -0.581166   \n",
       "max         1.999665       0.222484    -543.847012      -0.000280   \n",
       "\n",
       "           alpha_160      alpha_161      alpha_162      alpha_163  \\\n",
       "count  240975.000000  240961.000000  240975.000000  240925.000000   \n",
       "mean        0.146701       0.445936       0.499978       0.498000   \n",
       "std         0.220396       0.549876       0.380183       0.298284   \n",
       "min         0.000000       0.011098       0.000000       0.000274   \n",
       "25%         0.043823       0.168435       0.108062       0.229227   \n",
       "50%         0.087857       0.291460       0.499494       0.498864   \n",
       "75%         0.169591       0.515833       0.892883       0.765541   \n",
       "max         8.302637      15.123333       1.000000       1.000000   \n",
       "\n",
       "          alpha_164      alpha_165      alpha_166      alpha_167  \\\n",
       "count  2.409730e+05  204930.000000  240847.000000  240961.000000   \n",
       "mean   1.993393e+06      69.988140       0.000134       1.355779   \n",
       "std    2.096202e+08      68.383088       0.005870       1.941630   \n",
       "min    0.000000e+00    -640.835341      -0.039300       0.000000   \n",
       "25%    8.214782e+02      31.131695      -0.003030       0.417991   \n",
       "50%    3.135596e+03      56.380612      -0.000038       0.802333   \n",
       "75%    1.005314e+04      93.033166       0.003009       1.560000   \n",
       "max    5.961918e+10    1458.492815       0.050941      68.740000   \n",
       "\n",
       "           alpha_168      alpha_169       alpha_17      alpha_170  \\\n",
       "count  240925.000000  240971.000000   2.409520e+05  240925.000000   \n",
       "mean       -1.047639       0.000600  2.814547e+173      -0.286147   \n",
       "std         0.612612       0.055371            inf       0.352034   \n",
       "min       -17.523487      -1.477141   2.479430e-56      -0.999982   \n",
       "25%        -1.247281      -0.011536   9.083300e-01      -0.532839   \n",
       "50%        -0.898512       0.000571   9.989138e-01      -0.280382   \n",
       "75%        -0.662176       0.013337   1.235508e+00      -0.055636   \n",
       "max        -0.041625       1.784002  6.781707e+178       6.304742   \n",
       "\n",
       "           alpha_171      alpha_172      alpha_173      alpha_174  \\\n",
       "count  240975.000000  240925.000000  240975.000000  240975.000000   \n",
       "mean       -3.056303      35.979017      12.957392       0.148784   \n",
       "std         7.557312      19.884465      11.061382       0.221431   \n",
       "min      -464.128134       1.335389      -0.916975       0.000000   \n",
       "25%        -2.775993      19.861183       6.961465       0.042157   \n",
       "50%        -0.968317      32.300023      10.289687       0.088637   \n",
       "75%        -0.316373      49.458636      15.398042       0.173207   \n",
       "max         0.000000     100.000000     332.005744       8.563506   \n",
       "\n",
       "           alpha_175      alpha_176      alpha_177     alpha_178  \\\n",
       "count  240973.000000  240968.000000  240925.000000  2.409750e+05   \n",
       "mean        0.448273       0.291458      49.971879  7.755351e+04   \n",
       "std         0.564735       0.512424      34.600474  1.355707e+06   \n",
       "min         0.010762      -1.000000       5.000000 -5.985623e+07   \n",
       "25%         0.164366      -0.068123      15.000000 -8.686767e+04   \n",
       "50%         0.288374       0.399593      50.000000  0.000000e+00   \n",
       "75%         0.518751       0.719792      85.000000  1.062787e+05   \n",
       "max        18.650000       1.000000     100.000000  5.630381e+07   \n",
       "\n",
       "          alpha_179       alpha_18     alpha_180      alpha_181  \\\n",
       "count  2.394080e+05  240974.000000  2.409750e+05  240965.000000   \n",
       "mean   2.518993e-01       1.001715 -7.560313e+06      -0.777249   \n",
       "std    2.311085e-01       0.065763  1.414257e+07      56.657688   \n",
       "min    4.128967e-07       0.535885 -7.672276e+08   -1152.865522   \n",
       "25%    6.055760e-02       0.967026 -8.710874e+06      -2.604084   \n",
       "50%    1.826856e-01       1.000637 -3.356330e+06      -0.637430   \n",
       "75%    3.882571e-01       1.034795 -8.833333e-01       2.249876   \n",
       "max    9.982312e-01       1.617021  1.000000e+00     565.086314   \n",
       "\n",
       "           alpha_182      alpha_183      alpha_184      alpha_185  \\\n",
       "count  240975.000000  235995.000000  240975.000000  240975.000000   \n",
       "mean        0.673641      30.001292       1.027685       0.515235   \n",
       "std         0.169722      30.394664       0.391332       0.282178   \n",
       "min         0.000000    -289.303474       0.000830       0.000289   \n",
       "25%         0.600000      12.116926       0.756651       0.275966   \n",
       "50%         0.700000      24.605664       1.035074       0.523281   \n",
       "75%         0.800000      41.784454       1.308907       0.757537   \n",
       "max         1.000000     628.778412       1.994113       0.999409   \n",
       "\n",
       "           alpha_186      alpha_187      alpha_188      alpha_189  \\\n",
       "count  240826.000000  240914.000000  240975.000000  240962.000000   \n",
       "mean       35.931785       3.339376       0.734805       0.288957   \n",
       "std        16.623761       4.467129      38.910087       0.432321   \n",
       "min         3.730972       0.000000    -100.000000       0.001667   \n",
       "25%        23.059354       1.100000     -26.560031       0.088990   \n",
       "50%        33.242744       2.067185      -6.476253       0.168916   \n",
       "75%        46.202922       3.892354      20.309119       0.324045   \n",
       "max       100.000000     138.870000     450.000000      17.708333   \n",
       "\n",
       "            alpha_19      alpha_190      alpha_191        alpha_2  \\\n",
       "count  240975.000000  240054.000000  240918.000000  240975.000000   \n",
       "mean       -0.043254      -0.112718       0.155116      -0.026644   \n",
       "std         0.047686       0.795230       0.710875       0.875550   \n",
       "min        -0.617021      -5.083281     -12.300213      -2.000000   \n",
       "25%        -0.060858      -0.643006      -0.480450      -0.666667   \n",
       "50%        -0.032346      -0.112240       0.295214      -0.006325   \n",
       "75%        -0.013857       0.427397       0.775543       0.618827   \n",
       "max         0.343915       5.086490       7.314052       2.000000   \n",
       "\n",
       "            alpha_20       alpha_21       alpha_22       alpha_23  \\\n",
       "count  240975.000000  240974.000000  240932.000000  240950.000000   \n",
       "mean        0.207667       1.042345       0.000362      48.882372   \n",
       "std         7.043155       4.107713       0.015821      12.810531   \n",
       "min       -46.861925      -7.283429      -0.120330       0.000000   \n",
       "25%        -3.480278      -0.032391      -0.008554      41.374340   \n",
       "50%         0.000000       0.007769       0.000285      49.173788   \n",
       "75%         3.698630       0.067087       0.009339      56.864947   \n",
       "max        77.335640     100.187884       0.119379     100.000000   \n",
       "\n",
       "            alpha_24       alpha_25       alpha_26       alpha_27  \\\n",
       "count  240966.000000  238149.000000  240973.000000  240974.000000   \n",
       "mean       -0.000924      -0.754872       0.879356       0.328110   \n",
       "std         0.895208       0.482055       0.676291       7.316449   \n",
       "min       -35.056320      -2.000000     -33.912013     -59.188854   \n",
       "25%        -0.200754      -1.078191       0.704268      -3.445754   \n",
       "50%         0.005741      -0.687572       0.881072       0.070719   \n",
       "75%         0.211200      -0.364315       1.050854       3.898714   \n",
       "max        34.768560      -0.000547      35.575706      78.364758   \n",
       "\n",
       "            alpha_28      alpha_29        alpha_3       alpha_30  \\\n",
       "count  240969.000000  2.409730e+05  240975.000000  240975.000000   \n",
       "mean       48.738994  4.131160e+05      -0.034449       4.041927   \n",
       "std        69.622534  3.991377e+06       1.009495       4.553998   \n",
       "min      -148.862760 -2.769467e+08     -35.170000       0.000000   \n",
       "25%        -6.595143 -1.904197e+05      -0.253904       1.275985   \n",
       "50%        49.102802  4.030977e+02       0.000000       2.555362   \n",
       "75%       103.122907  3.233235e+05       0.197360       5.041505   \n",
       "max       261.632745  3.184020e+08      53.220000      86.460011   \n",
       "\n",
       "            alpha_31       alpha_32       alpha_33       alpha_34  \\\n",
       "count  240961.000000  240975.000000  238721.000000  240961.000000   \n",
       "mean        0.046532      -1.429595       0.005385       1.002641   \n",
       "std         5.497908       0.657339       0.425148       0.057211   \n",
       "min       -40.387293      -2.999030     -19.471768       0.624556   \n",
       "25%        -2.694462      -1.919321      -0.040430       0.972631   \n",
       "50%        -0.015699      -1.453215      -0.001442       1.000157   \n",
       "75%         2.813918      -0.984669       0.031300       1.027691   \n",
       "max        60.113677       0.000000      58.900891       1.677495   \n",
       "\n",
       "            alpha_35       alpha_36       alpha_37       alpha_38  \\\n",
       "count  240946.000000  240975.000000  240965.000000  240975.000000   \n",
       "mean       -0.340144       0.491076      -0.501472      -0.074577   \n",
       "std         0.234413       0.288362       0.282603       0.455767   \n",
       "min        -0.997873       0.000269      -1.000000     -31.980000   \n",
       "25%        -0.502144       0.241323      -0.743676      -0.029877   \n",
       "50%        -0.300664       0.487313      -0.502417       0.000000   \n",
       "75%        -0.145206       0.739283      -0.258404       0.000000   \n",
       "max        -0.000269       1.000000      -0.000275      14.239576   \n",
       "\n",
       "            alpha_39        alpha_4       alpha_40       alpha_41  \\\n",
       "count  164578.000000  240975.000000  240975.000000  240973.000000   \n",
       "mean       -0.005304      -0.274738       1.400739      -0.489869   \n",
       "std         0.403006       0.961521      13.231738       0.285265   \n",
       "min        -0.992915      -1.000000       0.011860      -1.000000   \n",
       "25%        -0.289031      -1.000000       0.839073      -0.734996   \n",
       "50%        -0.009652      -1.000000       1.147926      -0.485955   \n",
       "75%         0.277632       1.000000       1.547627      -0.240984   \n",
       "max         0.997625       1.000000    3836.973726      -0.000271   \n",
       "\n",
       "            alpha_42      alpha_43       alpha_44       alpha_45  \\\n",
       "count  240966.000000  2.409750e+05  240946.000000  201414.000000   \n",
       "mean       -0.238008  1.009143e+07       1.191480       0.256519   \n",
       "std         0.259150  6.410925e+07       0.509358       0.232961   \n",
       "min        -0.990058 -1.586135e+09       0.316667       0.000004   \n",
       "25%        -0.413066 -6.962079e+06       0.783333       0.064896   \n",
       "50%        -0.200070  1.896121e+06       1.166667       0.189556   \n",
       "75%        -0.052920  1.778677e+07       1.600000       0.389131   \n",
       "max         0.948055  2.556015e+09       2.000000       0.995753   \n",
       "\n",
       "            alpha_46       alpha_47       alpha_48       alpha_49  \\\n",
       "count  240869.000000  240975.000000  240925.000000  240961.000000   \n",
       "mean        1.002326      50.440034      -0.128266       0.499326   \n",
       "std         0.046933      17.869932       0.089054       0.199480   \n",
       "min         0.732932       0.000000      -0.886676       0.000000   \n",
       "25%         0.978197      37.497602      -0.174892       0.346369   \n",
       "50%         1.000346      50.377444      -0.111476       0.497797   \n",
       "75%         1.022386      63.256163      -0.061928       0.651623   \n",
       "max         1.530215     100.000000      -0.000193       1.000000   \n",
       "\n",
       "             alpha_5       alpha_50       alpha_51       alpha_52  \\\n",
       "count  240975.000000  240961.000000  240961.000000  240805.000000   \n",
       "mean       -0.617461       0.001396       0.500674     106.764448   \n",
       "std         0.403665       0.397871       0.199480      44.495613   \n",
       "min        -1.000000      -1.000000       0.000000       0.000000   \n",
       "25%        -0.917942      -0.302317       0.348377      74.106749   \n",
       "50%        -0.766965       0.004405       0.502203      99.342723   \n",
       "75%        -0.449467       0.306122       0.653631     131.535010   \n",
       "max         1.000000       1.000000       1.000000     657.948718   \n",
       "\n",
       "            alpha_53       alpha_54       alpha_55       alpha_56  \\\n",
       "count  240975.000000  240966.000000  240914.000000  237849.000000   \n",
       "mean       47.128679      -0.490482      -8.622405       0.008868   \n",
       "std        16.127809       0.284351      55.793806       0.392233   \n",
       "min         0.000000      -1.000000   -1724.149728      -0.999518   \n",
       "25%        41.666667      -0.733598     -20.405192      -0.265145   \n",
       "50%        50.000000      -0.485845      -4.312457       0.015003   \n",
       "75%        58.333333      -0.245248       8.377322       0.286898   \n",
       "max       100.000000      -0.000281    1363.517840       0.988932   \n",
       "\n",
       "           alpha_57       alpha_58       alpha_59        alpha_6  \\\n",
       "count  2.409750e+05  240975.000000  240975.000000  240974.000000   \n",
       "mean   4.938340e+01      44.953128       0.210878      -0.498392   \n",
       "std    2.287423e+01      13.192576       2.426699       0.204989   \n",
       "min    6.702527e-21       0.000000     -86.607696      -0.995742   \n",
       "25%    2.920154e+01      40.000000      -0.471209      -0.643299   \n",
       "50%    4.990231e+01      45.000000       0.070000      -0.509112   \n",
       "75%    6.955538e+01      55.000000       0.783679      -0.361731   \n",
       "max    1.000000e+02      90.000000      78.015406      -0.004754   \n",
       "\n",
       "           alpha_60       alpha_61       alpha_62      alpha_63  \\\n",
       "count  2.409210e+05  227004.000000  240975.000000  2.409750e+05   \n",
       "mean   1.210269e+07      -0.658504      -0.369486  9.967506e+11   \n",
       "std    6.866239e+07       0.238425       0.524144  4.892969e+14   \n",
       "min   -1.132213e+09      -1.000000      -1.000000 -4.955000e+06   \n",
       "25%   -7.891093e+06      -0.861775      -0.810775 -5.033621e+01   \n",
       "50%    4.115050e+06      -0.695672      -0.522710  1.000000e+02   \n",
       "75%    2.238889e+07      -0.489370      -0.016943  1.521970e+02   \n",
       "max    2.947028e+09      -0.001398       1.000000  2.401920e+17   \n",
       "\n",
       "            alpha_64       alpha_65       alpha_66       alpha_67  \\\n",
       "count  231783.000000  240974.000000  240974.000000  240975.000000   \n",
       "mean       -0.663503       1.001089      -0.108919      50.039808   \n",
       "std         0.240565       0.036543       3.654281      22.801326   \n",
       "min        -1.000000       0.797368     -32.283465       0.000000   \n",
       "25%        -0.868094       0.981504      -1.858974      30.667156   \n",
       "50%        -0.704583       0.999608       0.039216      50.964690   \n",
       "75%        -0.491604       1.018590       1.849614      69.059890   \n",
       "max        -0.001182       1.322835      20.263158     100.000000   \n",
       "\n",
       "           alpha_68       alpha_69        alpha_7      alpha_70  \\\n",
       "count  2.409750e+05  240914.000000  240974.000000  2.409740e+05   \n",
       "mean  -3.184871e-09       0.290258       0.483799  5.914137e+07   \n",
       "std    3.814653e-07       0.427406       0.355527  9.546036e+07   \n",
       "min   -5.393257e-05      -0.984802       0.000044  1.645243e+05   \n",
       "25%   -2.387963e-09      -0.012195       0.178835  1.310954e+07   \n",
       "50%   -2.036053e-10       0.393701       0.422114  2.929459e+07   \n",
       "75%    4.606029e-10       0.637500       0.738043  6.592281e+07   \n",
       "max    3.972884e-05       1.000000       1.969380  4.220538e+09   \n",
       "\n",
       "            alpha_71       alpha_72       alpha_73       alpha_74  \\\n",
       "count  240869.000000  240975.000000  240654.000000  238266.000000   \n",
       "mean        0.185763      50.449628      -0.115513       1.007942   \n",
       "std         8.140531      15.603696       0.445954       0.427054   \n",
       "min       -53.325438       0.000000      -0.999723       0.002646   \n",
       "25%        -4.064555      39.429423      -0.444342       0.707101   \n",
       "50%        -0.063412      50.445407      -0.109785       1.009127   \n",
       "75%         4.263357      61.341227       0.206776       1.310316   \n",
       "max        90.154991     100.000000       0.800000       1.997964   \n",
       "\n",
       "            alpha_75       alpha_76       alpha_77       alpha_78  \\\n",
       "count  240975.000000  240914.000000  239504.000000  240975.000000   \n",
       "mean        0.243162       0.759373       0.331662       0.609077   \n",
       "std         0.116788       0.185281       0.227638     100.583653   \n",
       "min         0.000000       0.269248       0.000275    -393.811533   \n",
       "25%         0.160000       0.654237       0.140452     -57.858311   \n",
       "50%         0.238095       0.735868       0.295416      -1.012261   \n",
       "75%         0.318182       0.828583       0.492537      60.147538   \n",
       "max         0.840000       4.347976       1.000000   28533.333333   \n",
       "\n",
       "            alpha_79        alpha_8       alpha_80      alpha_81  \\\n",
       "count  240975.000000  240974.000000  240975.000000  2.409750e+05   \n",
       "mean       49.199715      -0.500902      30.954515  1.455383e+07   \n",
       "std        24.818349       0.282431     530.272278  1.940660e+07   \n",
       "min         0.000000      -1.000000     -98.131314  8.200000e+03   \n",
       "25%        28.044940      -0.743186     -33.273201  4.416000e+06   \n",
       "50%        49.036951      -0.500000      -2.352515  8.518502e+06   \n",
       "75%        70.199823      -0.259543      45.009806  1.681707e+07   \n",
       "max       100.000000      -0.000279  115753.915293  5.002194e+08   \n",
       "\n",
       "            alpha_82       alpha_83      alpha_84       alpha_85  \\\n",
       "count  240975.000000  240975.000000  2.409140e+05  240925.000000   \n",
       "mean       50.393842      -0.488100  3.676714e+07       0.280926   \n",
       "std        15.319835       0.278556  1.245679e+08       0.238676   \n",
       "min         0.000000      -1.000000 -2.010123e+09       0.006250   \n",
       "25%        39.265986      -0.720375 -7.025896e+06       0.100000   \n",
       "50%        50.619396      -0.492156  1.212199e+07       0.200000   \n",
       "75%        61.507459      -0.252923  5.119516e+07       0.400000   \n",
       "max       100.000000      -0.000273  3.904067e+09       1.000000   \n",
       "\n",
       "            alpha_86       alpha_87       alpha_88       alpha_89  \\\n",
       "count  240975.000000  240961.000000  240959.000000  240975.000000   \n",
       "mean        0.409037      -1.077064       0.746239       0.001220   \n",
       "std         0.629594       0.434089      13.158720       0.328509   \n",
       "min        -8.096986      -2.000000     -64.213564     -11.150667   \n",
       "25%        -0.039129      -1.390691      -6.716418      -0.072608   \n",
       "50%         0.310000      -1.076210      -0.208238       0.001116   \n",
       "75%         1.000000      -0.761277       7.256686       0.080714   \n",
       "max         8.020000      -0.143145     250.966851      10.167177   \n",
       "\n",
       "            alpha_9       alpha_90       alpha_91       alpha_92  \\\n",
       "count  2.409750e+05  240975.000000  239952.000000  184604.000000   \n",
       "mean  -2.915563e-09      -0.493496      -0.261731      -0.710929   \n",
       "std    5.507456e-07       0.288568       0.217823       0.256234   \n",
       "min   -5.542381e-05      -1.000000      -0.981646      -1.000000   \n",
       "25%   -2.441408e-09      -0.741728      -0.396303      -0.938311   \n",
       "50%   -1.014994e-10      -0.490413      -0.206149      -0.751275   \n",
       "75%    9.509501e-10      -0.244149      -0.081056      -0.533333   \n",
       "max    7.199519e-05      -0.000279      -0.000003      -0.066667   \n",
       "\n",
       "            alpha_93      alpha_94      alpha_95       alpha_96      alpha_97  \\\n",
       "count  240914.000000  2.406840e+05  2.409250e+05  240975.000000  2.409750e+05   \n",
       "mean        2.021408  5.635388e+07  7.837346e+07      49.402508  5.831073e+06   \n",
       "std         2.832495  1.582944e+08  1.108456e+08      20.310674  9.322911e+06   \n",
       "min         0.000000 -1.698386e+09  4.162544e+05       0.000000  3.041156e+04   \n",
       "25%         0.666508 -4.254197e+06  2.099584e+07      32.337113  1.458137e+06   \n",
       "50%         1.253019  2.068090e+07  4.320630e+07      49.833020  3.032624e+06   \n",
       "75%         2.328216  7.397858e+07  8.965824e+07      66.556920  6.444395e+06   \n",
       "max        91.560000  3.868521e+09  3.932279e+09     100.000000  3.373454e+08   \n",
       "\n",
       "            alpha_98       alpha_99  \n",
       "count  240974.000000  240975.000000  \n",
       "mean       -0.391971      -0.487776  \n",
       "std         1.554516       0.280986  \n",
       "min       -69.000000      -1.000000  \n",
       "25%        -0.525177      -0.723499  \n",
       "50%        -0.110000      -0.493511  \n",
       "75%         0.101709      -0.244013  \n",
       "max        48.410000      -0.000273  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factor_data_org.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/workenv/vision/lib/python3.6/site-packages/pandas/core/nanops.py:670: RuntimeWarning: overflow encountered in square\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n"
     ]
    }
   ],
   "source": [
    "# 因子预处理\n",
    "## 确失值填充\n",
    "factor_mean = factor_data_org.mean()\n",
    "factor_std = factor_data_org.std()\n",
    "factor_data_org = factor_data_org.fillna(factor_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.83 s, sys: 1.93 s, total: 9.77 s\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 获取所属行业\n",
    "industry = engine.fetch_industry_range(universe, dates=ref_dates)\n",
    "# factor_data = pd.merge(factor_data_org, industry, on=['trade_date', 'code']).fillna(0.)\n",
    "factor_data = pd.merge(factor_data_org, industry, on=['trade_date', 'code'])\n",
    "\n",
    "# 获取风险因子\n",
    "risk_total = engine.fetch_risk_model_range(universe, dates=ref_dates)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.24 s, sys: 60.6 ms, total: 2.3 s\n",
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "return_data = engine.fetch_dx_return_range(universe, dates=ref_dates, horizon=horizon, offset=0,benchmark = benchmark_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.76 s, sys: 1.91 s, total: 7.67 s\n",
      "Wall time: 9.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "benchmark_total = engine.fetch_benchmark_range(dates=ref_dates, benchmark=benchmark_code)\n",
    "industry_total = engine.fetch_industry_matrix_range(universe, dates=ref_dates, category=industry_name, level=industry_level)\n",
    "\n",
    "train_data = pd.merge(factor_data, return_data, on=['trade_date', 'code']).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240975, 621)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraintes settings\n",
    "industry_names = industry_list(industry_name, industry_level)\n",
    "constraint_risk = ['EARNYILD', 'LIQUIDTY', 'GROWTH', 'SIZE', 'SIZENL', 'BETA', 'MOMENTUM'] + industry_names\n",
    "# constraint_risk = ['EARNYILD', 'LIQUIDTY', 'GROWTH', 'SIZE', 'BETA', 'MOMENTUM'] + industry_names\n",
    "\n",
    "total_risk_names = constraint_risk + ['benchmark', 'total']\n",
    "\n",
    "b_type = []\n",
    "l_val = []\n",
    "u_val = []\n",
    "\n",
    "# for name in total_risk_names:\n",
    "#     if name == 'benchmark':\n",
    "#         b_type.append(BoundaryType.RELATIVE)\n",
    "#         l_val.append(0.0)\n",
    "#         u_val.append(1.0)\n",
    "#     elif name == 'total':\n",
    "#         b_type.append(BoundaryType.ABSOLUTE)\n",
    "#         l_val.append(-0.0)\n",
    "#         u_val.append(0.0)\n",
    "#     elif name == 'SIZE':\n",
    "#         b_type.append(BoundaryType.ABSOLUTE)\n",
    "#         l_val.append(-0.1)\n",
    "#         u_val.append(0.1)\n",
    "#     elif name == 'SIZENL':\n",
    "#         b_type.append(BoundaryType.ABSOLUTE)\n",
    "#         l_val.append(-0.1)\n",
    "#         u_val.append(-0.1)\n",
    "#     elif name in industry_names:\n",
    "#         b_type.append(BoundaryType.ABSOLUTE)\n",
    "#         l_val.append(-0.005)\n",
    "#         u_val.append(0.005)\n",
    "#     else:\n",
    "#         b_type.append(BoundaryType.ABSOLUTE)\n",
    "#         l_val.append(-1.0)\n",
    "#         u_val.append(1.0)\n",
    "\n",
    "for name in total_risk_names:\n",
    "    if name == 'benchmark':\n",
    "        b_type.append(BoundaryType.RELATIVE)\n",
    "        l_val.append(0.0)\n",
    "        u_val.append(1.0)\n",
    "    elif name == 'total':\n",
    "        b_type.append(BoundaryType.ABSOLUTE)\n",
    "        l_val.append(.0)\n",
    "        u_val.append(.0)\n",
    "    else:\n",
    "        b_type.append(BoundaryType.ABSOLUTE)\n",
    "        l_val.append(-1.0)\n",
    "        u_val.append(1.0)\n",
    "\n",
    "bounds = create_box_bounds(total_risk_names, b_type, l_val, u_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取特征名\n",
    "features = list(basic_factor_store.keys())\n",
    "alpha_features = list(alpha_factor_store.keys())\n",
    "# features = feature_list.uqer_features\n",
    "# alpha_features = feature_list.alpha_features\n",
    "features.extend(alpha_features)\n",
    "\n",
    "label = ['dx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-16 07:15:22,236 - ALPHA_MIND - INFO - previous_data: 2019-12-03 00:00:00, pos_len: 0\n",
      "2019-12-16 07:15:22,238 - ALPHA_MIND - INFO - 2019-12-10 00:00:00 is start\n",
      "2019-12-16 07:15:22,239 - ALPHA_MIND - INFO - trade_date_pre 2019-12-09 00:00:00\n",
      "2019-12-16 07:15:26,427 - ALPHA_MIND - INFO - len_x_train: 240475, len_y_train: 240475\n",
      "2019-12-16 07:15:26,428 - ALPHA_MIND - INFO - X_train.shape=(240475, 614), X_test.shape = (240475, 1)\n",
      "2019-12-16 07:15:26,429 - ALPHA_MIND - INFO - params before: {'booster': 'gbtree', 'objective': 'reg:linear', 'eval_metric': ['rmse', 'logloss'], 'learning_rate': 0.01, 'max_depth': 5, 'eta': 1, 'silent': 1}\n",
      "2019-12-16 07:15:26,440 - ../../../optimization/bayes_optimization_xgb.py[line:139] - INFO: parameters: \n",
      "{'task': 'train', 'objective': 'reg:linear', 'booster': 'dart', 'eval_metric': ['rmse', 'logloss'], 'nthread': 4, 'silent': 0, 'eta': 0.1, 'max_depth': 3, 'gamma': 0, 'subsample': 0.4283444342525429, 'colsample_bytree': 0.8299121604042449, 'min_child_weight': 16, 'max_delta_step': 9, 'cv_seed': 2019}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |   gamma   | max_de... | max_depth | min_ch... | subsample |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "[07:15:34] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:15:34] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[0]\ttrain-rmse:0.453211\ttrain-logloss:0.597571\n",
      "Multiple eval metrics have been passed: 'train-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until train-logloss hasn't improved in 300 rounds.\n",
      "[07:15:35] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:15:35] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[1]\ttrain-rmse:0.408438\ttrain-logloss:0.518711\n",
      "[07:15:36] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:15:36] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[2]\ttrain-rmse:0.368213\ttrain-logloss:0.452689\n",
      "[07:15:38] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:15:38] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[3]\ttrain-rmse:0.33207\ttrain-logloss:0.396753\n",
      "[07:15:39] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:15:39] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[4]\ttrain-rmse:0.29961\ttrain-logloss:0.34893\n",
      "[07:15:41] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:15:41] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[5]\ttrain-rmse:0.27048\ttrain-logloss:0.307751\n",
      "[07:15:42] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:15:42] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[6]\ttrain-rmse:0.244364\ttrain-logloss:0.27209\n",
      "[07:15:44] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:15:44] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[7]\ttrain-rmse:0.220922\ttrain-logloss:0.240986\n",
      "[07:15:45] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:15:45] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[8]\ttrain-rmse:0.199948\ttrain-logloss:0.213805\n",
      "[07:15:46] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:15:46] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[9]\ttrain-rmse:0.181193\ttrain-logloss:0.189951\n",
      "[07:15:48] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:15:48] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[10]\ttrain-rmse:0.164432\ttrain-logloss:0.168934\n",
      "[07:15:49] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:15:49] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[11]\ttrain-rmse:0.149487\ttrain-logloss:0.150379\n",
      "[07:15:50] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:15:50] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[12]\ttrain-rmse:0.136185\ttrain-logloss:0.133955\n",
      "[07:15:52] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:15:52] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[13]\ttrain-rmse:0.124368\ttrain-logloss:0.119382\n",
      "[07:15:53] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:15:53] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[14]\ttrain-rmse:0.113906\ttrain-logloss:0.106437\n",
      "[07:15:55] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:15:55] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[15]\ttrain-rmse:0.10465\ttrain-logloss:0.094891\n",
      "[07:15:56] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:15:56] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[16]\ttrain-rmse:0.096507\ttrain-logloss:0.084593\n",
      "[07:15:57] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:15:57] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[17]\ttrain-rmse:0.08936\ttrain-logloss:0.07502\n",
      "[07:15:59] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:15:59] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[18]\ttrain-rmse:0.083131\ttrain-logloss:0.066197\n",
      "[07:16:00] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:00] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[19]\ttrain-rmse:0.077699\ttrain-logloss:0.058727\n",
      "[07:16:01] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:01] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[20]\ttrain-rmse:0.07301\ttrain-logloss:0.051844\n",
      "[07:16:03] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:03] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[21]\ttrain-rmse:0.068986\ttrain-logloss:0.045765\n",
      "[07:16:04] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:04] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[22]\ttrain-rmse:0.065548\ttrain-logloss:0.040362\n",
      "[07:16:06] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:06] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[23]\ttrain-rmse:0.062626\ttrain-logloss:0.035215\n",
      "[07:16:07] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:07] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[24]\ttrain-rmse:0.060146\ttrain-logloss:0.030404\n",
      "[07:16:09] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:09] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[25]\ttrain-rmse:0.058056\ttrain-logloss:0.025974\n",
      "[07:16:10] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:10] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[26]\ttrain-rmse:0.056303\ttrain-logloss:0.021683\n",
      "[07:16:12] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:12] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[27]\ttrain-rmse:0.054847\ttrain-logloss:0.017416\n",
      "[07:16:13] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:13] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[28]\ttrain-rmse:0.05364\ttrain-logloss:0.013272\n",
      "[07:16:14] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:14] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[29]\ttrain-rmse:0.052633\ttrain-logloss:0.009363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:16:16] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:16] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[30]\ttrain-rmse:0.051803\ttrain-logloss:0.005646\n",
      "[07:16:17] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:17] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[31]\ttrain-rmse:0.051126\ttrain-logloss:0.001939\n",
      "[07:16:18] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:18] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[32]\ttrain-rmse:0.050563\ttrain-logloss:-0.001868\n",
      "[07:16:20] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:20] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[33]\ttrain-rmse:0.050098\ttrain-logloss:-0.00624\n",
      "[07:16:21] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:21] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[34]\ttrain-rmse:0.049717\ttrain-logloss:-0.010976\n",
      "[07:16:23] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:23] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[35]\ttrain-rmse:0.049412\ttrain-logloss:-0.015011\n",
      "[07:16:24] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:24] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[36]\ttrain-rmse:0.049161\ttrain-logloss:-0.019112\n",
      "[07:16:26] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:26] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[37]\ttrain-rmse:0.048956\ttrain-logloss:-0.023701\n",
      "[07:16:27] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:27] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[38]\ttrain-rmse:0.048787\ttrain-logloss:-0.027658\n",
      "[07:16:29] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:29] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[39]\ttrain-rmse:0.048643\ttrain-logloss:-0.031913\n",
      "[07:16:30] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:30] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[40]\ttrain-rmse:0.048526\ttrain-logloss:-0.036158\n",
      "[07:16:32] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:32] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[41]\ttrain-rmse:0.048432\ttrain-logloss:-0.0402\n",
      "[07:16:33] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:33] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[42]\ttrain-rmse:0.048355\ttrain-logloss:-0.044737\n",
      "[07:16:35] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:35] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[43]\ttrain-rmse:0.048294\ttrain-logloss:-0.048685\n",
      "[07:16:36] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:36] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[44]\ttrain-rmse:0.048239\ttrain-logloss:-0.053371\n",
      "[07:16:38] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:38] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[45]\ttrain-rmse:0.048195\ttrain-logloss:-0.05706\n",
      "[07:16:39] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:39] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[46]\ttrain-rmse:0.048162\ttrain-logloss:-0.060512\n",
      "[07:16:41] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:41] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[47]\ttrain-rmse:0.048127\ttrain-logloss:-0.063337\n",
      "[07:16:43] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:43] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[48]\ttrain-rmse:0.048098\ttrain-logloss:-0.06607\n",
      "[07:16:44] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:44] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[49]\ttrain-rmse:0.048073\ttrain-logloss:-0.069009\n",
      "[07:16:46] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:46] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[50]\ttrain-rmse:0.048045\ttrain-logloss:-0.07099\n",
      "[07:16:47] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:47] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[51]\ttrain-rmse:0.04803\ttrain-logloss:-0.072982\n",
      "[07:16:49] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:49] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[52]\ttrain-rmse:0.048015\ttrain-logloss:-0.074757\n",
      "[07:16:50] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:50] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[53]\ttrain-rmse:0.047997\ttrain-logloss:-0.077001\n",
      "[07:16:52] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:52] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[54]\ttrain-rmse:0.047983\ttrain-logloss:-0.078853\n",
      "[07:16:53] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:53] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[55]\ttrain-rmse:0.047971\ttrain-logloss:-0.080001\n",
      "[07:16:55] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:55] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[56]\ttrain-rmse:0.047939\ttrain-logloss:-0.081803\n",
      "[07:16:56] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:56] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[57]\ttrain-rmse:0.047929\ttrain-logloss:-0.08272\n",
      "[07:16:58] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:58] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[58]\ttrain-rmse:0.047923\ttrain-logloss:-0.083792\n",
      "[07:16:59] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:16:59] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[59]\ttrain-rmse:0.047917\ttrain-logloss:-0.084392\n",
      "[07:17:01] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:01] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60]\ttrain-rmse:0.047909\ttrain-logloss:-0.085323\n",
      "[07:17:03] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:03] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[61]\ttrain-rmse:0.047901\ttrain-logloss:-0.086211\n",
      "[07:17:04] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:04] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[62]\ttrain-rmse:0.047893\ttrain-logloss:-0.086677\n",
      "[07:17:06] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:06] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[63]\ttrain-rmse:0.047886\ttrain-logloss:-0.087409\n",
      "[07:17:08] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:08] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[64]\ttrain-rmse:0.047882\ttrain-logloss:-0.087548\n",
      "[07:17:09] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:09] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[65]\ttrain-rmse:0.047874\ttrain-logloss:-0.088136\n",
      "[07:17:11] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:11] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[66]\ttrain-rmse:0.047869\ttrain-logloss:-0.08834\n",
      "[07:17:12] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:12] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[67]\ttrain-rmse:0.047862\ttrain-logloss:-0.089519\n",
      "[07:17:14] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:14] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[68]\ttrain-rmse:0.047854\ttrain-logloss:-0.090112\n",
      "[07:17:16] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:16] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[69]\ttrain-rmse:0.047848\ttrain-logloss:-0.090241\n",
      "[07:17:17] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:17] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[70]\ttrain-rmse:0.04784\ttrain-logloss:-0.090268\n",
      "[07:17:19] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:19] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[71]\ttrain-rmse:0.047828\ttrain-logloss:-0.090788\n",
      "[07:17:20] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:20] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[72]\ttrain-rmse:0.047824\ttrain-logloss:-0.090772\n",
      "[07:17:22] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:22] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[73]\ttrain-rmse:0.047818\ttrain-logloss:-0.091532\n",
      "[07:17:24] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:24] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[74]\ttrain-rmse:0.047802\ttrain-logloss:-0.092796\n",
      "[07:17:25] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:25] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[75]\ttrain-rmse:0.047796\ttrain-logloss:-0.093699\n",
      "[07:17:27] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:27] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[76]\ttrain-rmse:0.04779\ttrain-logloss:-0.09412\n",
      "[07:17:28] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:28] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[77]\ttrain-rmse:0.047785\ttrain-logloss:-0.09404\n",
      "[07:17:30] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:30] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[78]\ttrain-rmse:0.047782\ttrain-logloss:-0.094056\n",
      "[07:17:32] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:32] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[79]\ttrain-rmse:0.047779\ttrain-logloss:-0.094845\n",
      "[07:17:33] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:33] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[80]\ttrain-rmse:0.047775\ttrain-logloss:-0.094865\n",
      "[07:17:35] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:35] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[81]\ttrain-rmse:0.047769\ttrain-logloss:-0.09519\n",
      "[07:17:37] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:37] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[82]\ttrain-rmse:0.047765\ttrain-logloss:-0.095193\n",
      "[07:17:38] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:38] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[83]\ttrain-rmse:0.047754\ttrain-logloss:-0.096517\n",
      "[07:17:40] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:40] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[84]\ttrain-rmse:0.047751\ttrain-logloss:-0.096863\n",
      "[07:17:42] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:42] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[85]\ttrain-rmse:0.047746\ttrain-logloss:-0.097135\n",
      "[07:17:43] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:43] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[86]\ttrain-rmse:0.04774\ttrain-logloss:-0.096976\n",
      "[07:17:45] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:45] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[87]\ttrain-rmse:0.047732\ttrain-logloss:-0.097625\n",
      "[07:17:47] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:47] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[88]\ttrain-rmse:0.047726\ttrain-logloss:-0.097837\n",
      "[07:17:48] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:48] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[89]\ttrain-rmse:0.04772\ttrain-logloss:-0.098354\n",
      "[07:17:50] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:50] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[90]\ttrain-rmse:0.047717\ttrain-logloss:-0.098304\n",
      "[07:17:52] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:52] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[91]\ttrain-rmse:0.047713\ttrain-logloss:-0.098532\n",
      "[07:17:53] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:53] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[92]\ttrain-rmse:0.047704\ttrain-logloss:-0.098728\n",
      "[07:17:55] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:55] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[93]\ttrain-rmse:0.047699\ttrain-logloss:-0.099207\n",
      "[07:17:57] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:57] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[94]\ttrain-rmse:0.047696\ttrain-logloss:-0.099339\n",
      "[07:17:58] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:17:58] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[95]\ttrain-rmse:0.047688\ttrain-logloss:-0.099783\n",
      "[07:18:00] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:00] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[96]\ttrain-rmse:0.047683\ttrain-logloss:-0.100043\n",
      "[07:18:02] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:02] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[97]\ttrain-rmse:0.04768\ttrain-logloss:-0.100203\n",
      "[07:18:03] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:03] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[98]\ttrain-rmse:0.047673\ttrain-logloss:-0.100233\n",
      "[07:18:05] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:05] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[99]\ttrain-rmse:0.047666\ttrain-logloss:-0.100246\n",
      "[07:18:06] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:06] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[100]\ttrain-rmse:0.04766\ttrain-logloss:-0.101499\n",
      "[07:18:08] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:08] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[101]\ttrain-rmse:0.047657\ttrain-logloss:-0.101417\n",
      "[07:18:10] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:10] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[102]\ttrain-rmse:0.04765\ttrain-logloss:-0.101834\n",
      "[07:18:12] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:12] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[103]\ttrain-rmse:0.047646\ttrain-logloss:-0.101998\n",
      "[07:18:13] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:13] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[104]\ttrain-rmse:0.04764\ttrain-logloss:-0.102494\n",
      "[07:18:15] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:15] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[105]\ttrain-rmse:0.047633\ttrain-logloss:-0.10319\n",
      "[07:18:17] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:17] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[106]\ttrain-rmse:0.047628\ttrain-logloss:-0.10302\n",
      "[07:18:19] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:19] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[107]\ttrain-rmse:0.047621\ttrain-logloss:-0.103383\n",
      "[07:18:20] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:20] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[108]\ttrain-rmse:0.047617\ttrain-logloss:-0.103615\n",
      "[07:18:22] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:22] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[109]\ttrain-rmse:0.047613\ttrain-logloss:-0.10378\n",
      "[07:18:24] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:24] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[110]\ttrain-rmse:0.047608\ttrain-logloss:-0.103998\n",
      "[07:18:26] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:26] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[111]\ttrain-rmse:0.047603\ttrain-logloss:-0.103724\n",
      "[07:18:27] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:27] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[112]\ttrain-rmse:0.0476\ttrain-logloss:-0.104219\n",
      "[07:18:29] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:29] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[113]\ttrain-rmse:0.047598\ttrain-logloss:-0.104229\n",
      "[07:18:31] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:31] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[114]\ttrain-rmse:0.047594\ttrain-logloss:-0.104069\n",
      "[07:18:33] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:33] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[115]\ttrain-rmse:0.047587\ttrain-logloss:-0.104108\n",
      "[07:18:34] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:34] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[116]\ttrain-rmse:0.047582\ttrain-logloss:-0.104243\n",
      "[07:18:36] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:36] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[117]\ttrain-rmse:0.047576\ttrain-logloss:-0.104767\n",
      "[07:18:38] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:38] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[118]\ttrain-rmse:0.047573\ttrain-logloss:-0.104915\n",
      "[07:18:40] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:40] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[119]\ttrain-rmse:0.047568\ttrain-logloss:-0.104986\n",
      "[07:18:42] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:42] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[120]\ttrain-rmse:0.047563\ttrain-logloss:-0.105074\n",
      "[07:18:43] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:43] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[121]\ttrain-rmse:0.047558\ttrain-logloss:-0.105286\n",
      "[07:18:45] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:45] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[122]\ttrain-rmse:0.047553\ttrain-logloss:-0.105768\n",
      "[07:18:47] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:47] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[123]\ttrain-rmse:0.047551\ttrain-logloss:-0.105823\n",
      "[07:18:49] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:49] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[124]\ttrain-rmse:0.047549\ttrain-logloss:-0.105683\n",
      "[07:18:51] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:51] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[125]\ttrain-rmse:0.047545\ttrain-logloss:-0.105846\n",
      "[07:18:53] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:53] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[126]\ttrain-rmse:0.047541\ttrain-logloss:-0.105929\n",
      "[07:18:54] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:54] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[127]\ttrain-rmse:0.047536\ttrain-logloss:-0.106021\n",
      "[07:18:56] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:56] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[128]\ttrain-rmse:0.047529\ttrain-logloss:-0.10632\n",
      "[07:18:58] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:18:58] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[129]\ttrain-rmse:0.047526\ttrain-logloss:-0.106686\n",
      "[07:19:00] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:00] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[130]\ttrain-rmse:0.047523\ttrain-logloss:-0.106663\n",
      "[07:19:02] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:02] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[131]\ttrain-rmse:0.04752\ttrain-logloss:-0.107057\n",
      "[07:19:04] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:04] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[132]\ttrain-rmse:0.047515\ttrain-logloss:-0.107191\n",
      "[07:19:06] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:06] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[133]\ttrain-rmse:0.047512\ttrain-logloss:-0.106982\n",
      "[07:19:07] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:07] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[134]\ttrain-rmse:0.04751\ttrain-logloss:-0.106981\n",
      "[07:19:09] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:09] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[135]\ttrain-rmse:0.047507\ttrain-logloss:-0.107032\n",
      "[07:19:11] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:11] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[136]\ttrain-rmse:0.047503\ttrain-logloss:-0.10734\n",
      "[07:19:13] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:13] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[137]\ttrain-rmse:0.0475\ttrain-logloss:-0.107178\n",
      "[07:19:15] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:15] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[138]\ttrain-rmse:0.047496\ttrain-logloss:-0.107541\n",
      "[07:19:17] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:17] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[139]\ttrain-rmse:0.047493\ttrain-logloss:-0.107479\n",
      "[07:19:19] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:19] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[140]\ttrain-rmse:0.047487\ttrain-logloss:-0.107801\n",
      "[07:19:20] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:20] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[141]\ttrain-rmse:0.047484\ttrain-logloss:-0.107755\n",
      "[07:19:22] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:22] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[142]\ttrain-rmse:0.047479\ttrain-logloss:-0.107865\n",
      "[07:19:24] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:24] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[143]\ttrain-rmse:0.047477\ttrain-logloss:-0.107916\n",
      "[07:19:26] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:26] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[144]\ttrain-rmse:0.047473\ttrain-logloss:-0.108035\n",
      "[07:19:28] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:28] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[145]\ttrain-rmse:0.047472\ttrain-logloss:-0.107916\n",
      "[07:19:30] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:30] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[146]\ttrain-rmse:0.047468\ttrain-logloss:-0.108076\n",
      "[07:19:32] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:32] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[147]\ttrain-rmse:0.047467\ttrain-logloss:-0.107978\n",
      "[07:19:34] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:34] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[148]\ttrain-rmse:0.047462\ttrain-logloss:-0.108486\n",
      "[07:19:35] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:35] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[149]\ttrain-rmse:0.047461\ttrain-logloss:-0.108515\n",
      "[07:19:37] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:37] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[150]\ttrain-rmse:0.047457\ttrain-logloss:-0.109021\n",
      "[07:19:39] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:39] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[151]\ttrain-rmse:0.047455\ttrain-logloss:-0.109059\n",
      "[07:19:41] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:41] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[152]\ttrain-rmse:0.047452\ttrain-logloss:-0.10899\n",
      "[07:19:43] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:43] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[153]\ttrain-rmse:0.047448\ttrain-logloss:-0.109815\n",
      "[07:19:45] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:45] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[154]\ttrain-rmse:0.047444\ttrain-logloss:-0.109844\n",
      "[07:19:47] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:47] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[155]\ttrain-rmse:0.047439\ttrain-logloss:-0.11061\n",
      "[07:19:49] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:49] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[156]\ttrain-rmse:0.047436\ttrain-logloss:-0.110541\n",
      "[07:19:51] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:51] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[157]\ttrain-rmse:0.047427\ttrain-logloss:-0.110508\n",
      "[07:19:53] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:53] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[158]\ttrain-rmse:0.047422\ttrain-logloss:-0.110435\n",
      "[07:19:55] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:55] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[159]\ttrain-rmse:0.047419\ttrain-logloss:-0.110631\n",
      "[07:19:57] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:57] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[160]\ttrain-rmse:0.047416\ttrain-logloss:-0.111075\n",
      "[07:19:59] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:19:59] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[161]\ttrain-rmse:0.047411\ttrain-logloss:-0.11104\n",
      "[07:20:01] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:01] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[162]\ttrain-rmse:0.047408\ttrain-logloss:-0.111498\n",
      "[07:20:02] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:02] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[163]\ttrain-rmse:0.047404\ttrain-logloss:-0.111512\n",
      "[07:20:04] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:04] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[164]\ttrain-rmse:0.047399\ttrain-logloss:-0.111502\n",
      "[07:20:06] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:06] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[165]\ttrain-rmse:0.047396\ttrain-logloss:-0.111664\n",
      "[07:20:08] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:08] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[166]\ttrain-rmse:0.047391\ttrain-logloss:-0.111932\n",
      "[07:20:10] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:10] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[167]\ttrain-rmse:0.047388\ttrain-logloss:-0.111966\n",
      "[07:20:12] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:12] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[168]\ttrain-rmse:0.047382\ttrain-logloss:-0.112129\n",
      "[07:20:14] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:14] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[169]\ttrain-rmse:0.047379\ttrain-logloss:-0.111914\n",
      "[07:20:16] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:16] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[170]\ttrain-rmse:0.047377\ttrain-logloss:-0.111962\n",
      "[07:20:18] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:18] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[171]\ttrain-rmse:0.047373\ttrain-logloss:-0.111946\n",
      "[07:20:20] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:20] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[172]\ttrain-rmse:0.047369\ttrain-logloss:-0.112138\n",
      "[07:20:22] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:22] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[173]\ttrain-rmse:0.047364\ttrain-logloss:-0.112194\n",
      "[07:20:24] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:24] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[174]\ttrain-rmse:0.047361\ttrain-logloss:-0.112369\n",
      "[07:20:26] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:26] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[175]\ttrain-rmse:0.047354\ttrain-logloss:-0.112718\n",
      "[07:20:28] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:28] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[176]\ttrain-rmse:0.047347\ttrain-logloss:-0.113057\n",
      "[07:20:29] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:29] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[177]\ttrain-rmse:0.047343\ttrain-logloss:-0.112992\n",
      "[07:20:31] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:31] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[178]\ttrain-rmse:0.04734\ttrain-logloss:-0.113298\n",
      "[07:20:33] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:33] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[179]\ttrain-rmse:0.047336\ttrain-logloss:-0.113065\n",
      "[07:20:35] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:35] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[180]\ttrain-rmse:0.047331\ttrain-logloss:-0.113383\n",
      "[07:20:37] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:37] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[181]\ttrain-rmse:0.047326\ttrain-logloss:-0.113464\n",
      "[07:20:39] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:39] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[182]\ttrain-rmse:0.047323\ttrain-logloss:-0.113435\n",
      "[07:20:41] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:41] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[183]\ttrain-rmse:0.047319\ttrain-logloss:-0.113638\n",
      "[07:20:43] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:43] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[184]\ttrain-rmse:0.047316\ttrain-logloss:-0.113611\n",
      "[07:20:45] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:45] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[185]\ttrain-rmse:0.047309\ttrain-logloss:-0.113945\n",
      "[07:20:47] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:47] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[186]\ttrain-rmse:0.047305\ttrain-logloss:-0.114038\n",
      "[07:20:49] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:49] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[187]\ttrain-rmse:0.0473\ttrain-logloss:-0.114533\n",
      "[07:20:51] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:51] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[188]\ttrain-rmse:0.047298\ttrain-logloss:-0.114766\n",
      "[07:20:53] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:53] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[189]\ttrain-rmse:0.047294\ttrain-logloss:-0.114987\n",
      "[07:20:55] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:55] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[190]\ttrain-rmse:0.047291\ttrain-logloss:-0.114963\n",
      "[07:20:57] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:57] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[191]\ttrain-rmse:0.047288\ttrain-logloss:-0.115454\n",
      "[07:20:59] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:20:59] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[192]\ttrain-rmse:0.047285\ttrain-logloss:-0.1155\n",
      "[07:21:01] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:01] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[193]\ttrain-rmse:0.04728\ttrain-logloss:-0.115834\n",
      "[07:21:03] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:03] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[194]\ttrain-rmse:0.047275\ttrain-logloss:-0.116103\n",
      "[07:21:05] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:05] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[195]\ttrain-rmse:0.047272\ttrain-logloss:-0.115926\n",
      "[07:21:08] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:08] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[196]\ttrain-rmse:0.047271\ttrain-logloss:-0.115906\n",
      "[07:21:09] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:09] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[197]\ttrain-rmse:0.047269\ttrain-logloss:-0.115988\n",
      "[07:21:12] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:12] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[198]\ttrain-rmse:0.047265\ttrain-logloss:-0.11628\n",
      "[07:21:13] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:13] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[199]\ttrain-rmse:0.047261\ttrain-logloss:-0.11622\n",
      "[07:21:16] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:16] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[200]\ttrain-rmse:0.047258\ttrain-logloss:-0.116142\n",
      "[07:21:18] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:18] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[201]\ttrain-rmse:0.047255\ttrain-logloss:-0.116207\n",
      "[07:21:20] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:20] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[202]\ttrain-rmse:0.047253\ttrain-logloss:-0.116032\n",
      "[07:21:22] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:22] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[203]\ttrain-rmse:0.047248\ttrain-logloss:-0.116454\n",
      "[07:21:24] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:24] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[204]\ttrain-rmse:0.047243\ttrain-logloss:-0.116958\n",
      "[07:21:26] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:26] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[205]\ttrain-rmse:0.047238\ttrain-logloss:-0.117007\n",
      "[07:21:28] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:28] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[206]\ttrain-rmse:0.047234\ttrain-logloss:-0.11707\n",
      "[07:21:30] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:30] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[207]\ttrain-rmse:0.047231\ttrain-logloss:-0.117664\n",
      "[07:21:32] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:32] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[208]\ttrain-rmse:0.047227\ttrain-logloss:-0.117829\n",
      "[07:21:34] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:34] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[209]\ttrain-rmse:0.047224\ttrain-logloss:-0.117694\n",
      "[07:21:36] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:36] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[210]\ttrain-rmse:0.047214\ttrain-logloss:-0.11764\n",
      "[07:21:38] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:38] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[211]\ttrain-rmse:0.047209\ttrain-logloss:-0.117487\n",
      "[07:21:40] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:40] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[212]\ttrain-rmse:0.047203\ttrain-logloss:-0.117339\n",
      "[07:21:42] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:42] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[213]\ttrain-rmse:0.047199\ttrain-logloss:-0.11755\n",
      "[07:21:44] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:44] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[214]\ttrain-rmse:0.047197\ttrain-logloss:-0.117674\n",
      "[07:21:46] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:46] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[215]\ttrain-rmse:0.047192\ttrain-logloss:-0.117853\n",
      "[07:21:48] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:48] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[216]\ttrain-rmse:0.04719\ttrain-logloss:-0.117827\n",
      "[07:21:51] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:51] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[217]\ttrain-rmse:0.047185\ttrain-logloss:-0.117968\n",
      "[07:21:53] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:53] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[218]\ttrain-rmse:0.047182\ttrain-logloss:-0.118085\n",
      "[07:21:55] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:55] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[219]\ttrain-rmse:0.047177\ttrain-logloss:-0.118177\n",
      "[07:21:57] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:57] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[220]\ttrain-rmse:0.047173\ttrain-logloss:-0.118167\n",
      "[07:21:59] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:21:59] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[221]\ttrain-rmse:0.047167\ttrain-logloss:-0.11852\n",
      "[07:22:01] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:01] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[222]\ttrain-rmse:0.047162\ttrain-logloss:-0.118649\n",
      "[07:22:03] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:03] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[223]\ttrain-rmse:0.047158\ttrain-logloss:-0.118681\n",
      "[07:22:06] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:06] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[224]\ttrain-rmse:0.047155\ttrain-logloss:-0.118433\n",
      "[07:22:08] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:08] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[225]\ttrain-rmse:0.047153\ttrain-logloss:-0.118522\n",
      "[07:22:10] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:10] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[226]\ttrain-rmse:0.047151\ttrain-logloss:-0.118564\n",
      "[07:22:12] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:12] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[227]\ttrain-rmse:0.047149\ttrain-logloss:-0.118861\n",
      "[07:22:14] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:14] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[228]\ttrain-rmse:0.047145\ttrain-logloss:-0.119156\n",
      "[07:22:16] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:16] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[229]\ttrain-rmse:0.047142\ttrain-logloss:-0.119588\n",
      "[07:22:19] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:19] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[230]\ttrain-rmse:0.047139\ttrain-logloss:-0.119468\n",
      "[07:22:21] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:21] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[231]\ttrain-rmse:0.047137\ttrain-logloss:-0.119973\n",
      "[07:22:23] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:23] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[232]\ttrain-rmse:0.047132\ttrain-logloss:-0.120275\n",
      "[07:22:25] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:25] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[233]\ttrain-rmse:0.047131\ttrain-logloss:-0.120356\n",
      "[07:22:27] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:27] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[234]\ttrain-rmse:0.047129\ttrain-logloss:-0.120422\n",
      "[07:22:30] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:30] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[235]\ttrain-rmse:0.047127\ttrain-logloss:-0.120474\n",
      "[07:22:32] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:32] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[236]\ttrain-rmse:0.047118\ttrain-logloss:-0.120511\n",
      "[07:22:34] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:34] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[237]\ttrain-rmse:0.047115\ttrain-logloss:-0.120088\n",
      "[07:22:37] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:37] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[238]\ttrain-rmse:0.04711\ttrain-logloss:-0.120732\n",
      "[07:22:39] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:39] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[239]\ttrain-rmse:0.047106\ttrain-logloss:-0.12057\n",
      "[07:22:41] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:41] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[240]\ttrain-rmse:0.047104\ttrain-logloss:-0.120646\n",
      "[07:22:43] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:43] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[241]\ttrain-rmse:0.047101\ttrain-logloss:-0.120941\n",
      "[07:22:46] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:46] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[242]\ttrain-rmse:0.047094\ttrain-logloss:-0.120822\n",
      "[07:22:48] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:48] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[243]\ttrain-rmse:0.047091\ttrain-logloss:-0.120899\n",
      "[07:22:51] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:51] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[244]\ttrain-rmse:0.04709\ttrain-logloss:-0.120936\n",
      "[07:22:53] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:53] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[245]\ttrain-rmse:0.047085\ttrain-logloss:-0.120954\n",
      "[07:22:55] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:55] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[246]\ttrain-rmse:0.047084\ttrain-logloss:-0.121113\n",
      "[07:22:58] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:22:58] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[247]\ttrain-rmse:0.047081\ttrain-logloss:-0.121245\n",
      "[07:23:00] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:00] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[248]\ttrain-rmse:0.047078\ttrain-logloss:-0.121369\n",
      "[07:23:02] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:02] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[249]\ttrain-rmse:0.047074\ttrain-logloss:-0.121384\n",
      "[07:23:05] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:05] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[250]\ttrain-rmse:0.047071\ttrain-logloss:-0.121654\n",
      "[07:23:07] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:07] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[251]\ttrain-rmse:0.047068\ttrain-logloss:-0.121602\n",
      "[07:23:09] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:09] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[252]\ttrain-rmse:0.047065\ttrain-logloss:-0.121718\n",
      "[07:23:11] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:11] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[253]\ttrain-rmse:0.047062\ttrain-logloss:-0.121953\n",
      "[07:23:14] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:14] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[254]\ttrain-rmse:0.047056\ttrain-logloss:-0.121985\n",
      "[07:23:16] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:16] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[255]\ttrain-rmse:0.047045\ttrain-logloss:-0.122087\n",
      "[07:23:18] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:18] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[256]\ttrain-rmse:0.047043\ttrain-logloss:-0.122087\n",
      "[07:23:20] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:20] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[257]\ttrain-rmse:0.047037\ttrain-logloss:-0.122621\n",
      "[07:23:23] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:23] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[258]\ttrain-rmse:0.047034\ttrain-logloss:-0.12264\n",
      "[07:23:25] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:25] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[259]\ttrain-rmse:0.047031\ttrain-logloss:-0.122705\n",
      "[07:23:27] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:27] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[260]\ttrain-rmse:0.047028\ttrain-logloss:-0.122962\n",
      "[07:23:29] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:29] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[261]\ttrain-rmse:0.047026\ttrain-logloss:-0.122916\n",
      "[07:23:31] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:31] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[262]\ttrain-rmse:0.047023\ttrain-logloss:-0.123122\n",
      "[07:23:34] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:34] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[263]\ttrain-rmse:0.047018\ttrain-logloss:-0.123306\n",
      "[07:23:36] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:36] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[264]\ttrain-rmse:0.047016\ttrain-logloss:-0.123657\n",
      "[07:23:38] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:38] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[265]\ttrain-rmse:0.047014\ttrain-logloss:-0.123623\n",
      "[07:23:41] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:41] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[266]\ttrain-rmse:0.047011\ttrain-logloss:-0.124115\n",
      "[07:23:43] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:43] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[267]\ttrain-rmse:0.047007\ttrain-logloss:-0.124301\n",
      "[07:23:45] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:45] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[268]\ttrain-rmse:0.047005\ttrain-logloss:-0.124253\n",
      "[07:23:48] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:48] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[269]\ttrain-rmse:0.047004\ttrain-logloss:-0.124215\n",
      "[07:23:50] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:50] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[270]\ttrain-rmse:0.047\ttrain-logloss:-0.124667\n",
      "[07:23:52] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:52] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[271]\ttrain-rmse:0.046997\ttrain-logloss:-0.124586\n",
      "[07:23:54] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:54] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[272]\ttrain-rmse:0.046995\ttrain-logloss:-0.124662\n",
      "[07:23:57] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:57] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[273]\ttrain-rmse:0.046993\ttrain-logloss:-0.124668\n",
      "[07:23:59] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:23:59] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[274]\ttrain-rmse:0.046989\ttrain-logloss:-0.124828\n",
      "[07:24:01] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:01] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[275]\ttrain-rmse:0.046987\ttrain-logloss:-0.124749\n",
      "[07:24:03] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:03] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[276]\ttrain-rmse:0.046985\ttrain-logloss:-0.124993\n",
      "[07:24:06] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:06] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[277]\ttrain-rmse:0.046983\ttrain-logloss:-0.124969\n",
      "[07:24:08] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:08] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[278]\ttrain-rmse:0.04698\ttrain-logloss:-0.125204\n",
      "[07:24:11] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:11] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[279]\ttrain-rmse:0.046977\ttrain-logloss:-0.125266\n",
      "[07:24:13] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:13] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[280]\ttrain-rmse:0.046976\ttrain-logloss:-0.125427\n",
      "[07:24:15] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:15] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[281]\ttrain-rmse:0.046972\ttrain-logloss:-0.12537\n",
      "[07:24:18] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:18] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[282]\ttrain-rmse:0.046971\ttrain-logloss:-0.12544\n",
      "[07:24:20] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:20] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[283]\ttrain-rmse:0.046969\ttrain-logloss:-0.125462\n",
      "[07:24:22] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:22] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[284]\ttrain-rmse:0.046967\ttrain-logloss:-0.125526\n",
      "[07:24:25] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:25] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[285]\ttrain-rmse:0.046966\ttrain-logloss:-0.125494\n",
      "[07:24:27] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:27] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[286]\ttrain-rmse:0.046964\ttrain-logloss:-0.125715\n",
      "[07:24:29] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:29] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[287]\ttrain-rmse:0.046959\ttrain-logloss:-0.126092\n",
      "[07:24:32] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:32] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[288]\ttrain-rmse:0.046955\ttrain-logloss:-0.125874\n",
      "[07:24:34] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:34] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[289]\ttrain-rmse:0.046953\ttrain-logloss:-0.126175\n",
      "[07:24:36] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:36] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[290]\ttrain-rmse:0.046948\ttrain-logloss:-0.126425\n",
      "[07:24:39] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:39] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[291]\ttrain-rmse:0.046945\ttrain-logloss:-0.126655\n",
      "[07:24:41] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:41] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[292]\ttrain-rmse:0.046942\ttrain-logloss:-0.126911\n",
      "[07:24:43] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:43] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[293]\ttrain-rmse:0.046939\ttrain-logloss:-0.126757\n",
      "[07:24:46] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:46] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[294]\ttrain-rmse:0.046937\ttrain-logloss:-0.127051\n",
      "[07:24:48] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:48] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[295]\ttrain-rmse:0.046932\ttrain-logloss:-0.126939\n",
      "[07:24:50] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:50] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[296]\ttrain-rmse:0.046928\ttrain-logloss:-0.127064\n",
      "[07:24:53] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:53] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[297]\ttrain-rmse:0.046927\ttrain-logloss:-0.12715\n",
      "[07:24:55] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:55] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[298]\ttrain-rmse:0.046924\ttrain-logloss:-0.127279\n",
      "[07:24:58] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:24:58] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[299]\ttrain-rmse:0.046921\ttrain-logloss:-0.12742\n",
      "[07:25:00] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:00] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[300]\ttrain-rmse:0.046917\ttrain-logloss:-0.127347\n",
      "[07:25:03] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:03] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[301]\ttrain-rmse:0.046914\ttrain-logloss:-0.12746\n",
      "[07:25:05] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:05] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[302]\ttrain-rmse:0.046913\ttrain-logloss:-0.127418\n",
      "[07:25:07] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:07] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[303]\ttrain-rmse:0.046911\ttrain-logloss:-0.127447\n",
      "[07:25:10] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:10] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[304]\ttrain-rmse:0.04691\ttrain-logloss:-0.127453\n",
      "[07:25:12] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:12] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[305]\ttrain-rmse:0.046907\ttrain-logloss:-0.127566\n",
      "[07:25:15] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:15] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[306]\ttrain-rmse:0.046905\ttrain-logloss:-0.127318\n",
      "[07:25:17] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:17] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[307]\ttrain-rmse:0.046902\ttrain-logloss:-0.127732\n",
      "[07:25:19] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:19] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[308]\ttrain-rmse:0.0469\ttrain-logloss:-0.127743\n",
      "[07:25:22] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:22] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[309]\ttrain-rmse:0.046897\ttrain-logloss:-0.12781\n",
      "[07:25:24] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:24] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[310]\ttrain-rmse:0.046894\ttrain-logloss:-0.127634\n",
      "[07:25:27] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:27] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[311]\ttrain-rmse:0.046893\ttrain-logloss:-0.127689\n",
      "[07:25:29] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:29] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[312]\ttrain-rmse:0.046889\ttrain-logloss:-0.127561\n",
      "[07:25:31] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:31] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[313]\ttrain-rmse:0.046887\ttrain-logloss:-0.127413\n",
      "[07:25:34] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:34] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[314]\ttrain-rmse:0.046882\ttrain-logloss:-0.127448\n",
      "[07:25:36] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:36] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[315]\ttrain-rmse:0.046879\ttrain-logloss:-0.127595\n",
      "[07:25:39] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:39] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[316]\ttrain-rmse:0.046876\ttrain-logloss:-0.127654\n",
      "[07:25:41] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:41] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[317]\ttrain-rmse:0.046874\ttrain-logloss:-0.127777\n",
      "[07:25:44] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:44] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[318]\ttrain-rmse:0.04687\ttrain-logloss:-0.128075\n",
      "[07:25:46] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:46] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[319]\ttrain-rmse:0.046865\ttrain-logloss:-0.128236\n",
      "[07:25:48] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:48] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[320]\ttrain-rmse:0.046862\ttrain-logloss:-0.128054\n",
      "[07:25:51] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:51] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[321]\ttrain-rmse:0.04686\ttrain-logloss:-0.128127\n",
      "[07:25:53] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:53] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[322]\ttrain-rmse:0.046857\ttrain-logloss:-0.1282\n",
      "[07:25:56] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:56] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[323]\ttrain-rmse:0.046854\ttrain-logloss:-0.128378\n",
      "[07:25:58] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:25:58] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[324]\ttrain-rmse:0.046851\ttrain-logloss:-0.128468\n",
      "[07:26:01] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:26:01] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[325]\ttrain-rmse:0.046848\ttrain-logloss:-0.128585\n",
      "[07:26:03] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:26:03] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[326]\ttrain-rmse:0.046845\ttrain-logloss:-0.128619\n",
      "[07:26:06] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:26:06] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[327]\ttrain-rmse:0.046843\ttrain-logloss:-0.128649\n",
      "[07:26:08] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:26:08] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[328]\ttrain-rmse:0.046837\ttrain-logloss:-0.12873\n",
      "[07:26:11] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:26:11] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[329]\ttrain-rmse:0.046833\ttrain-logloss:-0.128828\n",
      "[07:26:13] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:26:13] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[330]\ttrain-rmse:0.046832\ttrain-logloss:-0.128947\n",
      "[07:26:16] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:26:16] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[331]\ttrain-rmse:0.04683\ttrain-logloss:-0.129087\n",
      "[07:26:19] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:26:19] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[332]\ttrain-rmse:0.046827\ttrain-logloss:-0.129181\n",
      "[07:26:21] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:26:21] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[333]\ttrain-rmse:0.046826\ttrain-logloss:-0.129169\n",
      "[07:26:24] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:26:24] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[334]\ttrain-rmse:0.046821\ttrain-logloss:-0.129188\n",
      "[07:26:26] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:26:26] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[335]\ttrain-rmse:0.046817\ttrain-logloss:-0.129551\n",
      "[07:26:29] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:26:29] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[336]\ttrain-rmse:0.046815\ttrain-logloss:-0.129874\n",
      "[07:26:31] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:26:31] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[337]\ttrain-rmse:0.046813\ttrain-logloss:-0.129726\n",
      "[07:26:34] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:26:34] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[338]\ttrain-rmse:0.046811\ttrain-logloss:-0.13016\n",
      "[07:26:36] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:26:36] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[339]\ttrain-rmse:0.046808\ttrain-logloss:-0.130319\n",
      "[07:26:39] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:26:39] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[340]\ttrain-rmse:0.046806\ttrain-logloss:-0.130308\n",
      "[07:26:41] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:26:41] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[341]\ttrain-rmse:0.046804\ttrain-logloss:-0.130307\n",
      "[07:26:44] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:26:44] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[342]\ttrain-rmse:0.046803\ttrain-logloss:-0.130389\n",
      "[07:26:47] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:26:47] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[343]\ttrain-rmse:0.046799\ttrain-logloss:-0.130696\n",
      "[07:26:49] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:26:49] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[344]\ttrain-rmse:0.046796\ttrain-logloss:-0.130859\n",
      "[07:26:52] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:26:52] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[345]\ttrain-rmse:0.046793\ttrain-logloss:-0.1309\n",
      "[07:26:54] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:26:54] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[346]\ttrain-rmse:0.046791\ttrain-logloss:-0.130952\n",
      "[07:26:57] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:26:57] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[347]\ttrain-rmse:0.046789\ttrain-logloss:-0.130993\n",
      "[07:26:59] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:26:59] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[348]\ttrain-rmse:0.046786\ttrain-logloss:-0.130907\n",
      "[07:27:02] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:27:02] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[349]\ttrain-rmse:0.046783\ttrain-logloss:-0.130906\n",
      "[07:27:05] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:27:05] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[350]\ttrain-rmse:0.046782\ttrain-logloss:-0.131364\n",
      "[07:27:07] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:27:07] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[351]\ttrain-rmse:0.046779\ttrain-logloss:-0.131182\n",
      "[07:27:10] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:27:10] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[352]\ttrain-rmse:0.046776\ttrain-logloss:-0.131232\n",
      "[07:27:13] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:27:13] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[353]\ttrain-rmse:0.046775\ttrain-logloss:-0.131367\n",
      "[07:27:15] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:27:15] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[354]\ttrain-rmse:0.046773\ttrain-logloss:-0.131192\n",
      "[07:27:18] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:27:18] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[355]\ttrain-rmse:0.046769\ttrain-logloss:-0.131312\n",
      "[07:27:21] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:27:21] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[356]\ttrain-rmse:0.046766\ttrain-logloss:-0.131377\n",
      "[07:27:23] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:27:23] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[357]\ttrain-rmse:0.046764\ttrain-logloss:-0.131347\n",
      "[07:27:26] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:27:26] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[358]\ttrain-rmse:0.04676\ttrain-logloss:-0.131691\n",
      "[07:27:28] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:27:28] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[359]\ttrain-rmse:0.046756\ttrain-logloss:-0.131645\n",
      "[07:27:31] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:27:31] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[360]\ttrain-rmse:0.046755\ttrain-logloss:-0.132138\n",
      "[07:27:34] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:27:34] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[361]\ttrain-rmse:0.046753\ttrain-logloss:-0.132242\n",
      "[07:27:36] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:27:36] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[362]\ttrain-rmse:0.04675\ttrain-logloss:-0.132244\n",
      "[07:27:39] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:27:39] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[363]\ttrain-rmse:0.046748\ttrain-logloss:-0.132338\n",
      "[07:27:42] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:27:42] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[364]\ttrain-rmse:0.046746\ttrain-logloss:-0.132453\n",
      "[07:27:44] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:27:44] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[365]\ttrain-rmse:0.046745\ttrain-logloss:-0.132617\n",
      "[07:27:47] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:27:47] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[366]\ttrain-rmse:0.046742\ttrain-logloss:-0.132643\n",
      "[07:27:50] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:27:50] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[367]\ttrain-rmse:0.046738\ttrain-logloss:-0.132892\n",
      "[07:27:52] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:27:52] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[368]\ttrain-rmse:0.046735\ttrain-logloss:-0.132918\n",
      "[07:27:55] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:27:55] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[369]\ttrain-rmse:0.046733\ttrain-logloss:-0.132693\n",
      "[07:27:58] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:27:58] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[370]\ttrain-rmse:0.04673\ttrain-logloss:-0.132841\n",
      "[07:28:01] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:28:01] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[371]\ttrain-rmse:0.046728\ttrain-logloss:-0.132788\n",
      "[07:28:03] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:28:03] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[372]\ttrain-rmse:0.046727\ttrain-logloss:-0.133028\n",
      "[07:28:06] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:28:06] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[373]\ttrain-rmse:0.046727\ttrain-logloss:-0.133087\n",
      "[07:28:09] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:28:09] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[374]\ttrain-rmse:0.046727\ttrain-logloss:-0.133073\n",
      "[07:28:12] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:28:12] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[375]\ttrain-rmse:0.046724\ttrain-logloss:-0.13322\n",
      "[07:28:14] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:28:14] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[376]\ttrain-rmse:0.046722\ttrain-logloss:-0.133119\n",
      "[07:28:17] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:28:17] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[377]\ttrain-rmse:0.046718\ttrain-logloss:-0.133567\n",
      "[07:28:20] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:28:20] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[378]\ttrain-rmse:0.046716\ttrain-logloss:-0.133678\n",
      "[07:28:23] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:28:23] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[379]\ttrain-rmse:0.046712\ttrain-logloss:-0.133597\n",
      "[07:28:26] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:28:26] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[380]\ttrain-rmse:0.046711\ttrain-logloss:-0.133646\n",
      "[07:28:28] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:28:28] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[381]\ttrain-rmse:0.046708\ttrain-logloss:-0.133551\n",
      "[07:28:31] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:28:31] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[382]\ttrain-rmse:0.046706\ttrain-logloss:-0.133283\n",
      "[07:28:34] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:28:34] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[383]\ttrain-rmse:0.046704\ttrain-logloss:-0.133327\n",
      "[07:28:37] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:28:37] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[384]\ttrain-rmse:0.046701\ttrain-logloss:-0.133625\n",
      "[07:28:40] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:28:40] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[385]\ttrain-rmse:0.0467\ttrain-logloss:-0.133437\n",
      "[07:28:43] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:28:43] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[386]\ttrain-rmse:0.046698\ttrain-logloss:-0.133735\n",
      "[07:28:46] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:28:46] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[387]\ttrain-rmse:0.046695\ttrain-logloss:-0.133775\n",
      "[07:28:48] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:28:48] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[388]\ttrain-rmse:0.046691\ttrain-logloss:-0.134033\n",
      "[07:28:51] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:28:51] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[389]\ttrain-rmse:0.046689\ttrain-logloss:-0.134174\n",
      "[07:28:54] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:28:54] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[390]\ttrain-rmse:0.046687\ttrain-logloss:-0.134198\n",
      "[07:28:57] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:28:57] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[391]\ttrain-rmse:0.046685\ttrain-logloss:-0.134203\n",
      "[07:29:00] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:29:00] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[392]\ttrain-rmse:0.046682\ttrain-logloss:-0.134216\n",
      "[07:29:03] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:29:03] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[393]\ttrain-rmse:0.04668\ttrain-logloss:-0.134235\n",
      "[07:29:06] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:29:06] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[394]\ttrain-rmse:0.046679\ttrain-logloss:-0.134235\n",
      "[07:29:09] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:29:09] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[395]\ttrain-rmse:0.046676\ttrain-logloss:-0.1343\n",
      "[07:29:12] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:29:12] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[396]\ttrain-rmse:0.046674\ttrain-logloss:-0.134392\n",
      "[07:29:15] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:29:15] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[397]\ttrain-rmse:0.046671\ttrain-logloss:-0.134448\n",
      "[07:29:18] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:29:18] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[398]\ttrain-rmse:0.04667\ttrain-logloss:-0.134494\n",
      "[07:29:20] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:29:20] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[399]\ttrain-rmse:0.046667\ttrain-logloss:-0.134715\n",
      "[07:29:23] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:29:23] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[400]\ttrain-rmse:0.046664\ttrain-logloss:-0.134693\n",
      "[07:29:26] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:29:26] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[401]\ttrain-rmse:0.046662\ttrain-logloss:-0.134788\n",
      "[07:29:29] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:29:29] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[402]\ttrain-rmse:0.046661\ttrain-logloss:-0.134786\n",
      "[07:29:32] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:29:32] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[403]\ttrain-rmse:0.046658\ttrain-logloss:-0.134718\n",
      "[07:29:34] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:29:34] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[404]\ttrain-rmse:0.046656\ttrain-logloss:-0.134706\n",
      "[07:29:37] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:29:37] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[405]\ttrain-rmse:0.046654\ttrain-logloss:-0.134702\n",
      "[07:29:40] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:29:40] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[406]\ttrain-rmse:0.046654\ttrain-logloss:-0.134839\n",
      "[07:29:43] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:29:43] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[407]\ttrain-rmse:0.046651\ttrain-logloss:-0.134676\n",
      "[07:29:46] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:29:46] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[408]\ttrain-rmse:0.046649\ttrain-logloss:-0.134846\n",
      "[07:29:49] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:29:49] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[409]\ttrain-rmse:0.046646\ttrain-logloss:-0.134889\n",
      "[07:29:52] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:29:52] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[410]\ttrain-rmse:0.046642\ttrain-logloss:-0.134926\n",
      "[07:29:55] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:29:55] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[411]\ttrain-rmse:0.046639\ttrain-logloss:-0.134874\n",
      "[07:29:58] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:29:58] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[412]\ttrain-rmse:0.046635\ttrain-logloss:-0.135364\n",
      "[07:30:01] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:30:01] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[413]\ttrain-rmse:0.046633\ttrain-logloss:-0.135713\n",
      "[07:30:04] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:30:04] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[414]\ttrain-rmse:0.04663\ttrain-logloss:-0.135772\n",
      "[07:30:07] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:30:07] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[415]\ttrain-rmse:0.046628\ttrain-logloss:-0.135896\n",
      "[07:30:10] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:30:10] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[416]\ttrain-rmse:0.046622\ttrain-logloss:-0.135905\n",
      "[07:30:13] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:30:13] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[417]\ttrain-rmse:0.046621\ttrain-logloss:-0.135758\n",
      "[07:30:16] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:30:16] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[418]\ttrain-rmse:0.046617\ttrain-logloss:-0.136059\n",
      "[07:30:19] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:30:19] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[419]\ttrain-rmse:0.046615\ttrain-logloss:-0.136316\n",
      "[07:30:22] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:30:22] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[420]\ttrain-rmse:0.046613\ttrain-logloss:-0.136217\n",
      "[07:30:25] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:30:25] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[421]\ttrain-rmse:0.046611\ttrain-logloss:-0.13644\n",
      "[07:30:28] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:30:28] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[422]\ttrain-rmse:0.046608\ttrain-logloss:-0.136581\n",
      "[07:30:31] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:30:31] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[423]\ttrain-rmse:0.046605\ttrain-logloss:-0.136947\n",
      "[07:30:34] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:30:34] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[424]\ttrain-rmse:0.046604\ttrain-logloss:-0.137036\n",
      "[07:30:37] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:30:37] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[425]\ttrain-rmse:0.046598\ttrain-logloss:-0.137644\n",
      "[07:30:40] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:30:40] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[426]\ttrain-rmse:0.046594\ttrain-logloss:-0.137722\n",
      "[07:30:43] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:30:43] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[427]\ttrain-rmse:0.046592\ttrain-logloss:-0.137804\n",
      "[07:30:46] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:30:46] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[428]\ttrain-rmse:0.04659\ttrain-logloss:-0.137923\n",
      "[07:30:49] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:30:49] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[429]\ttrain-rmse:0.046587\ttrain-logloss:-0.13788\n",
      "[07:30:52] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:30:52] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[430]\ttrain-rmse:0.046584\ttrain-logloss:-0.138033\n",
      "[07:30:55] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:30:55] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[431]\ttrain-rmse:0.046582\ttrain-logloss:-0.13808\n",
      "[07:30:58] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:30:58] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[432]\ttrain-rmse:0.046579\ttrain-logloss:-0.138172\n",
      "[07:31:01] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:31:01] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[433]\ttrain-rmse:0.046575\ttrain-logloss:-0.138244\n",
      "[07:31:04] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:31:04] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[434]\ttrain-rmse:0.046571\ttrain-logloss:-0.138312\n",
      "[07:31:08] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:31:08] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[435]\ttrain-rmse:0.04657\ttrain-logloss:-0.138351\n",
      "[07:31:11] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:31:11] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[436]\ttrain-rmse:0.046567\ttrain-logloss:-0.138325\n",
      "[07:31:14] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:31:14] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[437]\ttrain-rmse:0.046566\ttrain-logloss:-0.138274\n",
      "[07:31:17] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:31:17] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[438]\ttrain-rmse:0.046564\ttrain-logloss:-0.138268\n",
      "[07:31:20] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:31:20] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[439]\ttrain-rmse:0.04656\ttrain-logloss:-0.138474\n",
      "[07:31:23] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:31:23] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[440]\ttrain-rmse:0.046558\ttrain-logloss:-0.138602\n",
      "[07:31:26] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:31:26] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[441]\ttrain-rmse:0.046555\ttrain-logloss:-0.138588\n",
      "[07:31:29] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:31:29] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[442]\ttrain-rmse:0.046552\ttrain-logloss:-0.139027\n",
      "[07:31:32] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:31:32] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[443]\ttrain-rmse:0.046547\ttrain-logloss:-0.139112\n",
      "[07:31:36] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:31:36] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[444]\ttrain-rmse:0.046544\ttrain-logloss:-0.13934\n",
      "[07:31:39] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:31:39] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[445]\ttrain-rmse:0.046543\ttrain-logloss:-0.13951\n",
      "[07:31:42] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:31:42] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[446]\ttrain-rmse:0.04654\ttrain-logloss:-0.139478\n",
      "[07:31:45] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:31:45] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[447]\ttrain-rmse:0.046538\ttrain-logloss:-0.139376\n",
      "[07:31:48] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:31:48] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[448]\ttrain-rmse:0.046535\ttrain-logloss:-0.139613\n",
      "[07:31:52] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:31:52] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[449]\ttrain-rmse:0.046531\ttrain-logloss:-0.139674\n",
      "[07:31:55] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:31:55] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[450]\ttrain-rmse:0.04653\ttrain-logloss:-0.139798\n",
      "[07:31:58] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:31:58] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[451]\ttrain-rmse:0.046528\ttrain-logloss:-0.139762\n",
      "[07:32:01] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:32:01] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[452]\ttrain-rmse:0.046525\ttrain-logloss:-0.139559\n",
      "[07:32:04] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:32:04] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[453]\ttrain-rmse:0.046524\ttrain-logloss:-0.13955\n",
      "[07:32:08] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:32:08] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[454]\ttrain-rmse:0.046522\ttrain-logloss:-0.139816\n",
      "[07:32:11] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:32:11] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[455]\ttrain-rmse:0.04652\ttrain-logloss:-0.13972\n",
      "[07:32:14] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:32:14] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[456]\ttrain-rmse:0.046519\ttrain-logloss:-0.139871\n",
      "[07:32:17] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:32:17] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[457]\ttrain-rmse:0.046515\ttrain-logloss:-0.139905\n",
      "[07:32:20] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:32:20] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[458]\ttrain-rmse:0.046512\ttrain-logloss:-0.139849\n",
      "[07:32:24] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:32:24] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[459]\ttrain-rmse:0.046511\ttrain-logloss:-0.139991\n",
      "[07:32:27] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:32:27] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[460]\ttrain-rmse:0.046508\ttrain-logloss:-0.139908\n",
      "[07:32:30] /root/alpha-mind/xgboost/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[07:32:30] /root/alpha-mind/xgboost/src/gbm/gbtree.cc:494: drop 0 trees, weight = 1\n",
      "[461]\ttrain-rmse:0.046504\ttrain-logloss:-0.140149\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from models.m1_xgb import *\n",
    "from conf.configuration import regress_conf\n",
    "from data.engines.sqlengine import SQLEngine\n",
    "import sqlalchemy as sa\n",
    "import sqlalchemy.orm as orm\n",
    "from data.engines.model import Record\n",
    "import xgboost as xgb\n",
    "import gc\n",
    "\n",
    "engine = SQLEngine('sqlite:////home/jupyter/jerry/workshop/MultiFactors/src/stacking/notebooks/real_tune_record.db')\n",
    "try:\n",
    "    # 获取当前持仓\n",
    "    pos_record = engine.fetch_record('pos_record')\n",
    "    previous_pos = pos_record[pos_record['trade_date'] == str(ref_date_pre)]\n",
    "except Exception as e:\n",
    "    alpha_logger.info('pos_record Exception:{0}'.format(e))\n",
    "    previous_pos = pd.DataFrame({'trade_date':[], 'weight':[],'industry':[], 'er':[],'code':[]})\n",
    "    \n",
    "alpha_logger.info('previous_data: {0}, pos_len: {1}'.format(ref_date_pre, len(previous_pos)))\n",
    "\n",
    "weight_gap = 1\n",
    "transact_cost = 0.003\n",
    "GPU_device = False\n",
    "\n",
    "executor = NaiveExecutor()\n",
    "leverags = []\n",
    "trade_dates = []\n",
    "current_pos = pd.DataFrame()\n",
    "tune_record = pd.DataFrame()\n",
    "rets = []\n",
    "net_rets = []\n",
    "turn_overs = []\n",
    "ics = []\n",
    "\n",
    "# take ref_dates[i] as an example\n",
    "alpha_logger.info('{0} is start'.format(ref_date))\n",
    "\n",
    "# machine learning model\n",
    "# Filter Training data\n",
    "# train data\n",
    "trade_date_pre = ref_date - timedelta(days=1)\n",
    "alpha_logger.info('trade_date_pre {0}'.format(trade_date_pre))\n",
    "# trade_date_pre_80 = ref_date - timedelta(days=80)\n",
    "\n",
    "# train = train_data[(train_data.trade_date <= trade_date_pre) & (trade_date_pre_80 <= train_data.trade_date)].dropna()\n",
    "# 训练集构造, 选择调仓日当天之前(不含当天)的因子数据作为训练集.\n",
    "train = train_data[train_data.trade_date <= trade_date_pre].dropna()\n",
    "\n",
    "if len(train) <= 0:\n",
    "    alpha_logger.info('{0} HAS NO TRAIN DATA!!!'.format(ref_date))\n",
    "\n",
    "x_train = train[features]\n",
    "y_train = train[label]\n",
    "alpha_logger.info('len_x_train: {0}, len_y_train: {1}'.format(len(x_train.values), len(y_train.values)))\n",
    "alpha_logger.info('X_train.shape={0}, X_test.shape = {1}'.format(np.shape(x_train), np.shape(y_train)))\n",
    "\n",
    "# load xgboost regression configuration\n",
    "regress_conf.xgb_config_r()\n",
    "regress_conf.cv_folds = None\n",
    "regress_conf.early_stop_round = 10\n",
    "regress_conf.max_round = 800\n",
    "if GPU_device:\n",
    "    # use GPUs\n",
    "    regress_conf.params.update({'tree_method': 'gpu_hist'})\n",
    "alpha_logger.info(\"params before: {}\".format(regress_conf.params))\n",
    "tic = time.time()\n",
    "\n",
    "# hyper_parameters optimization\n",
    "opt_parameters = {'max_depth': (2, 12),\n",
    "                   'gamma': (0.001, 10.0),\n",
    "                   'min_child_weight': (0, 20),\n",
    "                   'max_delta_step': (0, 10),\n",
    "                   'subsample': (0.01, 0.99),\n",
    "                   'colsample_bytree': (0.01, 0.99)}\n",
    "\n",
    "opt_xgb = BayesOptimizationXGB('regression', x_train, y_train)\n",
    "params_op = opt_xgb.train_opt(opt_parameters)\n",
    "regress_conf.params.update(params_op)\n",
    "alpha_logger.info(\"params after: {}\".format(regress_conf.params))\n",
    "alpha_logger.info(\"hyper params optimize time : {}\".format(time.time() - tic))\n",
    "\n",
    "# model training\n",
    "xgb_model = XGBooster(regress_conf)\n",
    "alpha_logger.info('xgb_model params: \\n{0}'.format(xgb_model.get_params()))\n",
    "\n",
    "\n",
    "best_score, best_round, best_model = xgb_model.fit(x_train, y_train)\n",
    "alpha_logger.info('Training time cost {}s'.format(time.time() - tic))\n",
    "alpha_logger.info('best_score = {}, best_round = {}'.format(best_score, best_round))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 当天数据预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取调仓日当天的因子数据作为输入.\n",
    "# total_data_test_excess = train_data[train_data.trade_date == str(ref_date)]\n",
    "total_data_test_excess = factor_data[factor_data.trade_date == ref_date]\n",
    "\n",
    "if len(total_data_test_excess) <=0:\n",
    "    alpha_logger.info('{} HAS NO DATA!!!'.format(ref_date))\n",
    "    exit()\n",
    "\n",
    "alpha_logger.info('{0} total_data_test_excess: {1}'.format(ref_date, len(total_data_test_excess)))\n",
    "\n",
    "# 获取调仓日当天的行业, 风险模型和基准权重数据\n",
    "industry_matrix = industry_total[industry_total.trade_date == ref_date]\n",
    "benchmark_weight = benchmark_total[benchmark_total.trade_date == ref_date]\n",
    "risk_matrix = risk_total[risk_total.trade_date == ref_date]\n",
    "\n",
    "total_data = pd.merge(industry_matrix, benchmark_weight, on=['code'], how='left').fillna(0.)\n",
    "total_data = pd.merge(total_data, risk_matrix, on=['code'])\n",
    "alpha_logger.info('{0} type_of_total_data: {1}'.format(ref_date, type(total_data)))\n",
    "alpha_logger.info('{0} shape_of_total_data: {1}'.format(ref_date, np.shape(total_data)))\n",
    "    \n",
    "total_data_test_excess = pd.merge(total_data, total_data_test_excess, on=['code'])\n",
    "alpha_logger.info('{0} len_of_total_data_test_excess: {1}'.format(ref_date, len(total_data_test_excess)))\n",
    "\n",
    "# 股票代码\n",
    "codes = total_data_test_excess.code.values.tolist()\n",
    "    \n",
    "## 获取调仓日当天的股票收益, 实时预测系统中没有dx\n",
    "# dx_returns = return_data[return_data.trade_date == ref_date][['code', 'dx']]\n",
    "    \n",
    "benchmark_w = total_data_test_excess.weight.values\n",
    "alpha_logger.info('type_of_benchmark_w: {}, shape_of_benchmark_w: {}'.format(type(benchmark_w), np.shape(benchmark_w)))\n",
    "is_in_benchmark = (benchmark_w > 0.).astype(float).reshape((-1, 1))\n",
    "# 风险模型数据合并\n",
    "total_risk_exp = np.concatenate([total_data_test_excess[constraint_risk].values.astype(float),\n",
    "                                 is_in_benchmark,\n",
    "                                 np.ones_like(is_in_benchmark)],\n",
    "                                axis=1)\n",
    "\n",
    "alpha_logger.info('shape_of_total_risk_exp_pre: {}'.format(np.shape(total_risk_exp)))\n",
    "total_risk_exp = pd.DataFrame(total_risk_exp, columns=total_risk_names)\n",
    "alpha_logger.info('shape_of_total_risk_exp: {}'.format(np.shape(total_risk_exp)))\n",
    "\n",
    "constraints = LinearConstraints(bounds, total_risk_exp, benchmark_w)\n",
    "alpha_logger.info('constraints: {0} in {1}'.format(np.shape(constraints.risk_targets()), ref_date))\n",
    "    \n",
    "lbound = np.maximum(0., benchmark_w - weight_gap)\n",
    "ubound = weight_gap + benchmark_w\n",
    "alpha_logger.info('lbound: {0} in {1}'.format(np.shape(lbound), ref_date))\n",
    "alpha_logger.info('ubound: {0} in {1}'.format(np.shape(ubound), ref_date))\n",
    "    \n",
    "# predict\n",
    "# alpha_logger.info('total_data_test_excess: \\n{}'.format(total_data_test_excess[['weight', 'code', 'industry']]))\n",
    "x_pred = total_data_test_excess[features]\n",
    "predict_xgboost = xgb_model.predict(best_model, x_pred)\n",
    "# alpha_logger.info('predict_xgboost: {}'.format(predict_xgboost))\n",
    "    \n",
    "a = np.shape(predict_xgboost)\n",
    "predict_xgboost = np.reshape(predict_xgboost, (a[0], -1)).astype(np.float64)\n",
    "alpha_logger.info('shape_of_predict_xgboost: {}'.format(np.shape(predict_xgboost)))\n",
    "alpha_logger.info('shape_of_predict_xgboost: {}'.format(type(predict_xgboost)))\n",
    "\n",
    "# 收益率预测结果    \n",
    "predict_xgboost_df = pd.DataFrame({'xgb_pre': list(predict_xgboost.reshape(-1))})\n",
    "predict_xgboost_df['trade_date'] = ref_date\n",
    "predict_xgboost_df['code'] = codes\n",
    "predict_xgboost_df['code'] = predict_xgboost_df['code'].apply(lambda x: \"{:06d}\".format(x) + '.XSHG' if len(str(x))==6 and str(x)[0] in '6' else \"{:06d}\".format(x) + '.XSHE')\n",
    "\n",
    "# 股票过滤, 组合优化之前过滤掉(未完成)\n",
    "    \n",
    "# 导入昨持仓并做适当调整\n",
    "previous_pos = total_data_test_excess[['code']].merge(previous_pos, on=['code'], how='left').fillna(0)\n",
    "# 组合优化\n",
    "try:\n",
    "    target_pos, _ = er_portfolio_analysis(predict_xgboost,\n",
    "                                              total_data_test_excess['industry'].values,\n",
    "                                              None,\n",
    "                                              constraints,\n",
    "                                              False,\n",
    "                                              benchmark_w,\n",
    "                                              method='risk_neutral',\n",
    "                                              lbound=lbound,\n",
    "                                              ubound=ubound,\n",
    "                                              turn_over_target=0.5,\n",
    "                                              current_pos=previous_pos.weight.values)\n",
    "except:\n",
    "    import pdb\n",
    "    pdb.set_trace()\n",
    "alpha_logger.info('shape_of_target_pos: {}'.format(np.shape(target_pos)))\n",
    "alpha_logger.info('len_codes:{}'.format(np.shape(codes)))\n",
    "target_pos['code'] = codes\n",
    "alpha_logger.info('target_pos: \\n{}'.format(target_pos))\n",
    "    \n",
    "#     result = pd.merge(target_pos, dx_returns, on=['code'])\n",
    "result = target_pos\n",
    "result['trade_date'] = ref_date\n",
    "tune_record = tune_record.append(result)\n",
    "alpha_logger.info('len_result: {}'.format(len(result)))\n",
    "    \n",
    "    # excess_return = np.exp(result.dx.values) - 1. - index_return.loc[ref_date, 'dx']\n",
    "#     excess_return = np.exp(result.dx.values) - 1.\n",
    "#     ret = result.weight.values @ excess_return\n",
    "    \n",
    "#     trade_dates.append(ref_date)\n",
    "#     rets.append(np.log(1. + ret))\n",
    "#     alpha_logger.info('len_rets: {}, len_trade_dates: {}'.format(len(rets), len(trade_dates)))\n",
    "    \n",
    "#     turn_over_org, current_pos = executor.execute(target_pos=target_pos)\n",
    "#     alpha_logger.info('turn_over_org: {}'.format(turn_over_org))\n",
    "#     current_pos['trade_date'] = str(ref_date)\n",
    "    \n",
    "    # 保存当前持仓信息\n",
    "#     engine.del_historical_data('pos_record', str(ref_date))  # 删除同日期的历史数据\n",
    "#     engine.write_data('pos_record', current_pos)\n",
    "\n",
    "#     turn_over = turn_over_org / sum(target_pos.weight.values)\n",
    "#     alpha_logger.info('turn_over: {}'.format(turn_over))\n",
    "#     turn_overs.append(turn_over)\n",
    "    \n",
    "#     executor.set_current(current_pos)\n",
    "#     net_rets.append(np.log(1. + ret - transact_cost * turn_over))\n",
    "#     alpha_logger.info('len_net_rets: {}, len_trade_dates: {}'.format(len(net_rets), len(trade_dates)))\n",
    "    \n",
    "alpha_logger.info('{} is finished'.format(ref_date))\n",
    "\n",
    "# ret_df = pd.DataFrame({'xgb_regress': rets}, index=trade_dates)\n",
    "# ret_df = pd.DataFrame({'xgb_regress': rets, 'net_xgb_regress': net_rets}, index=trade_dates)\n",
    "# ret_df.loc[advanceDateByCalendar('china.sse', ref_dates[-1], freq).strftime('%Y-%m-%d')] = 0.\n",
    "# ret_df = ret_df.shift(1)\n",
    "# ret_df.iloc[0] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP20\n",
    "# tune_record['code'] = tune_record['code'].apply(lambda x: \"{:06d}\".format(x) + '.XSHG' if len(str(x))==6 and str(x)[0] in '6' else \"{:06d}\".format(x) + '.XSHE')\n",
    "# tune_record.sort_values(by='weight', ascending=False)[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP20\n",
    "tune_record['code'] = tune_record['code'].apply(lambda x: \"{:06d}\".format(x) + '.SH' if len(str(x))==6 and str(x)[0] in '6' else \"{:06d}\".format(x) + '.SZ')\n",
    "tune_record.sort_values(by='weight', ascending=False)[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = tune_record.sort_values(by='weight', ascending=False)[:50]\n",
    "aa['weight'] = aa['weight'] / aa['weight'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.to_csv('20191210.csv', encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
