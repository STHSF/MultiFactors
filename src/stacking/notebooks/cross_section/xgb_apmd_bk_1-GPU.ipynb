{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# alpha1-alpha70 risk_neutral回测结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用risk neutral过程中, non kfolds的情况下, xgboost的max_round=1000, early_stop=20时的结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import pandas as pd\n",
    "from PyFin.api import *\n",
    "from alphamind.api import *\n",
    "from src.conf.models import *\n",
    "import numpy as np\n",
    "from alphamind.execution.naiveexecutor import NaiveExecutor\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data_source = 'postgresql+psycopg2://alpha:alpha@180.166.26.82:8889/alpha'\n",
    "engine = SqlEngine(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe = Universe('zz500')\n",
    "freq = '2b'\n",
    "benchmark_code = 905\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2019-05-01'\n",
    "ref_dates = makeSchedule(start_date, end_date, freq, 'china.sse')\n",
    "horizon = map_freq(freq)\n",
    "industry_name = 'sw'\n",
    "industry_level = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "factors_store = {\n",
    "    'f0': CSQuantiles(LAST('alpha_1')), 'f1': CSQuantiles(LAST('alpha_2')), 'f2': CSQuantiles(LAST('alpha_3')),\n",
    "    'f3': CSQuantiles(LAST('alpha_4')), 'f4': CSQuantiles(LAST('alpha_5')), 'f5': CSQuantiles(LAST('alpha_6')),\n",
    "    'f6': CSQuantiles(LAST('alpha_7')), 'f7': CSQuantiles(LAST('alpha_8')), 'f8': CSQuantiles(LAST('alpha_9')),\n",
    "    'f9': CSQuantiles(LAST('alpha_10')), 'f10': CSQuantiles(LAST('alpha_11')), 'f11': CSQuantiles(LAST('alpha_12')),\n",
    "    'f12': CSQuantiles(LAST('alpha_13')), 'f13': CSQuantiles(LAST('alpha_14')), 'f14': CSQuantiles(LAST('alpha_15')),\n",
    "    'f15': CSQuantiles(LAST('alpha_16')), 'f16': CSQuantiles(LAST('alpha_17')), 'f17': CSQuantiles(LAST('alpha_18')),\n",
    "    'f18': CSQuantiles(LAST('alpha_19')), 'f19': CSQuantiles(LAST('alpha_20')), 'f20': CSQuantiles(LAST('alpha_21')),\n",
    "    'f21': CSQuantiles(LAST('alpha_22')), 'f22': CSQuantiles(LAST('alpha_23')), 'f23': CSQuantiles(LAST('alpha_24')),\n",
    "    'f24': CSQuantiles(LAST('alpha_25')), 'f25': CSQuantiles(LAST('alpha_26')), 'f26': CSQuantiles(LAST('alpha_27')),\n",
    "    'f27': CSQuantiles(LAST('alpha_28')), 'f28': CSQuantiles(LAST('alpha_29')), 'f29': CSQuantiles(LAST('alpha_30')),\n",
    "    'f30': CSQuantiles(LAST('alpha_31')), 'f31': CSQuantiles(LAST('alpha_32')), 'f32': CSQuantiles(LAST('alpha_33')),\n",
    "    'f33': CSQuantiles(LAST('alpha_34')), 'f34': CSQuantiles(LAST('alpha_35')), 'f35': CSQuantiles(LAST('alpha_36')),\n",
    "    'f36': CSQuantiles(LAST('alpha_37')), 'f37': CSQuantiles(LAST('alpha_38')), 'f38': CSQuantiles(LAST('alpha_39')),\n",
    "    'f39': CSQuantiles(LAST('alpha_40')), 'f40': CSQuantiles(LAST('alpha_41')), 'f41': CSQuantiles(LAST('alpha_42')),\n",
    "    'f42': CSQuantiles(LAST('alpha_43')), 'f43': CSQuantiles(LAST('alpha_44')), 'f44': CSQuantiles(LAST('alpha_45')),\n",
    "    'f45': CSQuantiles(LAST('alpha_46')), 'f46': CSQuantiles(LAST('alpha_47')), 'f47': CSQuantiles(LAST('alpha_48')),\n",
    "    'f48': CSQuantiles(LAST('alpha_49')), 'f49': CSQuantiles(LAST('alpha_50')), 'f50': CSQuantiles(LAST('alpha_51')),\n",
    "    'f51': CSQuantiles(LAST('alpha_52')), 'f52': CSQuantiles(LAST('alpha_53')), 'f53': CSQuantiles(LAST('alpha_54')),\n",
    "    'f54': CSQuantiles(LAST('alpha_55')), 'f55': CSQuantiles(LAST('alpha_56')), 'f56': CSQuantiles(LAST('alpha_57')),\n",
    "    'f57': CSQuantiles(LAST('alpha_58')), 'f58': CSQuantiles(LAST('alpha_59')), 'f59': CSQuantiles(LAST('alpha_60')),\n",
    "    'f60': CSQuantiles(LAST('alpha_61')), 'f61': CSQuantiles(LAST('alpha_62')), 'f62': CSQuantiles(LAST('alpha_63')),\n",
    "    'f63': CSQuantiles(LAST('alpha_64')), 'f64': CSQuantiles(LAST('alpha_65')), 'f65': CSQuantiles(LAST('alpha_66')),\n",
    "    'f66': CSQuantiles(LAST('alpha_67')), 'f67': CSQuantiles(LAST('alpha_68')), 'f68': CSQuantiles(LAST('alpha_69')),\n",
    "    'f69': CSQuantiles(LAST('alpha_70')), 'f70': CSQuantiles(LAST('alpha_71')), 'f71': CSQuantiles(LAST('alpha_72')),\n",
    "    'f72': CSQuantiles(LAST('alpha_73')), 'f73': CSQuantiles(LAST('alpha_74')), 'f74': CSQuantiles(LAST('alpha_75')),\n",
    "    'f75': CSQuantiles(LAST('alpha_76')), 'f76': CSQuantiles(LAST('alpha_77')), 'f77': CSQuantiles(LAST('alpha_78')),\n",
    "    'f78': CSQuantiles(LAST('alpha_79')), 'f79': CSQuantiles(LAST('alpha_80')), 'f80': CSQuantiles(LAST('alpha_81')),\n",
    "    'f81': CSQuantiles(LAST('alpha_82')), 'f82': CSQuantiles(LAST('alpha_83')), 'f83': CSQuantiles(LAST('alpha_84')),\n",
    "    'f84': CSQuantiles(LAST('alpha_85')), 'f85': CSQuantiles(LAST('alpha_86')), 'f86': CSQuantiles(LAST('alpha_86')),\n",
    "    'f87': CSQuantiles(LAST('alpha_88')), 'f87': CSQuantiles(LAST('alpha_88')), 'f88': CSQuantiles(LAST('alpha_89')),\n",
    "    'f90': CSQuantiles(LAST('alpha_91')), 'f91': CSQuantiles(LAST('alpha_92')), 'f92': CSQuantiles(LAST('alpha_93')),\n",
    "    'f93': CSQuantiles(LAST('alpha_94')), 'f94': CSQuantiles(LAST('alpha_95')), 'f95': CSQuantiles(LAST('alpha_96')),\n",
    "    'f96': CSQuantiles(LAST('alpha_97')), 'f97': CSQuantiles(LAST('alpha_98')), 'f98': CSQuantiles(LAST('alpha_99')),\n",
    "    'f99': CSQuantiles(LAST('alpha_100')), 'f100': CSQuantiles(LAST('alpha_101')), 'f101': CSQuantiles(LAST('alpha_103')),\n",
    "    'f102': CSQuantiles(LAST('alpha_102')), 'f103': CSQuantiles(LAST('alpha_104')), 'f104': CSQuantiles(LAST('alpha_104')),\n",
    "    'f105': CSQuantiles(LAST('alpha_105')), 'f106': CSQuantiles(LAST('alpha_107')), 'f107': CSQuantiles(LAST('alpha_107')),\n",
    "    'f108': CSQuantiles(LAST('alpha_108')), 'f109': CSQuantiles(LAST('alpha_110')), 'f110': CSQuantiles(LAST('alpha_110')),\n",
    "    'f111': CSQuantiles(LAST('alpha_111')), 'f112': CSQuantiles(LAST('alpha_113')), 'f113': CSQuantiles(LAST('alpha_113')),\n",
    "    'f114': CSQuantiles(LAST('alpha_114')), 'f115': CSQuantiles(LAST('alpha_116')), 'f116': CSQuantiles(LAST('alpha_116')),\n",
    "    'f117': CSQuantiles(LAST('alpha_118')), 'f118': CSQuantiles(LAST('alpha_119')), 'f129': CSQuantiles(LAST('alpha_119')),\n",
    "    'f120': CSQuantiles(LAST('alpha_121')), 'f121': CSQuantiles(LAST('alpha_122')), 'f122': CSQuantiles(LAST('alpha_123')),\n",
    "    'f123': CSQuantiles(LAST('alpha_124')), 'f124': CSQuantiles(LAST('alpha_125')), 'f125': CSQuantiles(LAST('alpha_126')),\n",
    "    'f126': CSQuantiles(LAST('alpha_127')), 'f127': CSQuantiles(LAST('alpha_128')), 'f128': CSQuantiles(LAST('alpha_129')),\n",
    "    'f129': CSQuantiles(LAST('alpha_130')), 'f130': CSQuantiles(LAST('alpha_131')), 'f131': CSQuantiles(LAST('alpha_132')),\n",
    "    'f132': CSQuantiles(LAST('alpha_133')), 'f133': CSQuantiles(LAST('alpha_134')), 'f134': CSQuantiles(LAST('alpha_135')),\n",
    "    'f135': CSQuantiles(LAST('alpha_136')), 'f136': CSQuantiles(LAST('alpha_137')), 'f137': CSQuantiles(LAST('alpha_140')),\n",
    "    'f138': CSQuantiles(LAST('alpha_139')), 'f139': CSQuantiles(LAST('alpha_140')), 'f140': CSQuantiles(LAST('alpha_141')),\n",
    "    'f141': CSQuantiles(LAST('alpha_142')), 'f142': CSQuantiles(LAST('alpha_143')), 'f143': CSQuantiles(LAST('alpha_144')),\n",
    "    'f144': CSQuantiles(LAST('alpha_145')), 'f145': CSQuantiles(LAST('alpha_146')), 'f146': CSQuantiles(LAST('alpha_147')),\n",
    "    'f147': CSQuantiles(LAST('alpha_148')), 'f148': CSQuantiles(LAST('alpha_149')), 'f149': CSQuantiles(LAST('alpha_150')),\n",
    "    'f150': CSQuantiles(LAST('alpha_151')), 'f151': CSQuantiles(LAST('alpha_152')), 'f153': CSQuantiles(LAST('alpha_154')),\n",
    "    'f153': CSQuantiles(LAST('alpha_154')), 'f154': CSQuantiles(LAST('alpha_155')), 'f155': CSQuantiles(LAST('alpha_156')),\n",
    "    'f156': CSQuantiles(LAST('alpha_157')), 'f157': CSQuantiles(LAST('alpha_158')), 'f158': CSQuantiles(LAST('alpha_159')),\n",
    "    'f159': CSQuantiles(LAST('alpha_160')), 'f160': CSQuantiles(LAST('alpha_161')), 'f161': CSQuantiles(LAST('alpha_162')),\n",
    "    'f162': CSQuantiles(LAST('alpha_163')), 'f163': CSQuantiles(LAST('alpha_164')), 'f164': CSQuantiles(LAST('alpha_165')),\n",
    "    'f165': CSQuantiles(LAST('alpha_166')), 'f166': CSQuantiles(LAST('alpha_167')), 'f167': CSQuantiles(LAST('alpha_167')),\n",
    "    'f168': CSQuantiles(LAST('alpha_169')), 'f169': CSQuantiles(LAST('alpha_170')), 'f170': CSQuantiles(LAST('alpha_171')),\n",
    "    'f171': CSQuantiles(LAST('alpha_172')), 'f172': CSQuantiles(LAST('alpha_173')), 'f173': CSQuantiles(LAST('alpha_174')),\n",
    "    'f174': CSQuantiles(LAST('alpha_175')), 'f175': CSQuantiles(LAST('alpha_176')), 'f176': CSQuantiles(LAST('alpha_176')),\n",
    "    'f177': CSQuantiles(LAST('alpha_178')), 'f178': CSQuantiles(LAST('alpha_179')), 'f179': CSQuantiles(LAST('alpha_180')),\n",
    "    'f180': CSQuantiles(LAST('alpha_181')), 'f181': CSQuantiles(LAST('alpha_182')), 'f182': CSQuantiles(LAST('alpha_183')),\n",
    "    'f183': CSQuantiles(LAST('alpha_184')), 'f184': CSQuantiles(LAST('alpha_185')), 'f185': CSQuantiles(LAST('alpha_186')),\n",
    "    'f186': CSQuantiles(LAST('alpha_187')), 'f187': CSQuantiles(LAST('alpha_188')), 'f188': CSQuantiles(LAST('alpha_189')),\n",
    "    'f189': CSQuantiles(LAST('alpha_190')), 'f190': CSQuantiles(LAST('alpha_191'))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "factors_store = {\n",
    "    'f0': LAST('alpha_1'), 'f1': LAST('alpha_2'), 'f2': LAST('alpha_3'),\n",
    "    'f3': LAST('alpha_4'), 'f4': LAST('alpha_5'), 'f5': LAST('alpha_6'),\n",
    "    'f6': LAST('alpha_7'), 'f7': LAST('alpha_8'), 'f8': LAST('alpha_9'),\n",
    "    'f9': LAST('alpha_10'), 'f10': LAST('alpha_11'), 'f11': LAST('alpha_12'),\n",
    "    'f12': LAST('alpha_13'), 'f13': LAST('alpha_14'), 'f14': LAST('alpha_15'),\n",
    "    'f15': LAST('alpha_16'), 'f16': LAST('alpha_17'), 'f17': LAST('alpha_18'),\n",
    "    'f18': LAST('alpha_19'), 'f19': LAST('alpha_20'), 'f20': LAST('alpha_21'),\n",
    "    'f21': LAST('alpha_22'), 'f22': LAST('alpha_23'), 'f23': LAST('alpha_24'),\n",
    "    'f24': LAST('alpha_25'), 'f25': LAST('alpha_26'), 'f26': LAST('alpha_27'),\n",
    "    'f27': LAST('alpha_28'), 'f28': LAST('alpha_29'), 'f29': LAST('alpha_30'),\n",
    "    'f30': LAST('alpha_31'), 'f31': LAST('alpha_32'), 'f32': LAST('alpha_33'),\n",
    "    'f33': LAST('alpha_34'), 'f34': LAST('alpha_35'), 'f35': LAST('alpha_36'),\n",
    "    'f36': LAST('alpha_37'), 'f37': LAST('alpha_38'), 'f38': LAST('alpha_39'),\n",
    "    'f39': LAST('alpha_40'), 'f40': LAST('alpha_41'), 'f41': LAST('alpha_42'),\n",
    "    'f42': LAST('alpha_43'), 'f43': LAST('alpha_44'), 'f44': LAST('alpha_45'),\n",
    "    'f45': LAST('alpha_46'), 'f46': LAST('alpha_47'), 'f47': LAST('alpha_48'),\n",
    "    'f48': LAST('alpha_49'), 'f49': LAST('alpha_50'), 'f50': LAST('alpha_51'),\n",
    "    'f51': LAST('alpha_52'), 'f52': LAST('alpha_53'), 'f53': LAST('alpha_54'),\n",
    "    'f54': LAST('alpha_55'), 'f55': LAST('alpha_56'), 'f56': LAST('alpha_57'),\n",
    "    'f57': LAST('alpha_58'), 'f58': LAST('alpha_59'), 'f59': LAST('alpha_60'),\n",
    "    'f60': LAST('alpha_61'), 'f61': LAST('alpha_62'), 'f62': LAST('alpha_63'),\n",
    "    'f63': LAST('alpha_64'), 'f64': LAST('alpha_65'), 'f65': LAST('alpha_66'),\n",
    "    'f66': LAST('alpha_67'), 'f67': LAST('alpha_68'), 'f68': LAST('alpha_69'),\n",
    "    'f69': LAST('alpha_70'), 'f70': LAST('alpha_71'), 'f71': LAST('alpha_72'),\n",
    "    'f72': LAST('alpha_73'), 'f73': LAST('alpha_74'), 'f74': LAST('alpha_75'),\n",
    "    'f75': LAST('alpha_76'), 'f76': LAST('alpha_77'), 'f77': LAST('alpha_78'),\n",
    "    'f78': LAST('alpha_79'), 'f79': LAST('alpha_80'), 'f80': LAST('alpha_81'),\n",
    "    'f81': LAST('alpha_82'), 'f82': LAST('alpha_83'), 'f83': LAST('alpha_84'),\n",
    "    'f84': LAST('alpha_85'), 'f85': LAST('alpha_86'), 'f86': LAST('alpha_86'),\n",
    "    'f87': LAST('alpha_88'), 'f87': LAST('alpha_88'), 'f88': LAST('alpha_89'),\n",
    "    'f90': LAST('alpha_91'), 'f91': LAST('alpha_92'), 'f92': LAST('alpha_93'),\n",
    "    'f93': LAST('alpha_94'), 'f94': LAST('alpha_95'), 'f95': LAST('alpha_96'),\n",
    "    'f96': LAST('alpha_97'), 'f97': LAST('alpha_98'), 'f98': LAST('alpha_99'),\n",
    "    'f99': LAST('alpha_100'), 'f100': LAST('alpha_101'), 'f101': LAST('alpha_103'),\n",
    "    'f102': LAST('alpha_102'), 'f103': LAST('alpha_104'), 'f104': LAST('alpha_104'),\n",
    "    'f105': LAST('alpha_105'), 'f106': LAST('alpha_107'), 'f107': LAST('alpha_107'),\n",
    "    'f108': LAST('alpha_108'), 'f109': LAST('alpha_110'), 'f110': LAST('alpha_110'),\n",
    "    'f111': LAST('alpha_111'), 'f112': LAST('alpha_113'), 'f113': LAST('alpha_113'),\n",
    "    'f114': LAST('alpha_114'), 'f115': LAST('alpha_116'), 'f116': LAST('alpha_116'),\n",
    "    'f117': LAST('alpha_118'), 'f118': LAST('alpha_119'), 'f129': LAST('alpha_119'),\n",
    "    'f120': LAST('alpha_121'), 'f121': LAST('alpha_122'), 'f122': LAST('alpha_123'),\n",
    "    'f123': LAST('alpha_124'), 'f124': LAST('alpha_125'), 'f125': LAST('alpha_126'),\n",
    "    'f126': LAST('alpha_127'), 'f127': LAST('alpha_128'), 'f128': LAST('alpha_129'),\n",
    "    'f129': LAST('alpha_130'), 'f130': LAST('alpha_131'), 'f131': LAST('alpha_132'),\n",
    "    'f132': LAST('alpha_133'), 'f133': LAST('alpha_134'), 'f134': LAST('alpha_135'),\n",
    "    'f135': LAST('alpha_136'), 'f136': LAST('alpha_137'), 'f137': LAST('alpha_140'),\n",
    "    'f138': LAST('alpha_139'), 'f139': LAST('alpha_140'), 'f140': LAST('alpha_141'),\n",
    "    'f141': LAST('alpha_142'), 'f142': LAST('alpha_143'), 'f143': LAST('alpha_144'),\n",
    "    'f144': LAST('alpha_145'), 'f145': LAST('alpha_146'), 'f146': LAST('alpha_147'),\n",
    "    'f147': LAST('alpha_148'), 'f148': LAST('alpha_149'), 'f149': LAST('alpha_150'),\n",
    "    'f150': LAST('alpha_151'), 'f151': LAST('alpha_152'), 'f153': LAST('alpha_154'),\n",
    "    'f153': LAST('alpha_154'), 'f154': LAST('alpha_155'), 'f155': LAST('alpha_156'),\n",
    "    'f156': LAST('alpha_157'), 'f157': LAST('alpha_158'), 'f158': LAST('alpha_159'),\n",
    "    'f159': LAST('alpha_160'), 'f160': LAST('alpha_161'), 'f161': LAST('alpha_162'),\n",
    "    'f162': LAST('alpha_163'), 'f163': LAST('alpha_164'), 'f164': LAST('alpha_165'),\n",
    "    'f165': LAST('alpha_166'), 'f166': LAST('alpha_167'), 'f167': LAST('alpha_167'),\n",
    "    'f168': LAST('alpha_169'), 'f169': LAST('alpha_170'), 'f170': LAST('alpha_171'),\n",
    "    'f171': LAST('alpha_172'), 'f172': LAST('alpha_173'), 'f173': LAST('alpha_174'),\n",
    "    'f174': LAST('alpha_175'), 'f175': LAST('alpha_176'), 'f176': LAST('alpha_176'),\n",
    "    'f177': LAST('alpha_178'), 'f178': LAST('alpha_179'), 'f179': LAST('alpha_180'),\n",
    "    'f180': LAST('alpha_181'), 'f181': LAST('alpha_182'), 'f182': LAST('alpha_183'),\n",
    "    'f183': LAST('alpha_184'), 'f184': LAST('alpha_185'), 'f185': LAST('alpha_186'),\n",
    "    'f186': LAST('alpha_187'), 'f187': LAST('alpha_188'), 'f188': LAST('alpha_189'),\n",
    "    'f189': LAST('alpha_190'), 'f190': LAST('alpha_191')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "factor_data_org = engine.fetch_factor_range(universe, factors_store, dates=ref_dates, used_factor_tables=[Alpha191])\n",
    "factors = list(factors_store.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "industry = engine.fetch_industry_range(universe, dates=ref_dates)\n",
    "factor_data = pd.merge(factor_data_org, industry, on=['trade_date', 'code']).fillna(0.)\n",
    "risk_total = engine.fetch_risk_model_range(universe, dates=ref_dates)[1]\n",
    "len(factor_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "return_data = engine.fetch_dx_return_range(universe, dates=ref_dates, horizon=horizon, offset=0,benchmark = benchmark_code)\n",
    "len(return_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "benchmark_total = engine.fetch_benchmark_range(dates=ref_dates, benchmark=benchmark_code)\n",
    "industry_total = engine.fetch_industry_matrix_range(universe, dates=ref_dates, category=industry_name, level=industry_level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Constraintes settings\n",
    "weight_gap = 1\n",
    "industry_names = industry_list(industry_name, industry_level)\n",
    "constraint_risk = ['EARNYILD', 'LIQUIDTY', 'GROWTH', 'SIZE', 'BETA', 'MOMENTUM'] + industry_names\n",
    "total_risk_names = constraint_risk + ['benchmark', 'total']\n",
    "\n",
    "b_type = []\n",
    "l_val = []\n",
    "u_val = []\n",
    "\n",
    "previous_pos = pd.DataFrame()\n",
    "rets = []\n",
    "turn_overs = []\n",
    "leverags = []\n",
    "trade_dates = []\n",
    "\n",
    "transact_cost = 0.003\n",
    "current_pos = pd.DataFrame()\n",
    "executor = NaiveExecutor()\n",
    "net_rets = []\n",
    "\n",
    "for name in total_risk_names:\n",
    "        if name == 'benchmark':\n",
    "            b_type.append(BoundaryType.RELATIVE)\n",
    "            l_val.append(0.0)\n",
    "            u_val.append(1.0)\n",
    "        elif name == 'total':\n",
    "            b_type.append(BoundaryType.ABSOLUTE)\n",
    "            l_val.append(.0)\n",
    "            u_val.append(.0)\n",
    "        else:\n",
    "            b_type.append(BoundaryType.ABSOLUTE)\n",
    "            l_val.append(-1.005)\n",
    "            u_val.append(1.005)\n",
    "\n",
    "bounds = create_box_bounds(total_risk_names, b_type, l_val, u_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.merge(factor_data, return_data, on=['trade_date', 'code']).dropna()\n",
    "# train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13',\n",
    "            'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26',\n",
    "            'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39',\n",
    "            'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52',\n",
    "            'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65',\n",
    "            'f66', 'f67', 'f68', 'f69']\n",
    "# features = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9']\n",
    "label = ['dx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from datetime import datetime, timedelta\n",
    "from m1_xgb import *\n",
    "from src.conf.configuration import regress_conf\n",
    "import xgboost as xgb\n",
    "import gc\n",
    "pred_df = pd.DataFrame()\n",
    "\n",
    "def create_scenario(weight_gap):\n",
    "    previous_pos = pd.DataFrame()\n",
    "    rets = []\n",
    "    turn_overs = []\n",
    "    leverags = []\n",
    "    ics = []\n",
    "    # take ref_dates[i] as an example\n",
    "    for i, ref_date in enumerate(ref_dates):\n",
    "        alpha_logger.info('{0} is start'.format(ref_date))\n",
    "\n",
    "        # machine learning model\n",
    "        # Filter Training data\n",
    "        # train data\n",
    "        trade_date_pre = ref_date - timedelta(days=1)\n",
    "        trade_date_pre_80 = ref_date - timedelta(days=80)\n",
    "        \n",
    "        # train = train_data[(train_data.trade_date <= trade_date_pre) & (trade_date_pre_80 <= train_data.trade_date)].dropna()\n",
    "        train = train_data[train_data.trade_date <= trade_date_pre].dropna()\n",
    "\n",
    "        if len(train) <= 0:\n",
    "            continue\n",
    "        x_train = train[features]\n",
    "        y_train = train[label]\n",
    "        alpha_logger.info('len_x_train: {0}, len_y_train: {1}'.format(len(x_train.values), len(y_train.values)))\n",
    "        alpha_logger.info('X_train.shape={0}, X_test.shape = {1}'.format(np.shape(x_train), np.shape(y_train)))\n",
    "\n",
    "        # xgb_configuration\n",
    "        regress_conf.xgb_config_r()\n",
    "        regress_conf.cv_folds = None\n",
    "        regress_conf.early_stop_round = 10\n",
    "        regress_conf.max_round = 500\n",
    "        tic = time.time()\n",
    "        # training\n",
    "        xgb_model = XGBooster(regress_conf)\n",
    "        xgb_model.set_params(tree_method='gpu_hist', max_depth=5)\n",
    "        print(xgb_model.get_params)\n",
    "        best_score, best_round, cv_rounds, best_model = xgb_model.fit(x_train, y_train)\n",
    "        alpha_logger.info('Training time cost {}s'.format(time.time() - tic))\n",
    "        alpha_logger.info('best_score = {}, best_round = {}'.format(best_score, best_round))\n",
    "    \n",
    "        # Test data\n",
    "        total_data_test_excess = train_data[train_data.trade_date == ref_date]\n",
    "        alpha_logger.info('{0} total_data_test_excess: {1}'.format(ref_date, len(total_data_test_excess)))\n",
    "\n",
    "        if len(total_data_test_excess) <= 0:\n",
    "            alpha_logger.info('{0} HAS NO DATA!!!'.format(ref_date))\n",
    "            continue\n",
    "\n",
    "        industry_matrix = industry_total[industry_total.trade_date == ref_date]\n",
    "        benchmark_w = benchmark_total[benchmark_total.trade_date == ref_date]\n",
    "        risk_matrix = risk_total[risk_total.trade_date == ref_date]\n",
    "\n",
    "        total_data = pd.merge(industry_matrix, benchmark_w, on=['code'], how='left').fillna(0.)\n",
    "        total_data = pd.merge(total_data, risk_matrix, on=['code'])\n",
    "        alpha_logger.info('{0} len_of_total_data: {1}'.format(ref_date, len(total_data)))\n",
    "\n",
    "        total_data_test_excess = pd.merge(total_data, total_data_test_excess, on=['code'])\n",
    "        alpha_logger.info('{0} len_of_total_data_test_excess: {1}'.format(ref_date, len(total_data_test_excess)))\n",
    "        \n",
    "        codes = total_data_test_excess.code.values.tolist()\n",
    "        alpha_logger.info('{0} full re-balance: {1}'.format(ref_date, len(codes)))\n",
    "        dx_returns = return_data[return_data.trade_date == ref_date][['code', 'dx']]\n",
    "\n",
    "        benchmark_w = total_data_test_excess.weight.values\n",
    "        alpha_logger.info('shape_of_benchmark_w: {}'.format(np.shape(benchmark_w)))\n",
    "        is_in_benchmark = (benchmark_w > 0.).astype(float).reshape((-1, 1))\n",
    "        total_risk_exp = np.concatenate([total_data_test_excess[constraint_risk].values.astype(float),\n",
    "                                         is_in_benchmark,\n",
    "                                         np.ones_like(is_in_benchmark)],\n",
    "                                         axis=1)\n",
    "        alpha_logger.info('shape_of_total_risk_exp_pre: {}'.format(np.shape(total_risk_exp)))\n",
    "        total_risk_exp = pd.DataFrame(total_risk_exp, columns=total_risk_names)\n",
    "        alpha_logger.info('shape_of_total_risk_exp: {}'.format(np.shape(total_risk_exp)))\n",
    "        constraints = LinearConstraints(bounds, total_risk_exp, benchmark_w)\n",
    "        alpha_logger.info('constraints: {0} in {1}'.format(np.shape(constraints.risk_targets()), ref_date))\n",
    "\n",
    "        lbound = np.maximum(0., benchmark_w - weight_gap)\n",
    "        ubound = weight_gap + benchmark_w\n",
    "        alpha_logger.info('lbound: {0} in {1}'.format(np.shape(lbound), ref_date))\n",
    "        alpha_logger.info('ubound: {0} in {1}'.format(np.shape(ubound), ref_date))\n",
    "        # alpha_logger.info('lbound: \\n{}'.format(lbound))\n",
    "        # alpha_logger.info('ubound: \\n{}'.format(ubound))\n",
    "\n",
    "        # predict\n",
    "        x_pred = total_data_test_excess[features]\n",
    "        dpred = xgb.DMatrix(x_pred.values)\n",
    "        predict_xgboost = best_model.predict(dpred)\n",
    "        a = np.shape(predict_xgboost)\n",
    "        predict_xgboost = np.reshape(predict_xgboost, (a[0], -1)).astype(np.float64)\n",
    "        alpha_logger.info('shape_of_predict_xgboost: {}'.format(np.shape(predict_xgboost)))\n",
    "        # alpha_logger.info('predict_xgboost: {}'.format(predict_xgboost))\n",
    "        del xgb_model\n",
    "        del best_model\n",
    "        gc.collect()\n",
    "        \n",
    "        # backtest\n",
    "        try:\n",
    "            target_pos, _ = er_portfolio_analysis(predict_xgboost,\n",
    "                                                  total_data_test_excess['industry'].values,\n",
    "                                                  None,\n",
    "                                                  constraints,\n",
    "                                                  False,\n",
    "                                                  benchmark_w,\n",
    "                                                  method = 'risk_neutral',\n",
    "                                                  lbound=lbound,\n",
    "                                                  ubound=ubound)\n",
    "        except:\n",
    "            import ipdb\n",
    "            ipdb.set_trace()\n",
    "            alpha_logger.info('result: \\n{}'.format(result))\n",
    "\n",
    "        alpha_logger.info('target_pos: {}'.format(np.shape(target_pos)))\n",
    "        alpha_logger.info('len_codes:{}'.format(np.shape(codes)))\n",
    "        target_pos['code'] = codes\n",
    "        \n",
    "        result = pd.merge(target_pos, dx_returns, on=['code'])\n",
    "        alpha_logger.info('len_result: {}'.format(np.shape(result)))\n",
    "\n",
    "        # excess_return = np.exp(result.dx.values) - 1. - index_return.loc[ref_date, 'dx']\n",
    "        excess_return = np.exp(result.dx.values) - 1.\n",
    "        ret = result.weight.values @ excess_return\n",
    "        \n",
    "        trade_dates.append(ref_date)\n",
    "        rets.append(np.log(1. + ret))\n",
    "        alpha_logger.info('len_rets: {}, len_trade_dates: {}'.format(len(rets), len(trade_dates)))\n",
    "        \n",
    "        turn_over_org, current_pos = executor.execute(target_pos=target_pos)\n",
    "        turn_over = turn_over_org / sum(target_pos.weight.values)\n",
    "        executor.set_current(current_pos)\n",
    "        net_rets.append(np.log(1. + ret - transact_cost * turn_over))        \n",
    "        alpha_logger.info('len_net_rets: {}, len_trade_dates: {}'.format(len(net_rets), len(trade_dates)))\n",
    "        \n",
    "        alpha_logger.info('{} is finished'.format(ref_date))\n",
    "        \n",
    "    # ret_df = pd.DataFrame({'xgb_regress': rets}, index=trade_dates)\n",
    "    ret_df = pd.DataFrame({'xgb_regress': rets, 'net_xgb_regress':net_rets}, index=trade_dates)\n",
    "    ret_df.loc[advanceDateByCalendar('china.sse', ref_dates[-1], freq).strftime('%Y-%m-%d')] = 0.\n",
    "    ret_df = ret_df.shift(1)\n",
    "    ret_df.iloc[0] = 0.\n",
    "    return ret_df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_df = create_scenario(weight_gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ret_df[['xgb_regress', 'net_xgb_regress']].cumsum().plot(figsize=(12, 6), \n",
    "                                      title='Fixed freq rebalanced: {0}'.format(freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"年化收益: {0:.2f}\".format(len(rets) * np.mean(ret_df['net_xgb_regress'])))\n",
    "print(\"夏普比率: {0:.2f}\".format(np.sqrt(126) * np.mean(ret_df['net_xgb_regress']/ np.std(ret_df['net_xgb_regress']))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"年化收益: {0:.2f}\".format(len(rets) * np.mean(ret_df['xgb_regress'])))\n",
    "print(\"夏普比率: {0:.2f}\".format(np.sqrt(126) * np.mean(ret_df['xgb_regress']/ np.std(ret_df['xgb_regress']))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
