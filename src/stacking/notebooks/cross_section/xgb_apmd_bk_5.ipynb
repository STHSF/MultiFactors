{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# alpha1-alpha10因子risk—netural回测结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加费后曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import pandas as pd\n",
    "from PyFin.api import *\n",
    "from alphamind.api import *\n",
    "from src.conf.models import *\n",
    "import numpy as np\n",
    "from alphamind.execution.naiveexecutor import NaiveExecutor\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data_source = 'postgresql+psycopg2://alpha:alpha@180.166.26.82:8889/alpha'\n",
    "engine = SqlEngine(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe = Universe('zz500')\n",
    "freq = '2b'\n",
    "benchmark_code = 905\n",
    "start_date = '2014-01-01'\n",
    "end_date = '2019-05-01'\n",
    "ref_dates = makeSchedule(start_date, end_date, freq, 'china.sse')\n",
    "horizon = map_freq(freq)\n",
    "industry_name = 'sw'\n",
    "industry_level = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# factors_store = {\n",
    "#     'f0': CSQuantiles(LAST('alpha_1')), 'f1': CSQuantiles(LAST('alpha_2')), 'f2': CSQuantiles(LAST('alpha_3')),\n",
    "#     'f3': CSQuantiles(LAST('alpha_4')), 'f4': CSQuantiles(LAST('alpha_5')), 'f5': CSQuantiles(LAST('alpha_6')),\n",
    "#     'f6': CSQuantiles(LAST('alpha_7')), 'f7': CSQuantiles(LAST('alpha_8')), 'f8': CSQuantiles(LAST('alpha_9')),\n",
    "#     'f9': CSQuantiles(LAST('alpha_10')), 'f10': CSQuantiles(LAST('alpha_11')), 'f11': CSQuantiles(LAST('alpha_12')),\n",
    "#     'f12': CSQuantiles(LAST('alpha_13')), 'f13': CSQuantiles(LAST('alpha_14')), 'f14': CSQuantiles(LAST('alpha_15')),\n",
    "#     'f15': CSQuantiles(LAST('alpha_16')), 'f16': CSQuantiles(LAST('alpha_17')), 'f17': CSQuantiles(LAST('alpha_18')),\n",
    "#     'f18': CSQuantiles(LAST('alpha_19')), 'f19': CSQuantiles(LAST('alpha_20')), 'f20': CSQuantiles(LAST('alpha_21')),\n",
    "#     'f21': CSQuantiles(LAST('alpha_22')), 'f22': CSQuantiles(LAST('alpha_23')), 'f23': CSQuantiles(LAST('alpha_24')),\n",
    "#     'f24': CSQuantiles(LAST('alpha_25')), 'f25': CSQuantiles(LAST('alpha_26')), 'f26': CSQuantiles(LAST('alpha_27')),\n",
    "#     'f27': CSQuantiles(LAST('alpha_28')), 'f28': CSQuantiles(LAST('alpha_29')), 'f29': CSQuantiles(LAST('alpha_30')),\n",
    "#     'f30': CSQuantiles(LAST('alpha_31')), 'f31': CSQuantiles(LAST('alpha_32')), 'f32': CSQuantiles(LAST('alpha_33')),\n",
    "#     'f33': CSQuantiles(LAST('alpha_34')), 'f34': CSQuantiles(LAST('alpha_35')), 'f35': CSQuantiles(LAST('alpha_36')),\n",
    "#     'f36': CSQuantiles(LAST('alpha_37')), 'f37': CSQuantiles(LAST('alpha_38')), 'f38': CSQuantiles(LAST('alpha_39')),\n",
    "#     'f39': CSQuantiles(LAST('alpha_40')), 'f40': CSQuantiles(LAST('alpha_41')), 'f41': CSQuantiles(LAST('alpha_42')),\n",
    "#     'f42': CSQuantiles(LAST('alpha_43')), 'f43': CSQuantiles(LAST('alpha_44')), 'f44': CSQuantiles(LAST('alpha_45')),\n",
    "#     'f45': CSQuantiles(LAST('alpha_46')), 'f46': CSQuantiles(LAST('alpha_47')), 'f47': CSQuantiles(LAST('alpha_48')),\n",
    "#     'f48': CSQuantiles(LAST('alpha_49')), 'f49': CSQuantiles(LAST('alpha_50')), 'f50': CSQuantiles(LAST('alpha_51')),\n",
    "#     'f51': CSQuantiles(LAST('alpha_52')), 'f52': CSQuantiles(LAST('alpha_53')), 'f53': CSQuantiles(LAST('alpha_54')),\n",
    "#     'f54': CSQuantiles(LAST('alpha_55')), 'f55': CSQuantiles(LAST('alpha_56')), 'f56': CSQuantiles(LAST('alpha_57')),\n",
    "#     'f57': CSQuantiles(LAST('alpha_58')), 'f58': CSQuantiles(LAST('alpha_59')), 'f59': CSQuantiles(LAST('alpha_60')),\n",
    "#     'f60': CSQuantiles(LAST('alpha_61')), 'f61': CSQuantiles(LAST('alpha_62')), 'f62': CSQuantiles(LAST('alpha_63')),\n",
    "#     'f63': CSQuantiles(LAST('alpha_64')), 'f64': CSQuantiles(LAST('alpha_65')), 'f65': CSQuantiles(LAST('alpha_66')),\n",
    "#     'f66': CSQuantiles(LAST('alpha_67')), 'f67': CSQuantiles(LAST('alpha_68')), 'f68': CSQuantiles(LAST('alpha_69')),\n",
    "#     'f69': CSQuantiles(LAST('alpha_70')), 'f70': CSQuantiles(LAST('alpha_71')), 'f71': CSQuantiles(LAST('alpha_72')),\n",
    "#     'f72': CSQuantiles(LAST('alpha_73')), 'f73': CSQuantiles(LAST('alpha_74')), 'f74': CSQuantiles(LAST('alpha_75')),\n",
    "    \n",
    "# }\n",
    "\n",
    "factors_store = {\n",
    "    'f0': CSQuantiles(LAST('alpha_1')),\n",
    "    'f1': CSQuantiles(LAST('alpha_2')),\n",
    "    'f2': CSQuantiles(LAST('alpha_3')),\n",
    "    'f3': CSQuantiles(LAST('alpha_4')),\n",
    "    'f4': CSQuantiles(LAST('alpha_5')),\n",
    "    'f5': CSQuantiles(LAST('alpha_6')),\n",
    "    'f6': CSQuantiles(LAST('alpha_7')),\n",
    "    'f7': CSQuantiles(LAST('alpha_8')),\n",
    "    'f8': CSQuantiles(LAST('alpha_9')),\n",
    "    'f9': CSQuantiles(LAST('alpha_10'))}\n",
    "\n",
    "factor_data_org = engine.fetch_factor_range(universe, factors_store, dates=ref_dates, used_factor_tables=[Alpha191])\n",
    "factors = list(factors_store.keys())\n",
    "factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "industry = engine.fetch_industry_range(universe, dates=ref_dates)\n",
    "factor_data = pd.merge(factor_data_org, industry, on=['trade_date', 'code']).fillna(0.)\n",
    "risk_total = engine.fetch_risk_model_range(universe, dates=ref_dates)[1]\n",
    "len(factor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "return_data = engine.fetch_dx_return_range(universe, dates=ref_dates, horizon=horizon, offset=0,benchmark = benchmark_code)\n",
    "len(return_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "benchmark_total = engine.fetch_benchmark_range(dates=ref_dates, benchmark=benchmark_code)\n",
    "industry_total = engine.fetch_industry_matrix_range(universe, dates=ref_dates, category=industry_name, level=industry_level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Constraintes settings\n",
    "weight_gap = 1\n",
    "\n",
    "industry_names = industry_list(industry_name, industry_level)\n",
    "constraint_risk = ['EARNYILD', 'LIQUIDTY', 'GROWTH', 'SIZE', 'BETA', 'MOMENTUM'] + industry_names\n",
    "total_risk_names = constraint_risk + ['benchmark', 'total']\n",
    "\n",
    "b_type = []\n",
    "l_val = []\n",
    "u_val = []\n",
    "\n",
    "previous_pos = pd.DataFrame()\n",
    "rets = []\n",
    "turn_overs = []\n",
    "leverags = []\n",
    "trade_dates = []\n",
    "\n",
    "transact_cost = 0.003\n",
    "current_pos = pd.DataFrame()\n",
    "executor = NaiveExecutor()\n",
    "net_rets = []\n",
    "\n",
    "\n",
    "for name in total_risk_names:\n",
    "    if name == 'benchmark':\n",
    "        b_type.append(BoundaryType.RELATIVE)\n",
    "        l_val.append(0.0)\n",
    "        u_val.append(1.0)\n",
    "    elif name == 'total':\n",
    "        b_type.append(BoundaryType.ABSOLUTE)\n",
    "        l_val.append(.0)\n",
    "        u_val.append(.0)\n",
    "    else:\n",
    "        b_type.append(BoundaryType.ABSOLUTE)\n",
    "        l_val.append(-0.005)\n",
    "        u_val.append(0.005)\n",
    "\n",
    "bounds = create_box_bounds(total_risk_names, b_type, l_val, u_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.merge(factor_data, return_data, on=['trade_date', 'code']).dropna()\n",
    "# train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13',\n",
    "#             'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26',\n",
    "#             'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39',\n",
    "#             'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52',\n",
    "#             'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65',\n",
    "#             'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74']\n",
    "\n",
    "features = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9']\n",
    "label = ['dx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from datetime import datetime, timedelta\n",
    "from m1_xgb import *\n",
    "from src.conf.configuration import regress_conf\n",
    "import xgboost as xgb\n",
    "\n",
    "# take ref_dates[i] as an example\n",
    "for i, date_inde in enumerate(ref_dates):\n",
    "    alpha_logger.info('{0} is start'.format(date_inde))\n",
    "    # Machine learning model\n",
    "    trade_date_pre = date_inde - timedelta(days=1)\n",
    "    trade_date_pre_220 = date_inde - timedelta(days=220)\n",
    "    # train_x = factor_data[factor_data.trade_date <= trade_date_pre]\n",
    "    # train_y_excess = return_data[return_data.trade_date <= trade_date_pre]\n",
    "    # train = pd.merge(train_x, train_y_excess, on=['trade_date', 'code'])\n",
    "    # if len(train_x) <= 0 or len(train_y_excess)<=0:\n",
    "    #     continue\n",
    "    # print('len_train_x: %s, len_train_y: %s' %(len(train_x), len(train_y_excess)))\n",
    "    \n",
    "    # Train data\n",
    "    train = train_data[(train_data.trade_date <= trade_date_pre) & (trade_date_pre_220 <= train_data.trade_date)]\n",
    "    # train = train_data[train_data.trade_date <= trade_date_pre]\n",
    "    if len(train) <= 0:\n",
    "        continue\n",
    "    x_train = train[features]\n",
    "    y_train = train[label]\n",
    "    alpha_logger.info('len_x_train: {0}, len_y_train: {1}'.format(len(x_train.values), len(y_train.values)))\n",
    "    alpha_logger.info('X_train.shape={0}, X_test.shape = {1}'.format(np.shape(x_train), np.shape(y_train)))\n",
    "    \n",
    "    # Train\n",
    "    # xgb_setting\n",
    "    regress_conf.xgb_config_r()\n",
    "    regress_conf.cv_folds = None\n",
    "    regress_conf.early_stop_round = 10\n",
    "    regress_conf.max_round = 500\n",
    "    # Training\n",
    "    tic = time.time()\n",
    "    alpha_logger.info('X_train.shape={}, X_test.shape = {}'.format(np.shape(x_train), np.shape(y_train)))\n",
    "    xgb_model = XGBooster(regress_conf)\n",
    "    best_score, best_round, cv_rounds, best_model = xgb_model.fit(x_train, y_train)\n",
    "    alpha_logger.info('Training time cost {}s'.format(time.time() - tic))\n",
    "    alpha_logger.info('best_score = {}, best_round = {}'.format(best_score, best_round))\n",
    "    \n",
    "    # Test\n",
    "    # test_x = factor_data[factor_data.trade_date == date_inde]\n",
    "    # test_y_excess = return_data[return_data.trade_date == date_inde]   \n",
    "    # total_data_test_excess = pd.merge(test_x, test_y_excess, on=['trade_date', 'code']).dropna()\n",
    "    total_data_test_excess = train_data[train_data.trade_date == date_inde]\n",
    "    total_data_test_excess_pre = total_data_test_excess\n",
    "    if len(total_data_test_excess) <= 0:\n",
    "        continue\n",
    "\n",
    "    industry_matrix = industry_total[industry_total.trade_date == date_inde]\n",
    "    benchmark_w = benchmark_total[benchmark_total.trade_date == date_inde]\n",
    "    risk_matrix = risk_total[risk_total.trade_date == date_inde]\n",
    "\n",
    "    total_data = pd.merge(industry_matrix, benchmark_w, on=['code'], how='left').fillna(0.)\n",
    "    total_data = pd.merge(total_data, risk_matrix, on=['code'])\n",
    "    alpha_logger.info('{0} len_total_data: {1}'.format(date_inde, len(total_data)))\n",
    "\n",
    "    total_data_test_excess = pd.merge(total_data, total_data_test_excess, on=['code'])\n",
    "    alpha_logger.info('{0} len_total_data_test_excess: {1}'.format(date_inde, len(total_data_test_excess)))\n",
    "    \n",
    "    if len(total_data_test_excess_pre) != len(total_data_test_excess):\n",
    "        print('total_data_test_exces_pre: %s' % len(total_data_test_excess_pre))\n",
    "        print('total_data_test_excess: %s' % len(total_data_test_excess))\n",
    "        \n",
    "    codes = total_data_test_excess.code.values.tolist()\n",
    "    alpha_logger.info('{0} full re-balance: {1}'.format(date_inde, len(codes)))\n",
    "    dx_returns = return_data[return_data.trade_date == date_inde][['code', 'dx']]\n",
    "\n",
    "    benchmark_w = total_data_test_excess.weight.values\n",
    "    alpha_logger.info('shape_of_benchmark_w: {}'.format(np.shape(benchmark_w)))\n",
    "    is_in_benchmark = (benchmark_w > 0.).astype(float).reshape((-1, 1))\n",
    "    total_risk_exp = np.concatenate([total_data_test_excess[constraint_risk].values.astype(float),\n",
    "                                     is_in_benchmark,\n",
    "                                     np.ones_like(is_in_benchmark)],\n",
    "                                     axis=1)\n",
    "    total_risk_exp = pd.DataFrame(total_risk_exp, columns=total_risk_names)\n",
    "    alpha_logger.info('shape_of_total_risk_exp: {}'.format(np.shape(total_risk_exp)))\n",
    "    constraints = LinearConstraints(bounds, total_risk_exp, benchmark_w)\n",
    "    alpha_logger.info('shape_of_constraints: {}'.format(np.shape(constraints.risk_targets())))\n",
    "\n",
    "    lbound = np.maximum(0., benchmark_w - weight_gap)\n",
    "    ubound = weight_gap + benchmark_w\n",
    "    alpha_logger.info('lbound: \\n{}'.format(lbound))\n",
    "    alpha_logger.info('ubound: \\n{}'.format(ubound))\n",
    "    \n",
    "    # predict\n",
    "    x_pred = total_data_test_excess[features]\n",
    "    dpred = xgb.DMatrix(x_pred.values)\n",
    "    predict_xgboost = best_model.predict(dpred)\n",
    "    a = np.shape(predict_xgboost)\n",
    "    predict_xgboost = np.reshape(predict_xgboost, (a[0], -1)).astype(np.float64)\n",
    "    alpha_logger.info('shape_of_predict_xgboost: {}'.format(np.shape(predict_xgboost)))\n",
    "\n",
    "\n",
    "    \n",
    "    # backtest\n",
    "    try:\n",
    "        target_pos, _ = er_portfolio_analysis(predict_xgboost,\n",
    "                                              total_data_test_excess['industry'].values,\n",
    "                                              None,\n",
    "                                              constraints,\n",
    "                                              False,\n",
    "                                              benchmark_w,\n",
    "                                              method = 'risk_neutral',\n",
    "                                              lbound=lbound,\n",
    "                                              ubound=ubound)\n",
    "    except:\n",
    "        import ipdb\n",
    "        ipdb.set_trace()\n",
    "#     target_pos, _ = er_portfolio_analysis(predict_xgboost,\n",
    "#                                           total_data_test_excess['industry'].values,\n",
    "#                                           None,\n",
    "#                                           None,\n",
    "#                                           False,\n",
    "#                                           None,\n",
    "#                                           method = 'ls')\n",
    "    alpha_logger.info('target_pos: {}'.format(np.shape(target_pos)))\n",
    "    target_pos['code'] = codes\n",
    "    \n",
    "    result = pd.merge(target_pos, dx_returns, on=['code'])\n",
    "    ret = result.weight.values @ (np.exp(result.dx.values) - 1.)\n",
    "    \n",
    "    trade_dates.append(date_inde)\n",
    "    rets.append(np.log(1. + ret))\n",
    "    \n",
    "    turn_over_org, current_pos = executor.execute(target_pos=target_pos)\n",
    "    turn_over = turn_over_org / sum(target_pos.weight.values)\n",
    "    executor.set_current(current_pos)\n",
    "    net_rets.append(np.log(1. + ret - transact_cost * turn_over))\n",
    "    \n",
    "    alpha_logger.info('{0} is finished'.format(date_inde))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(rets), len(trade_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_df = pd.DataFrame({'xgb_regress': rets, 'net_xgb_regress':net_rets}, index=trade_dates)\n",
    "ret_df.loc[advanceDateByCalendar('china.sse', ref_dates[-1], freq).strftime('%Y-%m-%d')] = 0.\n",
    "ret_df = ret_df.shift(1)\n",
    "ret_df.iloc[0] = 0.\n",
    "\n",
    "ret_df[['xgb_regress', 'net_xgb_regress']].cumsum().plot(figsize=(12, 6), \n",
    "                                                         title='Fixed freq rebalanced: {0}'.format(freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_df[['xgb_regress']].cumsum().plot(figsize=(12, 6),\n",
    "                                      title='Fixed freq rebalanced: {0}'.format(freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
