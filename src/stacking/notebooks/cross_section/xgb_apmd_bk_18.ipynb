{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础因子加alpha191回测结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加费后曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../../')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from PyFin.api import *\n",
    "from alphamind.api import *\n",
    "from conf.models import *\n",
    "from conf.config import *\n",
    "from alphamind.execution.naiveexecutor import NaiveExecutor\n",
    "from stacking import factor_store, feature_list\n",
    "\n",
    "data_source = alpha_db\n",
    "engine = SqlEngine(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "universe = Universe('zz500')\n",
    "freq = '5b'\n",
    "benchmark_code = 905\n",
    "# 回测时注意start_date和back_start_date的时间设置, 保证是同一时间间隔的数据\n",
    "# start_date = '2010-01-08'  # 训练集的起始时间\n",
    "start_date = '2018-01-01'  # 训练集的起始时间\n",
    "\n",
    "back_start_date = '2018-01-01'  # 回测的起始时间\n",
    "end_date = '2019-10-01'\n",
    "ref_dates = makeSchedule(start_date, end_date, freq, 'china.sse')\n",
    "back_ref_dates = makeSchedule(back_start_date, end_date, freq, 'china.sse')\n",
    "horizon = map_freq(freq)\n",
    "industry_name = 'sw'\n",
    "industry_level = 1\n",
    "back_ref_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ref_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uqer因子列表\n",
    "basic_factor_store = factor_store.basic_factor_store\n",
    "# alpha191因子列表\n",
    "# alpha_factor_store = factor_store.alpha_factor_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 提取Uqer因子\n",
    "basic_factor_org = engine.fetch_factor_range(universe, basic_factor_store, dates=ref_dates)\n",
    "# 提取alpha191因子\n",
    "# alpha191_factor_org = engine.fetch_factor_range(universe, alpha_factor_store, dates=ref_dates, used_factor_tables=[Alpha191])\n",
    "# 合并所有的因子\n",
    "# factor_data_org = pd.merge(basic_factor_org, alpha191_factor_org, on=['trade_date', 'code'], how='outer')\n",
    "factor_data_org = basic_factor_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 获取\n",
    "industry = engine.fetch_industry_range(universe, dates=ref_dates)\n",
    "factor_data = pd.merge(factor_data_org, industry, on=['trade_date', 'code']).fillna(0.)\n",
    "risk_total = engine.fetch_risk_model_range(universe, dates=ref_dates)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "return_data = engine.fetch_dx_return_range(universe, dates=ref_dates, \n",
    "                                           horizon=horizon, offset=0,\n",
    "                                           benchmark = benchmark_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.merge(factor_data_org, return_data, on=['trade_date', 'code']).dropna()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看因子间相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.heatmap(train_data.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "benchmark_total = engine.fetch_benchmark_range(dates=ref_dates, benchmark=benchmark_code)\n",
    "industry_total = engine.fetch_industry_matrix_range(universe, dates=ref_dates, category=industry_name, level=industry_level)\n",
    "\n",
    "train_data = pd.merge(factor_data, return_data, on=['trade_date', 'code']).dropna()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraintes settings\n",
    "\n",
    "industry_names = industry_list(industry_name, industry_level)\n",
    "constraint_risk = ['EARNYILD', 'LIQUIDTY', 'GROWTH', 'SIZE', 'SIZENL', 'BETA', 'MOMENTUM'] + industry_names\n",
    "\n",
    "total_risk_names = constraint_risk + ['benchmark', 'total']\n",
    "\n",
    "b_type = []\n",
    "l_val = []\n",
    "u_val = []\n",
    "\n",
    "for name in total_risk_names:\n",
    "    if name == 'benchmark':\n",
    "        b_type.append(BoundaryType.RELATIVE)\n",
    "        l_val.append(0.0)\n",
    "        u_val.append(1.0)\n",
    "    elif name == 'total':\n",
    "        b_type.append(BoundaryType.ABSOLUTE)\n",
    "        l_val.append(-0.0)\n",
    "        u_val.append(0.0)\n",
    "    elif name == 'SIZE':\n",
    "        b_type.append(BoundaryType.ABSOLUTE)\n",
    "        l_val.append(-1.0)\n",
    "        u_val.append(1.0)\n",
    "    elif name == 'SIZENL':\n",
    "        b_type.append(BoundaryType.ABSOLUTE)\n",
    "        l_val.append(-1.0)\n",
    "        u_val.append(1.0)\n",
    "    elif name in industry_names:\n",
    "        b_type.append(BoundaryType.ABSOLUTE)\n",
    "        l_val.append(-0.005)\n",
    "        u_val.append(0.005)\n",
    "    else:\n",
    "        b_type.append(BoundaryType.ABSOLUTE)\n",
    "        l_val.append(-2.0)\n",
    "        u_val.append(2.0)\n",
    "\n",
    "# for name in total_risk_names:\n",
    "#     if name == 'benchmark':\n",
    "#         b_type.append(BoundaryType.RELATIVE)\n",
    "#         l_val.append(0.0)\n",
    "#         u_val.append(1.0)\n",
    "#     elif name == 'total':\n",
    "#         b_type.append(BoundaryType.ABSOLUTE)\n",
    "#         l_val.append(.0)\n",
    "#         u_val.append(.0)\n",
    "#     else:\n",
    "#         b_type.append(BoundaryType.ABSOLUTE)\n",
    "#         l_val.append(-2.0)\n",
    "#         u_val.append(2.0)\n",
    "\n",
    "bounds = create_box_bounds(total_risk_names, b_type, l_val, u_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取特征名\n",
    "features = list(basic_factor_store.keys())\n",
    "# alpha_features = list(alpha_factor_store.keys())\n",
    "# features.extend(alpha_features)\n",
    "\n",
    "label = ['dx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from datetime import datetime, timedelta\n",
    "from models.m1_xgb import *\n",
    "from conf.configuration import xgb_conf\n",
    "import xgboost as xgb\n",
    "import gc\n",
    "\n",
    "\n",
    "def create_scenario():\n",
    "    weight_gap = 0.02\n",
    "    transact_cost = 0.003\n",
    "\n",
    "    executor = NaiveExecutor()\n",
    "    leverags = []\n",
    "    trade_dates = []\n",
    "    current_pos = pd.DataFrame({'trade_date':[], 'weight':[],'industry':[], 'er':[],'code':[]})\n",
    "    tune_record = pd.DataFrame()  \n",
    "    rets = []\n",
    "    net_rets = []\n",
    "    turn_overs = []\n",
    "    leverags = []\n",
    "    ics = []\n",
    "\n",
    "    # take ref_dates[i] as an example\n",
    "    for i, ref_date in enumerate(back_ref_dates):\n",
    "        alpha_logger.info('{0} is start'.format(ref_date))\n",
    "\n",
    "        # machine learning model\n",
    "        # Filter Training data\n",
    "        # train data\n",
    "        trade_date_pre = ref_date - timedelta(days=1)\n",
    "        trade_date_pre_80 = ref_date - timedelta(days=100)\n",
    "        # train = train_data[(train_data.trade_date <= trade_date_pre) & (trade_date_pre_80 <= train_data.trade_date)].dropna()\n",
    "        # 训练集构造, 选择当天之前(不含当天)的因子数据作为训练集.\n",
    "        train = train_data[train_data.trade_date <= trade_date_pre]\n",
    "\n",
    "        if len(train) <= 0:\n",
    "            continue\n",
    "        x_train = train[features]\n",
    "        y_train = train[label]\n",
    "        alpha_logger.info('len_x_train: {0}, len_y_train: {1}'.format(len(x_train.values), len(y_train.values)))\n",
    "        alpha_logger.info('X_train.shape={0}, X_test.shape = {1}'.format(np.shape(x_train), np.shape(y_train)))\n",
    "\n",
    "        # xgb_configuration\n",
    "        xgb_conf.xgb_config_r()\n",
    "        xgb_conf.cv_folds = None\n",
    "        xgb_conf.early_stop_round = 100\n",
    "        xgb_conf.max_round = 800\n",
    "        xgb_conf.params.update({'nthread': 12})\n",
    "\n",
    "        tic = time.time()\n",
    "        # training\n",
    "        xgb_model = XGBooster(xgb_conf)\n",
    "        GPU_device = False\n",
    "\n",
    "        if GPU_device:\n",
    "            xgb_model.set_params(tree_method='gpu_hist', max_depth=5)\n",
    "        else:\n",
    "            xgb_model.set_params(max_depth=5)\n",
    "        alpha_logger.info('train params {}s'.format(xgb_model.get_params))\n",
    "        \n",
    "        best_score, best_round, best_model = xgb_model.fit(x_train, y_train)\n",
    "        alpha_logger.info('Training time cost {}s'.format(time.time() - tic))\n",
    "        alpha_logger.info('best_score = {}, best_round = {}'.format(best_score, best_round))\n",
    "    \n",
    "        # 测试集, 取当天的因子数据作为输入.\n",
    "        total_data_test_excess = train_data[train_data.trade_date == ref_date]\n",
    "        alpha_logger.info('{0} total_data_test_excess: {1}'.format(ref_date, len(total_data_test_excess)))\n",
    "\n",
    "        if len(total_data_test_excess) <= 0:\n",
    "            alpha_logger.info('{0} HAS NO DATA!!!'.format(ref_date))\n",
    "            continue\n",
    "        \n",
    "        # 获取当天的行业, 风险模型和基准数据\n",
    "        industry_matrix = industry_total[industry_total.trade_date == ref_date]\n",
    "        benchmark_w = benchmark_total[benchmark_total.trade_date == ref_date]\n",
    "        risk_matrix = risk_total[risk_total.trade_date == ref_date]\n",
    "\n",
    "        total_data = pd.merge(industry_matrix, benchmark_w, on=['code'], how='left').fillna(0.)\n",
    "        total_data = pd.merge(total_data, risk_matrix, on=['code'])\n",
    "        alpha_logger.info('{0} len_of_total_data: {1}'.format(ref_date, len(total_data)))\n",
    "\n",
    "        total_data_test_excess = pd.merge(total_data, total_data_test_excess, on=['code'])\n",
    "        alpha_logger.info('{0} len_of_total_data_test_excess: {1}'.format(ref_date, len(total_data_test_excess)))\n",
    "        \n",
    "        codes = total_data_test_excess.code.values.tolist()\n",
    "        alpha_logger.info('{0} full re-balance: {1}'.format(ref_date, len(codes)))\n",
    "        ## 获取调仓日当天的股票收益\n",
    "        dx_returns = return_data[return_data.trade_date == ref_date][['code', 'dx']]\n",
    "\n",
    "\n",
    "        benchmark_w = total_data_test_excess.weight.values\n",
    "        alpha_logger.info('shape_of_benchmark_w: {}'.format(np.shape(benchmark_w)))\n",
    "        is_in_benchmark = (benchmark_w > 0.).astype(float).reshape((-1, 1))\n",
    "        total_risk_exp = np.concatenate([total_data_test_excess[constraint_risk].values.astype(float),\n",
    "                                         is_in_benchmark,\n",
    "                                         np.ones_like(is_in_benchmark)],\n",
    "                                         axis=1)\n",
    "        alpha_logger.info('shape_of_total_risk_exp_pre: {}'.format(np.shape(total_risk_exp)))\n",
    "        total_risk_exp = pd.DataFrame(total_risk_exp, columns=total_risk_names)\n",
    "        alpha_logger.info('shape_of_total_risk_exp: {}'.format(np.shape(total_risk_exp)))\n",
    "        constraints = LinearConstraints(bounds, total_risk_exp, benchmark_w)\n",
    "        alpha_logger.info('constraints: {0} in {1}'.format(np.shape(constraints.risk_targets()), ref_date))\n",
    "\n",
    "        lbound = np.maximum(0., benchmark_w - weight_gap)\n",
    "        ubound = weight_gap + benchmark_w\n",
    "        alpha_logger.info('lbound: {0} in {1}'.format(np.shape(lbound), ref_date))\n",
    "        alpha_logger.info('ubound: {0} in {1}'.format(np.shape(ubound), ref_date))\n",
    "\n",
    "        # predict\n",
    "        x_pred = total_data_test_excess[features]\n",
    "        predict_xgboost = xgb_model.predict(best_model, x_pred)\n",
    "        a = np.shape(predict_xgboost)\n",
    "        predict_xgboost = np.reshape(predict_xgboost, (a[0], -1)).astype(np.float64)\n",
    "        alpha_logger.info('shape_of_predict_xgboost: {}'.format(np.shape(predict_xgboost)))\n",
    "        alpha_logger.info('type_of_predict_xgboost: {}'.format(type(predict_xgboost)))\n",
    "\n",
    "        del xgb_model\n",
    "        del best_model\n",
    "        gc.collect()\n",
    "        \n",
    "        # 股票过滤, 组合优化之前过滤掉\n",
    "        \n",
    "        \n",
    "        # 导入昨持仓并与当前股票池中所有股票合并, \n",
    "        if len(current_pos) <= 0:\n",
    "            current_position = None\n",
    "        else:\n",
    "            # 保证当前持仓和昨持仓的股票顺序一致\n",
    "            current_pos = total_data_test_excess[['code']].merge(current_pos, on=['code'], how='left',).fillna(0)\n",
    "            current_position = current_pos.weight.values\n",
    "        alpha_logger.info('current_pos:\\n {}'.format(current_pos.head()))\n",
    "        alpha_logger.info('len_of_current_pos:\\n {}'.format(len(current_pos)))\n",
    "\n",
    "        \n",
    "        # backtest\n",
    "        try:\n",
    "            target_pos, _ = er_portfolio_analysis(predict_xgboost,\n",
    "                                                  total_data_test_excess['industry'].values,\n",
    "                                                  None,\n",
    "                                                  constraints,\n",
    "                                                  False,\n",
    "                                                  benchmark_w,\n",
    "                                                  method='risk_neutral',\n",
    "                                                  lbound=lbound,\n",
    "                                                  ubound=ubound,\n",
    "                                                  turn_over_target=0.1,\n",
    "                                                  current_position=current_position)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        # alpha_logger.info('target_pos: \\n{}'.format(target_pos))\n",
    "        alpha_logger.info('target_pos_shape: {}'.format(np.shape(target_pos)))\n",
    "        alpha_logger.info('len_codes:{}'.format(np.shape(codes)))\n",
    "        target_pos['code'] = codes\n",
    "        \n",
    "        result = pd.merge(target_pos, dx_returns, on=['code'])\n",
    "        result['trade_date'] = ref_date\n",
    "        tune_record = tune_record.append(result)\n",
    "        alpha_logger.info('len_result: {}'.format(len(result)))\n",
    "        ## 详细选股可以要求进行筛选, 对tune_record进行限制,过滤\n",
    "        \n",
    "        \n",
    "\n",
    "        # excess_return = np.exp(result.dx.values) - 1. - index_return.loc[ref_date, 'dx']\n",
    "        excess_return = np.exp(result.dx.values) - 1.\n",
    "        ret = result.weight.values @ excess_return\n",
    "\n",
    "        trade_dates.append(ref_date)\n",
    "        rets.append(np.log(1. + ret))\n",
    "        alpha_logger.info('len_rets: {}, len_trade_dates: {}'.format(len(rets), len(trade_dates)))\n",
    "        \n",
    "        executor.set_current(current_pos)\n",
    "        turn_over_org, current_pos = executor.execute(target_pos=target_pos)\n",
    "        alpha_logger.info('turn_over_org: {}'.format(turn_over_org))\n",
    "\n",
    "        turn_over = turn_over_org / sum(target_pos.weight.values)\n",
    "        alpha_logger.info('turn_over: {}'.format(turn_over))\n",
    "        turn_overs.append(turn_over)\n",
    "        \n",
    "        net_rets.append(np.log(1. + ret - transact_cost * turn_over))        \n",
    "        alpha_logger.info('len_net_rets: {}, len_trade_dates: {}'.format(len(net_rets), len(trade_dates)))\n",
    "        alpha_logger.info('{} is finished'.format(ref_date))\n",
    "    # ret_df = pd.DataFrame({'xgb_regress': rets}, index=trade_dates)\n",
    "    ret_df = pd.DataFrame({'xgb_regress': rets, 'net_xgb_regress':net_rets}, index=trade_dates)\n",
    "    ret_df.loc[advanceDateByCalendar('china.sse', ref_dates[-1], freq).strftime('%Y-%m-%d')] = 0.\n",
    "    ret_df = ret_df.shift(1)\n",
    "    ret_df.iloc[0] = 0.\n",
    "    return ret_df, tune_record, rets, net_rets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 滚动回测\n",
    "ret_df, tune_record, rets, net_rets = create_scenario()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_record.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyecharts import options as opts\n",
    "from example.commons import Collector\n",
    "from pyecharts.charts import Line\n",
    "\n",
    "plot = ret_df[['xgb_regress', 'net_xgb_regress']].cumsum()\n",
    "v1 = list(plot.index)\n",
    "v2 = list(plot.xgb_regress)\n",
    "v3 = list(plot.net_xgb_regress)\n",
    "\n",
    "line_chart = (\n",
    "    Line()\n",
    "    .add_xaxis(v1)\n",
    "    .add_yaxis(\"xgb_regress\", v2)\n",
    "    .add_yaxis(\"net_xgb_regress\", v3)\n",
    "    .set_series_opts(\n",
    "        label_opts=opts.LabelOpts(is_show=False),\n",
    "    )\n",
    "    .set_global_opts(\n",
    "        xaxis_opts=opts.AxisOpts(is_scale=True),\n",
    "        yaxis_opts=opts.AxisOpts(\n",
    "            is_scale=True,\n",
    "            splitarea_opts=opts.SplitAreaOpts(\n",
    "                is_show=True, areastyle_opts=opts.AreaStyleOpts(opacity=1)\n",
    "            ),\n",
    "        ),\n",
    "        datazoom_opts=[opts.DataZoomOpts(pos_bottom=\"-1%\")],\n",
    "        title_opts=opts.TitleOpts(title='Fixed freq rebalanced: {0}'.format(freq)),\n",
    "    )\n",
    ")\n",
    "\n",
    "line_chart.render_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 调仓记录保存\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "con = sqlite3.connect('./tune_record_real.db')\n",
    "tune_record.to_sql('tune_record', con=con, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = 'select * from tune_record'\n",
    "tune_record = pd.read_sql(sql, con)\n",
    "tune_record[['trade_date','code','portfolio_weight']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ##--------------因子组合回测--------------\n",
    "import json\n",
    "import pdb\n",
    "from rqalpha.api import *\n",
    "from rqalpha import run_func\n",
    "class DailyDefaultStrategy(object):\n",
    "    def __init__(self, session, stock_sets_ob):\n",
    "        self._stock_sets_ob = json.loads(stock_sets_ob)\n",
    "        self._session = session\n",
    "        self._start_time_str = datetime.now().strftime('%m-%d %H:%M')\n",
    "    \n",
    "    \n",
    "    def init(self,context):\n",
    "        #读取股票池文件\n",
    "        context.stock_sets = pd.DataFrame(self._stock_sets_ob)\n",
    "        context.trade_date_list= list(set(context.stock_sets.trade_date))\n",
    "        context.holding_stock_df = None\n",
    "        \n",
    "        # 是否已发送了order\n",
    "        context.fired = False\n",
    "\n",
    "    #盘前处理\n",
    "    def before_trading(self, context, bar_dict):\n",
    "        context.trade_signal = False\n",
    "        date = context.now.date()\n",
    "        if str(date) in context.trade_date_list:\n",
    "            context.trade_signal = True\n",
    "\n",
    "    #盘后处理\n",
    "    def after_trading(self, context):\n",
    "        pass\n",
    "    \n",
    "    # 你选择的证券的数据更新将会触发此段逻辑，例如日或分钟历史数据切片或者是实时数据切片更新\n",
    "    def handle_bar(self, context, bar_dict):\n",
    "        # 开始编写你的主要的算法逻辑\n",
    "        if context.trade_signal == False:\n",
    "            return\n",
    "        date = context.now.date()\n",
    "        # stock_dict = context.stock_sets.set_index('trade_date').loc[str(date)]\n",
    "        stock_set = context.stock_sets\n",
    "        stock_dict = stock_set[stock_set['trade_date'] == str(date)].set_index('trade_date')\n",
    "        stock_df = self.filter_specials(stock_dict, context)\n",
    "        #剔除后的股票\n",
    "        if 'portfolio_weight' in list(set(stock_df.columns)):\n",
    "            self.rebalance_weight(context, stock_df)\n",
    "        else:\n",
    "            self.rebalance_equal(context, stock_df)\n",
    "   \n",
    "    def _industry_distribute(self, stock_positions, industries):\n",
    "        industry_sets_dict = {}\n",
    "        industries = industries.rename(columns={'symbol':'code'})\n",
    "        stock_positions = stock_positions.reset_index().rename(\n",
    "            columns={'symbol':'name','order_book_id':'code','date':'trade_date'})\n",
    "        stock_positions['trade_date'] = stock_positions['trade_date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "        industries['trade_date'] = industries['trade_date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "        stock_positions = stock_positions.merge(industries, on=['trade_date','code'])\n",
    "        industry_grouped = stock_positions.dropna().groupby(['trade_date'])\n",
    "        for k, g in industry_grouped:\n",
    "            gt = g.groupby(['industry_code','industry']).count().reset_index()[['industry_code','industry','code']]\n",
    "            gt['ratio'] = gt['code']/gt['code'].values.sum()\n",
    "            gt=gt.rename(columns={'industry':'industry_name'})\n",
    "            industry_sets_dict[k] = gt[['industry_code','industry_name','ratio']].to_dict(orient='records')\n",
    "        return industry_sets_dict\n",
    "    \n",
    "    def _month_profit(self, portfolio, stock_capital):\n",
    "        pd.DataFrame(portfolio.reset_index()).shift(1)\n",
    "        month_profit = portfolio['total_value'].resample('M').mean()\n",
    "        month_profit = pd.DataFrame(month_profit.reset_index())\n",
    "        month_profit['last_total'] = month_profit['total_value'].shift(1)\n",
    "        month_profit = month_profit.fillna(stock_capital)\n",
    "        month_profit['month_ratio'] = np.log(month_profit.total_value / month_profit.last_total)\n",
    "        return month_profit.rename(columns={'date':'trade_date'})\n",
    "    \n",
    "    def set_result(self, result):\n",
    "        #summary 结算信息\n",
    "        summary = result['sys_analyser']['summary']\n",
    "        #trades 交易记录\n",
    "        trades = result['sys_analyser']['trades']\n",
    "        #portfolio 收益曲线\n",
    "        portfolio = result['sys_analyser']['portfolio']\n",
    "        # benchmark_portfolio 基础收益曲线benchmark_portfolio\n",
    "        benchmark_portfolio = result['sys_analyser']['benchmark_portfolio']\n",
    "        # stock_account 个人收益\n",
    "        stock_account = result['sys_analyser']['stock_account']\n",
    "        # stock_positions 股票持仓情况\n",
    "        stock_positions = result['sys_analyser']['stock_positions']\n",
    "        \n",
    "        \n",
    "    def filter_specials(self, stock_dict, context):\n",
    "        stock_df = pd.DataFrame(stock_dict)\n",
    "        stock_list = list(stock_df.code)\n",
    "        stock_list=[stock for stock in stock_list]\n",
    "        \n",
    "        return stock_df.set_index('code').loc[stock_list,:] \n",
    "    \n",
    "    def rebalance_equal(self, context, stock_df):\n",
    "        holding_list = list(set(stock_df.index))\n",
    "        if len(holding_list) > 0:\n",
    "            every_stock = context.portfolio.portfolio_value/len(holding_list) \n",
    "        # 空仓只有买入操作\n",
    "        if len(list(context.portfolio.positions.keys()))==0:\n",
    "            for stock_to_buy in list(holding_list):\n",
    "                #print(stock_to_buy)\n",
    "                order_target_value(stock_to_buy,every_stock)\n",
    "        else:\n",
    "            for stock_to_sell in list(context.portfolio.positions.keys()):\n",
    "                if stock_to_sell not in list(holding_list):\n",
    "                    order_target_value(stock_to_sell, 0)\n",
    "            for stock_to_buy in list(holding_list):\n",
    "                order_target_value(stock_to_buy, every_stock)\n",
    "                \n",
    "        \n",
    "    def rebalance_weight(self, context, stock_df):\n",
    "        #没有持仓则全部下单\n",
    "        if len(list(context.portfolio.positions.keys())) == 0:\n",
    "            weight_sum = stock_df.portfolio_weight.sum()\n",
    "            every_values = context.portfolio.total_value / weight_sum\n",
    "            for index, weight in stock_df.iterrows():\n",
    "                order_target_value(index, weight.portfolio_weight * every_values)\n",
    "            context.holding_stock_df = stock_df\n",
    "        else:\n",
    "            now_df = context.holding_stock_df #当前持有的股票\n",
    "            # 做交集查看上次实际成交股票\n",
    "            intersect = list(set(now_df.index).intersection(set(context.portfolio.stock_account.positions.keys())))\n",
    "            if len(intersect) <= 0:\n",
    "                now_df = pd.DataFrame({'now_weight':[]})\n",
    "            else:\n",
    "                now_df = now_df.loc[context.portfolio.stock_account.positions.keys()]\n",
    "                now_df = now_df.rename(columns={'portfolio_weight':'now_weight'}) #持仓\n",
    "            # 做并集\n",
    "            stock_df = stock_df.rename(columns={'portfolio_weight':'next_weight'}) #调仓\n",
    "            now_df = pd.merge(now_df, stock_df, left_index=True, right_index=True, how='outer')\n",
    "            now_df = now_df.fillna(0)\n",
    "            now_df['diff_weight'] = now_df.next_weight - now_df.now_weight #调仓股票和持仓股票进行对比\n",
    "            buy_stock = now_df[now_df['diff_weight'] > 0] #当前需买进\n",
    "            sell_stock = now_df[now_df['diff_weight'] < 0]\n",
    "            \n",
    "            # 根据当前权重分配，目的是持仓达到和调仓的权重一样\n",
    "            weight_sum = stock_df.next_weight.sum()\n",
    "            every_values = context.portfolio.total_value/ weight_sum\n",
    "            \n",
    "            # 平掉不需要股票\n",
    "            for index, weight in sell_stock.iterrows():\n",
    "                if weight.now_weight == abs(weight.diff_weight): #全部清仓\n",
    "                    order_target_value(index, 0)\n",
    "                else:# 调整权重，此处会有误差，每期的every_values不一样\n",
    "                    order_target_value(index, weight.diff_weight * every_values)\n",
    "            # 新建增仓\n",
    "            for index, weight in buy_stock.iterrows():\n",
    "                order_target_value(index, abs(weight.diff_weight) * every_values)\n",
    "            stock_df = stock_df.rename(columns={'next_weight':'portfolio_weight'})\n",
    "            context.holding_stock_df = stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ricequant_backtesting(conmbine):\n",
    "    benchmark_info = '000905.XSHG'\n",
    "    conmbine = conmbine.copy()\n",
    "    conmbine['trade_date'] = conmbine['trade_date'].apply(lambda x: x.split(' ')[0] if type(x) is str else x.date().strftime(\"%Y-%m-%d\"))\n",
    "    # stk_result = stk_data[stk_data['portfolio_weight']>0.001]#.set_index('trade_date').loc['2018-04-12']\n",
    "    conmbine['code'] = conmbine['code'].apply(lambda x: \"{:06d}\".format(x) + '.XSHG' \n",
    "                                              if len(str(x))==6 and str(x)[0] in '6' \n",
    "                                              else \"{:06d}\".format(x)+ '.XSHE')\n",
    "    backtesting = DailyDefaultStrategy(1234412, json.dumps(conmbine.to_dict(orient='records')))\n",
    "    trade_date_list = list(set(conmbine.trade_date))\n",
    "    trade_date_list.sort(reverse=False)\n",
    "    base_config = {}\n",
    "    base_config['start_date'] = trade_date_list[0]\n",
    "    base_config['end_date'] = trade_date_list[-1]\n",
    "    base_config['benchmark'] = benchmark_info\n",
    "            \n",
    "    accounts_config = {}\n",
    "    accounts_config['stock'] = 100000000\n",
    "    base_config['accounts'] = accounts_config\n",
    "    base_config['frequency'] = '1d'\n",
    "    base_config['skip_suspended'] = True\n",
    "    \n",
    "    extra_config = {}\n",
    "\n",
    "    mod_config = {}\n",
    "    sys_analyser_config = {}\n",
    "    sys_analyser_config['enabled'] = True\n",
    "    sys_analyser_config['plot'] = True\n",
    "    mod_config['sys_analyser'] = sys_analyser_config\n",
    "        \n",
    "    config = {}\n",
    "    config['base'] = base_config\n",
    "    config['extra'] = extra_config\n",
    "    config['mod'] = mod_config\n",
    "    result = run_func(init=backtesting.init, before_trading=backtesting.before_trading, \n",
    "                  after_trading=backtesting.after_trading, handle_bar=backtesting.handle_bar, config=config)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "tune_record  = tune_record.rename(columns={'weight':'portfolio_weight'})\n",
    "#处理权重小, 直接处理为0\n",
    "tune_record = tune_record[tune_record['portfolio_weight'] >= 0.05]\n",
    "result = ricequant_backtesting(tune_record[['trade_date','code','portfolio_weight']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tune_record[tune_record['trade_date'] == '2019-06-05']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回测记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.keys())\n",
    "record = result['sys_analyser']\n",
    "print(record.keys())\n",
    "trade_detail = record['trades']\n",
    "stock_account = record['stock_account']\n",
    "stock_positions = record['stock_positions']\n",
    "stock_positions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取指定交易日期的交易记录\n",
    "trade_detail[trade_detail['trading_datetime'] == '2018-11-29 15:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_detail[trade_detail['trading_datetime'] == '2018-12-06 15:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取指定交易日的持仓仓位\n",
    "stock_positions[(stock_positions.index == '2018-12-20') & (stock_positions.quantity != 0)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_positions[(stock_positions.index == '2018-12-27') & (stock_positions.quantity != 0)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_positions[(stock_positions.index == '2019-01-07') & (stock_positions.quantity != 0)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trade_detail.to_csv('./trades_record.csv', encoding=\"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
