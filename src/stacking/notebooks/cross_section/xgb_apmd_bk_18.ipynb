{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础因子加alpha191回测结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加费后曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../../')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from PyFin.api import *\n",
    "from alphamind.api import *\n",
    "from conf.models import *\n",
    "from conf.config import*\n",
    "from alphamind.execution.naiveexecutor import NaiveExecutor\n",
    "from stacking import factor_store, feature_list\n",
    "\n",
    "data_source = alpha_db\n",
    "engine = SqlEngine(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "universe = Universe('zz500')\n",
    "freq = '5b'\n",
    "benchmark_code = 905\n",
    "start_date = '2010-01-01'  # 训练集的起始时间\n",
    "back_start_date = '2018-01-01'  # 回测的起始时间\n",
    "end_date = '2019-10-01'\n",
    "ref_dates = makeSchedule(start_date, end_date, freq, 'china.sse')\n",
    "back_ref_dates = makeSchedule(back_start_date, end_date, freq, 'china.sse')\n",
    "horizon = map_freq(freq)\n",
    "industry_name = 'sw'\n",
    "industry_level = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uqer因子列表\n",
    "basic_factor_store = factor_store.basic_factor_store\n",
    "# alpha191因子列表\n",
    "# alpha_factor_store = factor_store.alpha_factor_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 9s, sys: 44.8 s, total: 13min 54s\n",
      "Wall time: 15min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 提取Uqer因子\n",
    "basic_factor_org = engine.fetch_factor_range(universe, basic_factor_store, dates=ref_dates)\n",
    "# 提取alpha191因子\n",
    "# alpha191_factor_org = engine.fetch_factor_range(universe, alpha_factor_store, dates=ref_dates, used_factor_tables=[Alpha191])\n",
    "# 合并所有的因子\n",
    "# factor_data_org = pd.merge(basic_factor_org, alpha191_factor_org, on=['trade_date', 'code'], how='outer')\n",
    "factor_data_org = basic_factor_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.97 s, sys: 556 ms, total: 8.53 s\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 获取\n",
    "industry = engine.fetch_industry_range(universe, dates=ref_dates)\n",
    "factor_data = pd.merge(factor_data_org, industry, on=['trade_date', 'code']).fillna(0.)\n",
    "risk_total = engine.fetch_risk_model_range(universe, dates=ref_dates)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.87 s, sys: 66.6 ms, total: 1.93 s\n",
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "return_data = engine.fetch_dx_return_range(universe, dates=ref_dates, \n",
    "                                           horizon=horizon, offset=0,\n",
    "                                           benchmark = benchmark_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trade_date</th>\n",
       "      <th>ACCA</th>\n",
       "      <th>ACD20</th>\n",
       "      <th>ACD6</th>\n",
       "      <th>AD</th>\n",
       "      <th>AD20</th>\n",
       "      <th>AD6</th>\n",
       "      <th>ADTM</th>\n",
       "      <th>ADX</th>\n",
       "      <th>ADXR</th>\n",
       "      <th>...</th>\n",
       "      <th>Volumn1M</th>\n",
       "      <th>Volumn3M</th>\n",
       "      <th>WVAD</th>\n",
       "      <th>WorkingCapital</th>\n",
       "      <th>minusDI</th>\n",
       "      <th>plusDI</th>\n",
       "      <th>code</th>\n",
       "      <th>chgPct</th>\n",
       "      <th>secShortName</th>\n",
       "      <th>dx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>0.253165</td>\n",
       "      <td>0.873418</td>\n",
       "      <td>0.835443</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.455696</td>\n",
       "      <td>0.835443</td>\n",
       "      <td>0.594937</td>\n",
       "      <td>0.265823</td>\n",
       "      <td>...</td>\n",
       "      <td>0.746835</td>\n",
       "      <td>0.848101</td>\n",
       "      <td>0.860759</td>\n",
       "      <td>0.430380</td>\n",
       "      <td>0.329114</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>600416</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>湘电股份</td>\n",
       "      <td>-0.086333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>0.253165</td>\n",
       "      <td>0.696203</td>\n",
       "      <td>0.101266</td>\n",
       "      <td>0.430380</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.430380</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.468354</td>\n",
       "      <td>0.088608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645570</td>\n",
       "      <td>0.582278</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.430380</td>\n",
       "      <td>0.721519</td>\n",
       "      <td>0.177215</td>\n",
       "      <td>600416</td>\n",
       "      <td>-0.0335</td>\n",
       "      <td>湘电股份</td>\n",
       "      <td>0.018293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>2010-01-18</td>\n",
       "      <td>0.253165</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.696203</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.594937</td>\n",
       "      <td>0.367089</td>\n",
       "      <td>0.215190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.493671</td>\n",
       "      <td>0.822785</td>\n",
       "      <td>0.430380</td>\n",
       "      <td>0.658228</td>\n",
       "      <td>0.468354</td>\n",
       "      <td>600416</td>\n",
       "      <td>-0.0040</td>\n",
       "      <td>湘电股份</td>\n",
       "      <td>0.001598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>2010-01-25</td>\n",
       "      <td>0.253165</td>\n",
       "      <td>0.227848</td>\n",
       "      <td>0.227848</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.455696</td>\n",
       "      <td>0.582278</td>\n",
       "      <td>0.316456</td>\n",
       "      <td>0.468354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202532</td>\n",
       "      <td>0.405063</td>\n",
       "      <td>0.734177</td>\n",
       "      <td>0.430380</td>\n",
       "      <td>0.772152</td>\n",
       "      <td>0.569620</td>\n",
       "      <td>600416</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>湘电股份</td>\n",
       "      <td>0.055240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2326</th>\n",
       "      <td>2010-02-01</td>\n",
       "      <td>0.271605</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.901235</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.728395</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.641975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.283951</td>\n",
       "      <td>0.234568</td>\n",
       "      <td>0.419753</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>600416</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>湘电股份</td>\n",
       "      <td>-0.075757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 428 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     trade_date      ACCA     ACD20      ACD6        AD      AD20       AD6  \\\n",
       "325  2010-01-04  0.253165  0.873418  0.835443  0.443038  0.443038  0.455696   \n",
       "825  2010-01-11  0.253165  0.696203  0.101266  0.430380  0.443038  0.430380   \n",
       "1325 2010-01-18  0.253165  0.810127  0.696203  0.443038  0.443038  0.443038   \n",
       "1825 2010-01-25  0.253165  0.227848  0.227848  0.443038  0.443038  0.455696   \n",
       "2326 2010-02-01  0.271605  0.629630  0.901235  0.481481  0.456790  0.481481   \n",
       "\n",
       "          ADTM       ADX      ADXR  ...  Volumn1M  Volumn3M      WVAD  \\\n",
       "325   0.835443  0.594937  0.265823  ...  0.746835  0.848101  0.860759   \n",
       "825   0.632911  0.468354  0.088608  ...  0.645570  0.582278  0.797468   \n",
       "1325  0.594937  0.367089  0.215190  ...  0.025316  0.493671  0.822785   \n",
       "1825  0.582278  0.316456  0.468354  ...  0.202532  0.405063  0.734177   \n",
       "2326  0.728395  0.703704  0.641975  ...  0.296296  0.283951  0.234568   \n",
       "\n",
       "      WorkingCapital   minusDI    plusDI    code  chgPct  secShortName  \\\n",
       "325         0.430380  0.329114  0.632911  600416  0.0232          湘电股份   \n",
       "825         0.430380  0.721519  0.177215  600416 -0.0335          湘电股份   \n",
       "1325        0.430380  0.658228  0.468354  600416 -0.0040          湘电股份   \n",
       "1825        0.430380  0.772152  0.569620  600416 -0.0043          湘电股份   \n",
       "2326        0.419753  0.777778  0.629630  600416  0.0147          湘电股份   \n",
       "\n",
       "            dx  \n",
       "325  -0.086333  \n",
       "825   0.018293  \n",
       "1325  0.001598  \n",
       "1825  0.055240  \n",
       "2326 -0.075757  \n",
       "\n",
       "[5 rows x 428 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.merge(factor_data_org, return_data, on=['trade_date', 'code']).dropna()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看因子间相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.heatmap(train_data.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.44 s, sys: 595 ms, total: 6.03 s\n",
      "Wall time: 7.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "benchmark_total = engine.fetch_benchmark_range(dates=ref_dates, benchmark=benchmark_code)\n",
    "industry_total = engine.fetch_industry_matrix_range(universe, dates=ref_dates, category=industry_name, level=industry_level)\n",
    "\n",
    "train_data = pd.merge(factor_data, return_data, on=['trade_date', 'code']).dropna()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraintes settings\n",
    "\n",
    "industry_names = industry_list(industry_name, industry_level)\n",
    "constraint_risk = ['EARNYILD', 'LIQUIDTY', 'GROWTH', 'SIZE', 'SIZENL', 'BETA', 'MOMENTUM'] + industry_names\n",
    "\n",
    "total_risk_names = constraint_risk + ['benchmark', 'total']\n",
    "\n",
    "b_type = []\n",
    "l_val = []\n",
    "u_val = []\n",
    "\n",
    "for name in total_risk_names:\n",
    "    if name == 'benchmark':\n",
    "        b_type.append(BoundaryType.RELATIVE)\n",
    "        l_val.append(0.0)\n",
    "        u_val.append(1.0)\n",
    "    elif name == 'total':\n",
    "        b_type.append(BoundaryType.ABSOLUTE)\n",
    "        l_val.append(-0.0)\n",
    "        u_val.append(0.0)\n",
    "    elif name == 'SIZE':\n",
    "        b_type.append(BoundaryType.ABSOLUTE)\n",
    "        l_val.append(-0.1)\n",
    "        u_val.append(0.1)\n",
    "    elif name == 'SIZENL':\n",
    "        b_type.append(BoundaryType.ABSOLUTE)\n",
    "        l_val.append(-0.1)\n",
    "        u_val.append(-0.1)\n",
    "    elif name in industry_names:\n",
    "        b_type.append(BoundaryType.ABSOLUTE)\n",
    "        l_val.append(-0.005)\n",
    "        u_val.append(0.005)\n",
    "    else:\n",
    "        b_type.append(BoundaryType.ABSOLUTE)\n",
    "        l_val.append(-1.0)\n",
    "        u_val.append(1.0)\n",
    "\n",
    "bounds = create_box_bounds(total_risk_names, b_type, l_val, u_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取特征名\n",
    "features = list(basic_factor_store.keys())\n",
    "# alpha_features = list(alpha_factor_store.keys())\n",
    "# features.extend(alpha_features)\n",
    "\n",
    "label = ['dx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.2 ms, sys: 1.06 ms, total: 6.26 ms\n",
      "Wall time: 5.45 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from datetime import datetime, timedelta\n",
    "from models.m1_xgb import *\n",
    "from conf.configuration import xgb_conf\n",
    "import xgboost as xgb\n",
    "import gc\n",
    "\n",
    "\n",
    "def create_scenario():\n",
    "    weight_gap = 0.02\n",
    "    transact_cost = 0.003\n",
    "\n",
    "    executor = NaiveExecutor()\n",
    "    leverags = []\n",
    "    trade_dates = []\n",
    "    current_pos = pd.DataFrame()\n",
    "    # previous_pos = pd.DataFrame()\n",
    "    previous_pos = pd.DataFrame({'trade_date':[], 'weight':[],'industry':[], 'er':[],'code':[]})\n",
    "    tune_record = pd.DataFrame()  \n",
    "    rets = []\n",
    "    net_rets = []\n",
    "    turn_overs = []\n",
    "    leverags = []\n",
    "    ics = []\n",
    "\n",
    "    # take ref_dates[i] as an example\n",
    "    for i, ref_date in enumerate(back_ref_dates):\n",
    "        alpha_logger.info('{0} is start'.format(ref_date))\n",
    "\n",
    "        # machine learning model\n",
    "        # Filter Training data\n",
    "        # train data\n",
    "        trade_date_pre = ref_date - timedelta(days=1)\n",
    "        # trade_date_pre_80 = ref_date - timedelta(days=80)\n",
    "        # train = train_data[(train_data.trade_date <= trade_date_pre) & (trade_date_pre_80 <= train_data.trade_date)].dropna()\n",
    "        # 训练集构造, 选择当天之前(不含当天)的因子数据作为训练集.\n",
    "        train = train_data[train_data.trade_date <= trade_date_pre].dropna()\n",
    "\n",
    "        if len(train) <= 0:\n",
    "            continue\n",
    "        x_train = train[features]\n",
    "        y_train = train[label]\n",
    "        alpha_logger.info('len_x_train: {0}, len_y_train: {1}'.format(len(x_train.values), len(y_train.values)))\n",
    "        alpha_logger.info('X_train.shape={0}, X_test.shape = {1}'.format(np.shape(x_train), np.shape(y_train)))\n",
    "\n",
    "        # xgb_configuration\n",
    "        xgb_conf.xgb_config_r()\n",
    "        xgb_conf.cv_folds = None\n",
    "        xgb_conf.early_stop_round = 100\n",
    "        xgb_conf.max_round = 800\n",
    "        tic = time.time()\n",
    "        # training\n",
    "        xgb_model = XGBooster(xgb_conf)\n",
    "        GPU_device = False\n",
    "\n",
    "        if GPU_device:\n",
    "            xgb_model.set_params(tree_method='gpu_hist', max_depth=5)\n",
    "        else:\n",
    "            xgb_model.set_params(max_depth=5)\n",
    "        alpha_logger.info('train params {}s'.format(xgb_model.get_params))\n",
    "        \n",
    "        best_score, best_round, best_model = xgb_model.fit(x_train, y_train)\n",
    "        alpha_logger.info('Training time cost {}s'.format(time.time() - tic))\n",
    "        alpha_logger.info('best_score = {}, best_round = {}'.format(best_score, best_round))\n",
    "    \n",
    "        # 测试集, 取当天的因子数据作为输入.\n",
    "        total_data_test_excess = train_data[train_data.trade_date == ref_date]\n",
    "        alpha_logger.info('{0} total_data_test_excess: {1}'.format(ref_date, len(total_data_test_excess)))\n",
    "\n",
    "        if len(total_data_test_excess) <= 0:\n",
    "            alpha_logger.info('{0} HAS NO DATA!!!'.format(ref_date))\n",
    "            continue\n",
    "        \n",
    "        # 获取当天的行业, 风险模型和基准数据\n",
    "        industry_matrix = industry_total[industry_total.trade_date == ref_date]\n",
    "        benchmark_w = benchmark_total[benchmark_total.trade_date == ref_date]\n",
    "        risk_matrix = risk_total[risk_total.trade_date == ref_date]\n",
    "\n",
    "        total_data = pd.merge(industry_matrix, benchmark_w, on=['code'], how='left').fillna(0.)\n",
    "        total_data = pd.merge(total_data, risk_matrix, on=['code'])\n",
    "        alpha_logger.info('{0} len_of_total_data: {1}'.format(ref_date, len(total_data)))\n",
    "\n",
    "        total_data_test_excess = pd.merge(total_data, total_data_test_excess, on=['code'])\n",
    "        alpha_logger.info('{0} len_of_total_data_test_excess: {1}'.format(ref_date, len(total_data_test_excess)))\n",
    "        \n",
    "        codes = total_data_test_excess.code.values.tolist()\n",
    "        alpha_logger.info('{0} full re-balance: {1}'.format(ref_date, len(codes)))\n",
    "        ## 获取调仓日当天的股票收益\n",
    "        dx_returns = return_data[return_data.trade_date == ref_date][['code', 'dx']]\n",
    "\n",
    "        benchmark_w = total_data_test_excess.weight.values\n",
    "        alpha_logger.info('shape_of_benchmark_w: {}'.format(np.shape(benchmark_w)))\n",
    "        is_in_benchmark = (benchmark_w > 0.).astype(float).reshape((-1, 1))\n",
    "        total_risk_exp = np.concatenate([total_data_test_excess[constraint_risk].values.astype(float),\n",
    "                                         is_in_benchmark,\n",
    "                                         np.ones_like(is_in_benchmark)],\n",
    "                                         axis=1)\n",
    "        alpha_logger.info('shape_of_total_risk_exp_pre: {}'.format(np.shape(total_risk_exp)))\n",
    "        total_risk_exp = pd.DataFrame(total_risk_exp, columns=total_risk_names)\n",
    "        alpha_logger.info('shape_of_total_risk_exp: {}'.format(np.shape(total_risk_exp)))\n",
    "        constraints = LinearConstraints(bounds, total_risk_exp, benchmark_w)\n",
    "        alpha_logger.info('constraints: {0} in {1}'.format(np.shape(constraints.risk_targets()), ref_date))\n",
    "\n",
    "        lbound = np.maximum(0., benchmark_w - weight_gap)\n",
    "        ubound = weight_gap + benchmark_w\n",
    "        alpha_logger.info('lbound: {0} in {1}'.format(np.shape(lbound), ref_date))\n",
    "        alpha_logger.info('ubound: {0} in {1}'.format(np.shape(ubound), ref_date))\n",
    "\n",
    "        # predict\n",
    "        x_pred = total_data_test_excess[features]\n",
    "        predict_xgboost = xgb_model.predict(best_model, x_pred)\n",
    "        a = np.shape(predict_xgboost)\n",
    "        predict_xgboost = np.reshape(predict_xgboost, (a[0], -1)).astype(np.float64)\n",
    "        alpha_logger.info('shape_of_predict_xgboost: {}'.format(np.shape(predict_xgboost)))\n",
    "        del xgb_model\n",
    "        del best_model\n",
    "        gc.collect()\n",
    "        \n",
    "        # 股票过滤, 组合优化之前过滤掉\n",
    "        # backtest\n",
    "        target_pos, _ = er_portfolio_analysis(predict_xgboost,\n",
    "                                              total_data_test_excess['industry'].values,\n",
    "                                              None,\n",
    "                                              constraints,\n",
    "                                              False,\n",
    "                                              benchmark_w,\n",
    "                                              method = 'risk_neutral',\n",
    "                                              lbound=lbound,\n",
    "                                              ubound=ubound,\n",
    "                                              turn_over_target=0.2,\n",
    "                                              current_pos=previous_pos.weight.values)\n",
    "        # alpha_logger.info('target_pos: \\n{}'.format(target_pos))\n",
    "        alpha_logger.info('target_pos_shape: {}'.format(np.shape(target_pos)))\n",
    "        alpha_logger.info('len_codes:{}'.format(np.shape(codes)))\n",
    "        target_pos['code'] = codes\n",
    "        \n",
    "        result = pd.merge(target_pos, dx_returns, on=['code'])\n",
    "        result['trade_date'] = ref_date\n",
    "        tune_record = tune_record.append(result)\n",
    "        alpha_logger.info('len_result: {}'.format(len(result)))\n",
    "\n",
    "        # excess_return = np.exp(result.dx.values) - 1. - index_return.loc[ref_date, 'dx']\n",
    "        excess_return = np.exp(result.dx.values) - 1.\n",
    "        ret = result.weight.values @ excess_return\n",
    "        \n",
    "        trade_dates.append(ref_date)\n",
    "        rets.append(np.log(1. + ret))\n",
    "        alpha_logger.info('len_rets: {}, len_trade_dates: {}'.format(len(rets), len(trade_dates)))\n",
    "        \n",
    "        executor.set_current(previous_pos)\n",
    "        turn_over_org, current_pos = executor.execute(target_pos=target_pos)\n",
    "        alpha_logger.info('turn_over_org: {}'.format(turn_over_org))\n",
    "\n",
    "        turn_over = turn_over_org / sum(target_pos.weight.values)\n",
    "        alpha_logger.info('turn_over: {}'.format(turn_over))\n",
    "        turn_overs.append(turn_over)\n",
    "        \n",
    "        net_rets.append(np.log(1. + ret - transact_cost * turn_over))        \n",
    "        alpha_logger.info('len_net_rets: {}, len_trade_dates: {}'.format(len(net_rets), len(trade_dates)))\n",
    "        alpha_logger.info('{} is finished'.format(ref_date))\n",
    "    # ret_df = pd.DataFrame({'xgb_regress': rets}, index=trade_dates)\n",
    "    ret_df = pd.DataFrame({'xgb_regress': rets, 'net_xgb_regress':net_rets}, index=trade_dates)\n",
    "    ret_df.loc[advanceDateByCalendar('china.sse', ref_dates[-1], freq).strftime('%Y-%m-%d')] = 0.\n",
    "    ret_df = ret_df.shift(1)\n",
    "    ret_df.iloc[0] = 0.\n",
    "    return ret_df, tune_record, rets, net_rets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-19 01:53:48,658 - ALPHA_MIND - INFO - 2018-01-02 00:00:00 is start\n",
      "2019-12-19 01:53:50,191 - ALPHA_MIND - INFO - len_x_train: 194480, len_y_train: 194480\n",
      "2019-12-19 01:53:50,192 - ALPHA_MIND - INFO - X_train.shape=(194480, 423), X_test.shape = (194480, 1)\n",
      "2019-12-19 01:53:50,193 - ALPHA_MIND - INFO - train params <bound method XGBooster.get_params of <models.m1_xgb.XGBooster object at 0x7f1a2ff20358>>s\n",
      "2019-12-19 01:53:50,193 - ../../../models/m1_xgb.py[line:95] - INFO: NonCrossValidation。。。。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.497973\ttrain-logloss:0.683174\n",
      "Multiple eval metrics have been passed: 'train-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until train-logloss hasn't improved in 100 rounds.\n",
      "[5]\ttrain-rmse:0.473805\ttrain-logloss:0.636139\n",
      "[10]\ttrain-rmse:0.450838\ttrain-logloss:0.593375\n",
      "[15]\ttrain-rmse:0.429008\ttrain-logloss:0.554327\n",
      "[20]\ttrain-rmse:0.408258\ttrain-logloss:0.518548\n",
      "[25]\ttrain-rmse:0.388539\ttrain-logloss:0.485666\n",
      "[30]\ttrain-rmse:0.369801\ttrain-logloss:0.455363\n",
      "[35]\ttrain-rmse:0.352\ttrain-logloss:0.427373\n",
      "[40]\ttrain-rmse:0.335086\ttrain-logloss:0.401458\n",
      "[45]\ttrain-rmse:0.319015\ttrain-logloss:0.377412\n",
      "[50]\ttrain-rmse:0.303751\ttrain-logloss:0.355068\n",
      "[55]\ttrain-rmse:0.289251\ttrain-logloss:0.334265\n",
      "[60]\ttrain-rmse:0.275483\ttrain-logloss:0.314875\n",
      "[65]\ttrain-rmse:0.262411\ttrain-logloss:0.296775\n",
      "[70]\ttrain-rmse:0.249999\ttrain-logloss:0.279854\n",
      "[75]\ttrain-rmse:0.238219\ttrain-logloss:0.264023\n",
      "[80]\ttrain-rmse:0.227034\ttrain-logloss:0.249188\n",
      "[85]\ttrain-rmse:0.216424\ttrain-logloss:0.23528\n",
      "[90]\ttrain-rmse:0.206361\ttrain-logloss:0.22223\n",
      "[95]\ttrain-rmse:0.196818\ttrain-logloss:0.209972\n",
      "[100]\ttrain-rmse:0.187764\ttrain-logloss:0.198443\n",
      "[105]\ttrain-rmse:0.179188\ttrain-logloss:0.187605\n",
      "[110]\ttrain-rmse:0.171059\ttrain-logloss:0.177398\n",
      "[115]\ttrain-rmse:0.163357\ttrain-logloss:0.167781\n",
      "[120]\ttrain-rmse:0.156065\ttrain-logloss:0.158717\n",
      "[125]\ttrain-rmse:0.149164\ttrain-logloss:0.150171\n",
      "[130]\ttrain-rmse:0.142636\ttrain-logloss:0.142107\n",
      "[135]\ttrain-rmse:0.136464\ttrain-logloss:0.134496\n",
      "[140]\ttrain-rmse:0.13063\ttrain-logloss:0.127306\n",
      "[145]\ttrain-rmse:0.12512\ttrain-logloss:0.120512\n",
      "[150]\ttrain-rmse:0.119919\ttrain-logloss:0.114091\n",
      "[155]\ttrain-rmse:0.115013\ttrain-logloss:0.108017\n",
      "[160]\ttrain-rmse:0.110385\ttrain-logloss:0.102267\n",
      "[165]\ttrain-rmse:0.106027\ttrain-logloss:0.096828\n",
      "[170]\ttrain-rmse:0.101928\ttrain-logloss:0.091678\n",
      "[175]\ttrain-rmse:0.098069\ttrain-logloss:0.086794\n",
      "[180]\ttrain-rmse:0.094442\ttrain-logloss:0.082072\n",
      "[185]\ttrain-rmse:0.091036\ttrain-logloss:0.077586\n",
      "[190]\ttrain-rmse:0.087846\ttrain-logloss:0.073427\n",
      "[195]\ttrain-rmse:0.084856\ttrain-logloss:0.069331\n",
      "[200]\ttrain-rmse:0.082055\ttrain-logloss:0.065545\n",
      "[205]\ttrain-rmse:0.079437\ttrain-logloss:0.061924\n",
      "[210]\ttrain-rmse:0.076991\ttrain-logloss:0.058505\n",
      "[215]\ttrain-rmse:0.07471\ttrain-logloss:0.055304\n",
      "[220]\ttrain-rmse:0.072585\ttrain-logloss:0.052259\n",
      "[225]\ttrain-rmse:0.070608\ttrain-logloss:0.049231\n",
      "[230]\ttrain-rmse:0.068772\ttrain-logloss:0.0464\n",
      "[235]\ttrain-rmse:0.067066\ttrain-logloss:0.043762\n",
      "[240]\ttrain-rmse:0.065481\ttrain-logloss:0.041119\n",
      "[245]\ttrain-rmse:0.064015\ttrain-logloss:0.038712\n",
      "[250]\ttrain-rmse:0.062658\ttrain-logloss:0.036385\n",
      "[255]\ttrain-rmse:0.061407\ttrain-logloss:0.034233\n",
      "[260]\ttrain-rmse:0.060251\ttrain-logloss:0.032132\n",
      "[265]\ttrain-rmse:0.059187\ttrain-logloss:0.030127\n",
      "[270]\ttrain-rmse:0.058205\ttrain-logloss:0.027983\n",
      "[275]\ttrain-rmse:0.057305\ttrain-logloss:0.026021\n",
      "[280]\ttrain-rmse:0.056477\ttrain-logloss:0.024108\n",
      "[285]\ttrain-rmse:0.055716\ttrain-logloss:0.022329\n",
      "[290]\ttrain-rmse:0.055018\ttrain-logloss:0.02057\n",
      "[295]\ttrain-rmse:0.054376\ttrain-logloss:0.018736\n",
      "[300]\ttrain-rmse:0.053789\ttrain-logloss:0.017031\n",
      "[305]\ttrain-rmse:0.053254\ttrain-logloss:0.015327\n",
      "[310]\ttrain-rmse:0.052762\ttrain-logloss:0.013527\n",
      "[315]\ttrain-rmse:0.052314\ttrain-logloss:0.011479\n",
      "[320]\ttrain-rmse:0.051904\ttrain-logloss:0.009527\n",
      "[325]\ttrain-rmse:0.051531\ttrain-logloss:0.007594\n",
      "[330]\ttrain-rmse:0.051189\ttrain-logloss:0.005403\n",
      "[335]\ttrain-rmse:0.050878\ttrain-logloss:0.002858\n",
      "[340]\ttrain-rmse:0.050594\ttrain-logloss:0.000455\n",
      "[345]\ttrain-rmse:0.050334\ttrain-logloss:-0.001896\n",
      "[350]\ttrain-rmse:0.050098\ttrain-logloss:-0.004387\n",
      "[355]\ttrain-rmse:0.049882\ttrain-logloss:-0.007123\n",
      "[360]\ttrain-rmse:0.049685\ttrain-logloss:-0.010167\n",
      "[365]\ttrain-rmse:0.049505\ttrain-logloss:-0.013375\n",
      "[370]\ttrain-rmse:0.049343\ttrain-logloss:-0.016533\n",
      "[375]\ttrain-rmse:0.049194\ttrain-logloss:-0.019891\n",
      "[380]\ttrain-rmse:0.049057\ttrain-logloss:-0.023759\n",
      "[385]\ttrain-rmse:0.048932\ttrain-logloss:-0.027516\n",
      "[390]\ttrain-rmse:0.048819\ttrain-logloss:-0.031678\n",
      "[395]\ttrain-rmse:0.048714\ttrain-logloss:-0.035587\n",
      "[400]\ttrain-rmse:0.048618\ttrain-logloss:-0.039893\n",
      "[405]\ttrain-rmse:0.048532\ttrain-logloss:-0.044232\n",
      "[410]\ttrain-rmse:0.048453\ttrain-logloss:-0.048358\n",
      "[415]\ttrain-rmse:0.048382\ttrain-logloss:-0.05213\n",
      "[420]\ttrain-rmse:0.048315\ttrain-logloss:-0.056659\n",
      "[425]\ttrain-rmse:0.048254\ttrain-logloss:-0.060953\n",
      "[430]\ttrain-rmse:0.048198\ttrain-logloss:-0.065512\n",
      "[435]\ttrain-rmse:0.048145\ttrain-logloss:-0.06954\n",
      "[440]\ttrain-rmse:0.048097\ttrain-logloss:-0.073412\n",
      "[445]\ttrain-rmse:0.048052\ttrain-logloss:-0.077134\n",
      "[450]\ttrain-rmse:0.048013\ttrain-logloss:-0.080738\n",
      "[455]\ttrain-rmse:0.047975\ttrain-logloss:-0.084277\n",
      "[460]\ttrain-rmse:0.047942\ttrain-logloss:-0.087478\n",
      "[465]\ttrain-rmse:0.047911\ttrain-logloss:-0.090919\n",
      "[470]\ttrain-rmse:0.047881\ttrain-logloss:-0.093693\n",
      "[475]\ttrain-rmse:0.047853\ttrain-logloss:-0.096436\n",
      "[480]\ttrain-rmse:0.047828\ttrain-logloss:-0.099419\n",
      "[485]\ttrain-rmse:0.047804\ttrain-logloss:-0.101718\n",
      "[490]\ttrain-rmse:0.047782\ttrain-logloss:-0.103848\n",
      "[495]\ttrain-rmse:0.047762\ttrain-logloss:-0.106211\n",
      "[500]\ttrain-rmse:0.047741\ttrain-logloss:-0.108483\n",
      "[505]\ttrain-rmse:0.047721\ttrain-logloss:-0.110144\n",
      "[510]\ttrain-rmse:0.047704\ttrain-logloss:-0.112355\n",
      "[515]\ttrain-rmse:0.047687\ttrain-logloss:-0.114163\n",
      "[520]\ttrain-rmse:0.047672\ttrain-logloss:-0.115559\n",
      "[525]\ttrain-rmse:0.047657\ttrain-logloss:-0.116859\n",
      "[530]\ttrain-rmse:0.047642\ttrain-logloss:-0.118529\n",
      "[535]\ttrain-rmse:0.047629\ttrain-logloss:-0.119834\n",
      "[540]\ttrain-rmse:0.047615\ttrain-logloss:-0.121003\n",
      "[545]\ttrain-rmse:0.047602\ttrain-logloss:-0.122594\n",
      "[550]\ttrain-rmse:0.047591\ttrain-logloss:-0.123946\n",
      "[555]\ttrain-rmse:0.047579\ttrain-logloss:-0.125183\n",
      "[560]\ttrain-rmse:0.047568\ttrain-logloss:-0.125988\n",
      "[565]\ttrain-rmse:0.047557\ttrain-logloss:-0.126946\n",
      "[570]\ttrain-rmse:0.047545\ttrain-logloss:-0.128174\n",
      "[575]\ttrain-rmse:0.047535\ttrain-logloss:-0.129373\n",
      "[580]\ttrain-rmse:0.047525\ttrain-logloss:-0.130226\n",
      "[585]\ttrain-rmse:0.047514\ttrain-logloss:-0.130718\n",
      "[590]\ttrain-rmse:0.047503\ttrain-logloss:-0.131455\n",
      "[595]\ttrain-rmse:0.047494\ttrain-logloss:-0.132037\n",
      "[600]\ttrain-rmse:0.047486\ttrain-logloss:-0.132958\n",
      "[605]\ttrain-rmse:0.047476\ttrain-logloss:-0.133549\n",
      "[610]\ttrain-rmse:0.047466\ttrain-logloss:-0.134179\n",
      "[615]\ttrain-rmse:0.047457\ttrain-logloss:-0.134853\n",
      "[620]\ttrain-rmse:0.047447\ttrain-logloss:-0.135472\n",
      "[625]\ttrain-rmse:0.04744\ttrain-logloss:-0.135534\n",
      "[630]\ttrain-rmse:0.047431\ttrain-logloss:-0.136032\n",
      "[635]\ttrain-rmse:0.047423\ttrain-logloss:-0.136735\n",
      "[640]\ttrain-rmse:0.047414\ttrain-logloss:-0.13741\n",
      "[645]\ttrain-rmse:0.047406\ttrain-logloss:-0.137887\n",
      "[650]\ttrain-rmse:0.047398\ttrain-logloss:-0.138152\n",
      "[655]\ttrain-rmse:0.047389\ttrain-logloss:-0.138683\n",
      "[660]\ttrain-rmse:0.047382\ttrain-logloss:-0.13908\n",
      "[665]\ttrain-rmse:0.047375\ttrain-logloss:-0.139534\n",
      "[670]\ttrain-rmse:0.047366\ttrain-logloss:-0.139931\n",
      "[675]\ttrain-rmse:0.047359\ttrain-logloss:-0.140456\n",
      "[680]\ttrain-rmse:0.047352\ttrain-logloss:-0.140608\n",
      "[685]\ttrain-rmse:0.047344\ttrain-logloss:-0.140838\n",
      "[690]\ttrain-rmse:0.047337\ttrain-logloss:-0.14124\n",
      "[695]\ttrain-rmse:0.04733\ttrain-logloss:-0.141797\n"
     ]
    }
   ],
   "source": [
    "# 滚动回测\n",
    "ret_df, tune_record, rets, net_rets = create_scenario()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_record.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyecharts import options as opts\n",
    "from example.commons import Collector\n",
    "from pyecharts.charts import Line\n",
    "\n",
    "plot = ret_df[['xgb_regress', 'net_xgb_regress']].cumsum()\n",
    "v1 = list(plot.index)\n",
    "v2 = list(plot.xgb_regress)\n",
    "v3 = list(plot.net_xgb_regress)\n",
    "\n",
    "line_chart = (\n",
    "    Line()\n",
    "    .add_xaxis(v1)\n",
    "    .add_yaxis(\"xgb_regress\", v2)\n",
    "    .add_yaxis(\"net_xgb_regress\", v3)\n",
    "    .set_series_opts(\n",
    "        label_opts=opts.LabelOpts(is_show=False),\n",
    "    )\n",
    "    .set_global_opts(\n",
    "        xaxis_opts=opts.AxisOpts(is_scale=True),\n",
    "        yaxis_opts=opts.AxisOpts(\n",
    "            is_scale=True,\n",
    "            splitarea_opts=opts.SplitAreaOpts(\n",
    "                is_show=True, areastyle_opts=opts.AreaStyleOpts(opacity=1)\n",
    "            ),\n",
    "        ),\n",
    "        datazoom_opts=[opts.DataZoomOpts(pos_bottom=\"-1%\")],\n",
    "        title_opts=opts.TitleOpts(title='Fixed freq rebalanced: {0}'.format(freq)),\n",
    "    )\n",
    ")\n",
    "\n",
    "line_chart.render_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 调仓记录保存\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "con = sqlite3.connect('./tune_record.db')\n",
    "# tune_record.to_sql('tune_record', con=con, if_exists='append', index=False)\n",
    "sql = 'select * from tune_record'\n",
    "tune_record = pd.read_sql(sql, con)\n",
    "tune_record[['trade_date','code','portfolio_weight']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ##--------------因子组合回测--------------\n",
    "import json\n",
    "import pdb\n",
    "from rqalpha.api import *\n",
    "from rqalpha import run_func\n",
    "class DailyDefaultStrategy(object):\n",
    "    def __init__(self, session, stock_sets_ob):\n",
    "        self._stock_sets_ob = json.loads(stock_sets_ob)\n",
    "        self._session = session\n",
    "        self._start_time_str = datetime.now().strftime('%m-%d %H:%M')\n",
    "    \n",
    "    \n",
    "    def init(self,context):\n",
    "        #读取股票池文件\n",
    "        context.stock_sets = pd.DataFrame(self._stock_sets_ob)\n",
    "        context.trade_date_list= list(set(context.stock_sets.trade_date))\n",
    "        context.holding_stock_df = None\n",
    "        \n",
    "        # 是否已发送了order\n",
    "        context.fired = False\n",
    "\n",
    "    #盘前处理\n",
    "    def before_trading(self, context, bar_dict):\n",
    "        context.trade_signal = False\n",
    "        date = context.now.date()\n",
    "        if str(date) in context.trade_date_list:\n",
    "            context.trade_signal = True\n",
    "\n",
    "    #盘后处理\n",
    "    def after_trading(self, context):\n",
    "        pass\n",
    "    \n",
    "    # 你选择的证券的数据更新将会触发此段逻辑，例如日或分钟历史数据切片或者是实时数据切片更新\n",
    "    def handle_bar(self, context, bar_dict):\n",
    "        # 开始编写你的主要的算法逻辑\n",
    "        if context.trade_signal == False:\n",
    "            return\n",
    "        date = context.now.date()\n",
    "        # stock_dict = context.stock_sets.set_index('trade_date').loc[str(date)]\n",
    "        stock_set = context.stock_sets\n",
    "        stock_dict = stock_set[stock_set['trade_date'] == str(date)].set_index('trade_date')\n",
    "        stock_df = self.filter_specials(stock_dict, context)\n",
    "        #剔除后的股票\n",
    "        if 'portfolio_weight' in list(set(stock_df.columns)):\n",
    "            self.rebalance_weight(context, stock_df)\n",
    "        else:\n",
    "            self.rebalance_equal(context, stock_df)\n",
    "   \n",
    "    def _industry_distribute(self, stock_positions, industries):\n",
    "        industry_sets_dict = {}\n",
    "        industries = industries.rename(columns={'symbol':'code'})\n",
    "        stock_positions = stock_positions.reset_index().rename(\n",
    "            columns={'symbol':'name','order_book_id':'code','date':'trade_date'})\n",
    "        stock_positions['trade_date'] = stock_positions['trade_date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "        industries['trade_date'] = industries['trade_date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "        stock_positions = stock_positions.merge(industries, on=['trade_date','code'])\n",
    "        industry_grouped = stock_positions.dropna().groupby(['trade_date'])\n",
    "        for k, g in industry_grouped:\n",
    "            gt = g.groupby(['industry_code','industry']).count().reset_index()[['industry_code','industry','code']]\n",
    "            gt['ratio'] = gt['code']/gt['code'].values.sum()\n",
    "            gt=gt.rename(columns={'industry':'industry_name'})\n",
    "            industry_sets_dict[k] = gt[['industry_code','industry_name','ratio']].to_dict(orient='records')\n",
    "        return industry_sets_dict\n",
    "    \n",
    "    def _month_profit(self, portfolio, stock_capital):\n",
    "        pd.DataFrame(portfolio.reset_index()).shift(1)\n",
    "        month_profit = portfolio['total_value'].resample('M').mean()\n",
    "        month_profit = pd.DataFrame(month_profit.reset_index())\n",
    "        month_profit['last_total'] = month_profit['total_value'].shift(1)\n",
    "        month_profit = month_profit.fillna(stock_capital)\n",
    "        month_profit['month_ratio'] = np.log(month_profit.total_value / month_profit.last_total)\n",
    "        return month_profit.rename(columns={'date':'trade_date'})\n",
    "    \n",
    "    def set_result(self, result):\n",
    "        #summary 结算信息\n",
    "        summary = result['sys_analyser']['summary']\n",
    "        #trades 交易记录\n",
    "        trades = result['sys_analyser']['trades']\n",
    "        #portfolio 收益曲线\n",
    "        portfolio = result['sys_analyser']['portfolio']\n",
    "        # benchmark_portfolio 基础收益曲线benchmark_portfolio\n",
    "        benchmark_portfolio = result['sys_analyser']['benchmark_portfolio']\n",
    "        # stock_account 个人收益\n",
    "        stock_account = result['sys_analyser']['stock_account']\n",
    "        # stock_positions 股票持仓情况\n",
    "        stock_positions = result['sys_analyser']['stock_positions']\n",
    "        \n",
    "        \n",
    "    def filter_specials(self, stock_dict, context):\n",
    "        stock_df = pd.DataFrame(stock_dict)\n",
    "        stock_list = list(stock_df.code)\n",
    "        stock_list=[stock for stock in stock_list]\n",
    "        \n",
    "        return stock_df.set_index('code').loc[stock_list,:] \n",
    "    \n",
    "    def rebalance_equal(self, context, stock_df):\n",
    "        holding_list = list(set(stock_df.index))\n",
    "        if len(holding_list) > 0:\n",
    "            every_stock = context.portfolio.portfolio_value/len(holding_list) \n",
    "        # 空仓只有买入操作\n",
    "        if len(list(context.portfolio.positions.keys()))==0:\n",
    "            for stock_to_buy in list(holding_list):\n",
    "                #print(stock_to_buy)\n",
    "                order_target_value(stock_to_buy,every_stock)\n",
    "        else:\n",
    "            for stock_to_sell in list(context.portfolio.positions.keys()):\n",
    "                if stock_to_sell not in list(holding_list):\n",
    "                    order_target_value(stock_to_sell, 0)\n",
    "            for stock_to_buy in list(holding_list):\n",
    "                order_target_value(stock_to_buy, every_stock)\n",
    "                \n",
    "        \n",
    "    def rebalance_weight(self, context, stock_df):\n",
    "        #没有持仓则全部下单\n",
    "        if len(list(context.portfolio.positions.keys())) == 0:\n",
    "            weight_sum = stock_df.portfolio_weight.sum()\n",
    "            every_values = context.portfolio.total_value / weight_sum\n",
    "            for index, weight in stock_df.iterrows():\n",
    "                order_target_value(index, weight.portfolio_weight * every_values)\n",
    "            context.holding_stock_df = stock_df\n",
    "        else:\n",
    "            now_df = context.holding_stock_df #当前持有的股票\n",
    "            # 做交集查看上次实际成交股票\n",
    "            intersect = list(set(now_df.index).intersection(set(context.portfolio.stock_account.positions.keys())))\n",
    "            if len(intersect) <= 0:\n",
    "                now_df = pd.DataFrame({'now_weight':[]})\n",
    "            else:\n",
    "                now_df = now_df.loc[context.portfolio.stock_account.positions.keys()]\n",
    "                now_df = now_df.rename(columns={'portfolio_weight':'now_weight'}) #持仓\n",
    "            # 做并集\n",
    "            stock_df = stock_df.rename(columns={'portfolio_weight':'next_weight'}) #调仓\n",
    "            now_df = pd.merge(now_df, stock_df, left_index=True, right_index=True, how='outer')\n",
    "            now_df = now_df.fillna(0)\n",
    "            now_df['diff_weight'] = now_df.next_weight - now_df.now_weight #调仓股票和持仓股票进行对比\n",
    "            buy_stock = now_df[now_df['diff_weight'] > 0] #当前需买进\n",
    "            sell_stock = now_df[now_df['diff_weight'] < 0]\n",
    "            \n",
    "            # 根据当前权重分配，目的是持仓达到和调仓的权重一样\n",
    "            weight_sum = stock_df.next_weight.sum()\n",
    "            every_values = context.portfolio.total_value/ weight_sum\n",
    "            \n",
    "            # 平掉不需要股票\n",
    "            for index, weight in sell_stock.iterrows():\n",
    "                if weight.now_weight == abs(weight.diff_weight): #全部清仓\n",
    "                    order_target_value(index, 0)\n",
    "                else:# 调整权重，此处会有误差，每期的every_values不一样\n",
    "                    order_target_value(index, weight.diff_weight * every_values)\n",
    "            # 新建增仓\n",
    "            for index, weight in buy_stock.iterrows():\n",
    "                order_target_value(index, abs(weight.diff_weight) * every_values)\n",
    "            stock_df = stock_df.rename(columns={'next_weight':'portfolio_weight'})\n",
    "            context.holding_stock_df = stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ricequant_backtesting(conmbine):\n",
    "    benchmark_info = '000905.XSHG'\n",
    "    conmbine = conmbine.copy()\n",
    "    conmbine['trade_date'] = conmbine['trade_date'].apply(lambda x: x.split(' ')[0] if type(x) is str else x.date().strftime(\"%Y-%m-%d\"))\n",
    "    # stk_result = stk_data[stk_data['portfolio_weight']>0.001]#.set_index('trade_date').loc['2018-04-12']\n",
    "    conmbine['code'] = conmbine['code'].apply(lambda x: \"{:06d}\".format(x) + '.XSHG' \n",
    "                                              if len(str(x))==6 and str(x)[0] in '6' \n",
    "                                              else \"{:06d}\".format(x)+ '.XSHE')\n",
    "    backtesting = DailyDefaultStrategy(1234412, json.dumps(conmbine.to_dict(orient='records')))\n",
    "    trade_date_list = list(set(conmbine.trade_date))\n",
    "    trade_date_list.sort(reverse=False)\n",
    "    base_config = {}\n",
    "    base_config['start_date'] = trade_date_list[0]\n",
    "    base_config['end_date'] = trade_date_list[-1]\n",
    "    base_config['benchmark'] = benchmark_info\n",
    "            \n",
    "    accounts_config = {}\n",
    "    accounts_config['stock'] = 100000000\n",
    "    base_config['accounts'] = accounts_config\n",
    "    base_config['frequency'] = '1d'\n",
    "    base_config['skip_suspended'] = True\n",
    "    \n",
    "    extra_config = {}\n",
    "\n",
    "    mod_config = {}\n",
    "    sys_analyser_config = {}\n",
    "    sys_analyser_config['enabled'] = True\n",
    "    sys_analyser_config['plot'] = True\n",
    "    mod_config['sys_analyser'] = sys_analyser_config\n",
    "        \n",
    "    config = {}\n",
    "    config['base'] = base_config\n",
    "    config['extra'] = extra_config\n",
    "    config['mod'] = mod_config\n",
    "    result = run_func(init=backtesting.init, before_trading=backtesting.before_trading, \n",
    "                  after_trading=backtesting.after_trading, handle_bar=backtesting.handle_bar, config=config)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "tune_record  = tune_record.rename(columns={'weight':'portfolio_weight'})\n",
    "#处理权重小, 直接处理为0\n",
    "# tune_record = tune_record[tune_record['portfolio_weight'] >= 0.05]\n",
    "result = ricequant_backtesting(tune_record[['trade_date','code','portfolio_weight']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tune_record[tune_record['trade_date'] == '2019-06-05']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回测记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.keys())\n",
    "record = result['sys_analyser']\n",
    "print(record.keys())\n",
    "trade_detail = record['trades']\n",
    "stock_account = record['stock_account']\n",
    "stock_positions = record['stock_positions']\n",
    "stock_positions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取指定交易日期的交易记录\n",
    "trade_detail[trade_detail['trading_datetime'] == '2018-11-29 15:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_detail[trade_detail['trading_datetime'] == '2018-12-06 15:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取指定交易日的持仓仓位\n",
    "stock_positions[(stock_positions.index == '2018-12-20') & (stock_positions.quantity != 0)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_positions[(stock_positions.index == '2018-12-27') & (stock_positions.quantity != 0)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_positions[(stock_positions.index == '2019-01-07') & (stock_positions.quantity != 0)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trade_detail.to_csv('./trades_record.csv', encoding=\"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
