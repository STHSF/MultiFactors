{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/workenv/vision/lib/python3.6/site-packages/uqer/__init__.py\", line 19, in <module>\n",
      "    DataAPI.api_base.replace_api_files()\n",
      "  File \"/opt/workenv/vision/lib/python3.6/site-packages/uqer/DataAPI/api_base.py\", line 265, in replace_api_files\n",
      "    os.remove(file_path)\n",
      "PermissionError: [Errno 13] Permission denied: '/opt/workenv/vision/lib/python3.6/site-packages/uqer/DataAPI/DATAYES.py'\n",
      "\n",
      "upgrade fail.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, select, and_, or_\n",
    "from sqlalchemy import MetaData, create_engine\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import select, and_, literal, bindparam, exists\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.inspection import inspect\n",
    "import sqlalchemy as sa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from alphamind.api import *\n",
    "from alphamind.data.processing import factor_processing\n",
    "from alphamind.data.standardize import standardize\n",
    "from alphamind.data.winsorize import winsorize_normal\n",
    "from alphamind.data.neutralize import neutralize\n",
    "from alphamind.portfolio.riskmodel import FactorRiskModel\n",
    "from alphamind.portfolio.constraints import LinearConstraints\n",
    "from alphamind.data.processing import factor_processing\n",
    "from alphamind.analysis.factoranalysis import er_portfolio_analysis\n",
    "from ultron.factor.genetic.geneticist.operators import calc_factor\n",
    "import pdb\n",
    "import uqer\n",
    "from uqer import DataAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#收益率计算\n",
    "def returns_processing(mkt_df, key='bar30_vwap',horizon=1):\n",
    "    price_tb = mkt_df[key].unstack()\n",
    "    return_tb = (price_tb.shift(-horizon) / price_tb - 1.0)\n",
    "    return_tb[return_tb>10.0] = np.NaN\n",
    "    return_tb = return_tb.shift(-1)\n",
    "    return_se = return_tb.stack().reindex(mkt_df.index)\n",
    "    mkt_df['nxt1_ret'] = return_se #索引为trade_date,code，复制时会根据索引复制\n",
    "    return mkt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据处理\n",
    "def data_processing(risk_se, mkt_se, factors_se, \n",
    "                        factor_sets, is_process=False):\n",
    "    factors_data = factors_se.reset_index()\n",
    "    if is_process:\n",
    "        ndiff_field = [column for column in factors_data.columns if column not in factor_sets]\n",
    "        alpha_res = []\n",
    "        grouped = factors_data.groupby(['trade_date'])\n",
    "        for k, g in grouped:\n",
    "            ret_preprocess = factor_processing(g[factor_sets].fillna(0).values,\n",
    "                                                   pre_process=[winsorize_normal, standardize])\n",
    "            f = pd.DataFrame(ret_preprocess, columns=factor_sets)\n",
    "            for k in ndiff_field:\n",
    "                f[k] = g[k].values\n",
    "            alpha_res.append(f)\n",
    "        factors_data = pd.concat(alpha_res)\n",
    "    return factors_data.merge(risk_se.reset_index(),on=['trade_date','code']).merge(\n",
    "                    mkt_se.reset_index(),on=['trade_date','code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_processing(univ_df, risk_df, mkt_df, factors_df, universe_code):\n",
    "    univ_se = univ_df[universe_code]\n",
    "    univ_se = univ_se[univ_se>0].dropna()\n",
    "    risk_se = risk_df.set_index(['trade_date','code']).reindex(univ_se.index)\n",
    "    risk_se.dropna(inplace=True)\n",
    "    mkt_se = mkt_df.set_index(['trade_date','code']).reindex(univ_se.index)\n",
    "    mkt_se.dropna(inplace=True)\n",
    "    factors_se = factors_df.set_index(['trade_date','code']).reindex(univ_se.index)\n",
    "    return risk_se, mkt_se, factors_se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def market_processing(mktbar_raw_df, mkt_df):\n",
    "    mktbar_raw_df['vwap'] = mktbar_raw_df['total_value'] / mktbar_raw_df['total_volume']\n",
    "    mktbar_raw_df = mktbar_raw_df.sort_values(['trade_date','code']).reset_index(drop=True)\n",
    "    mktbar_df = mktbar_raw_df.set_index(['trade_date', 'code', 'unit'])['vwap'].unstack()\n",
    "    mktbar_df.columns = ['bar'+str(x)+'_vwap' for x in mktbar_df.columns]\n",
    "    mktbar_df = mktbar_df.reset_index()\n",
    "    mkt_df = mkt_df.merge(mktbar_df, on=['trade_date', 'code'], how='left')\n",
    "    for price in ['closePrice', 'openPrice', 'lowestPrice', 'highestPrice', 'bar30_vwap', 'bar60_vwap']:\n",
    "        mkt_df[price] = mkt_df[price] * mkt_df['accumAdjFactor']\n",
    "            \n",
    "    return mkt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorsDB(object):\n",
    "    def __init__(self):\n",
    "        DB_URL = 'postgresql+psycopg2://alpha:alpha@180.166.26.82:8889/alpha'\n",
    "        self._engine = sa.create_engine(DB_URL)\n",
    "        self._session = sessionmaker(bind=self._engine, autocommit=False, autoflush=True)\n",
    "        self._base = automap_base()\n",
    "        self._base.prepare(self._engine,reflect=True)\n",
    "    \n",
    "    def _query_statements(self, start_date: str = None, end_date: str = None, universe: str = None, dates=None):\n",
    "        Universe = self._base.classes['universe']\n",
    "        return and_(\n",
    "            getattr(Universe, universe) == 1,\n",
    "            Universe.trade_date.in_(dates) if dates else Universe.trade_date.between(start_date, end_date)\n",
    "        )\n",
    "    \n",
    "    def universe(self, universe, start_date, end_date):\n",
    "        Universe = self._base.classes['universe']\n",
    "        pdb.set_trace()\n",
    "        query = select([Universe.trade_date, Universe.code]).where(\n",
    "            self._query_statements(start_date, end_date, universe)\n",
    "        ).order_by(Universe.trade_date, Universe.code)\n",
    "        return pd.read_sql(query, self._engine)\n",
    "    \n",
    "    def fetch_universes(self, universe_codes, start_date, end_date):\n",
    "        Universe = self._base.classes['universe']\n",
    "        col_list = [Universe.trade_date, Universe.code]\n",
    "        for universe in universe_codes:\n",
    "            col_list.append(Universe.__dict__[universe])\n",
    "        query = select(col_list).where(\n",
    "            and_(\n",
    "            Universe.trade_date >= start_date,\n",
    "            Universe.trade_date <= end_date,\n",
    "        ))\n",
    "        univ_df = pd.read_sql(query, self._engine)\n",
    "        univ_df = univ_df.set_index(['trade_date', 'code']).sort_index()\n",
    "        return univ_df\n",
    "    \n",
    "    \n",
    "    def fetch_risk_exposure(self, start_date, end_date):\n",
    "        RiskExposure = self._base.classes['risk_exposure']\n",
    "        query = select([RiskExposure]).where(\n",
    "            and_(\n",
    "                RiskExposure.trade_date >= start_date,\n",
    "                RiskExposure.trade_date <= end_date,\n",
    "        ))\n",
    "        return pd.read_sql(query, self._engine)\n",
    "        \n",
    "    def fetch_risk_cov(self, start_date, end_date, risk_model='short'):\n",
    "        RiskCov = self._base.classes['risk_cov_' + risk_model]\n",
    "        query = select([RiskCov]).where(\n",
    "            and_(\n",
    "            RiskCov.trade_date >= start_date,\n",
    "            RiskCov.trade_date <= end_date,\n",
    "        ))\n",
    "        return pd.read_sql(query, self._engine).sort_values('FactorID')\n",
    "        \n",
    "    def fetch_special_risk(self, start_date, end_date, risk_model='short'): \n",
    "        SpecificRisk = self._base.classes['specific_risk_' + risk_model]\n",
    "        query = select([SpecificRisk]).where(\n",
    "            and_(\n",
    "            SpecificRisk.trade_date >= start_date,\n",
    "            SpecificRisk.trade_date <= end_date\n",
    "        ))\n",
    "        return pd.read_sql(query, self._engine)\n",
    "    \n",
    "    def fetch_market(self, start_date, end_date):\n",
    "        Market = self._base.classes['market']\n",
    "        query = select([Market.trade_date, Market.code, Market.accumAdjFactor,\n",
    "                Market.closePrice, Market.openPrice, Market.lowestPrice,\n",
    "                Market.openPrice, Market.highestPrice, \n",
    "                Market.turnoverVol,Market.turnoverValue,Market.chgPct,\n",
    "                Market.marketValue]).where(\n",
    "        and_(\n",
    "            Market.trade_date >= start_date,\n",
    "            Market.trade_date <= end_date\n",
    "        ))\n",
    "        return pd.read_sql(query, self._engine)\n",
    "    \n",
    "    def fetch_marketraw(self, start_date, end_date):\n",
    "        MarketBar = self._base.classes['market_bar']\n",
    "        query = select([MarketBar.trade_date, MarketBar.code, MarketBar.bar_time, MarketBar.unit,\n",
    "                MarketBar.vwap, MarketBar.close_price, MarketBar.total_volume, MarketBar.total_value]).where(\n",
    "        and_(\n",
    "            MarketBar.trade_date >= start_date,\n",
    "            MarketBar.trade_date <= end_date,\n",
    "            or_(and_(MarketBar.unit == 60, MarketBar.bar_time == '10:30'),\n",
    "                and_(MarketBar.unit == 30, MarketBar.bar_time == '10:00'))\n",
    "        ))\n",
    "        return pd.read_sql(query, self._engine)\n",
    "    \n",
    "    def fetch_index_component(self,index_code_sets, start_date, end_date):\n",
    "        IndexComponent = self._base.classes['index_components']\n",
    "        query = select([IndexComponent.trade_date, IndexComponent.indexCode,\n",
    "                IndexComponent.code, IndexComponent.secShortName,\n",
    "                    (IndexComponent.weight / 100.).label('weight')]\n",
    "                  ).where(\n",
    "            and_(IndexComponent.trade_date >= start_date,\n",
    "                 IndexComponent.trade_date <= end_date,\n",
    "                 IndexComponent.indexCode.in_(index_code_sets)\n",
    "            )\n",
    "        )\n",
    "        return pd.read_sql(query, self._engine)\n",
    "        \n",
    "    def fetch_technical(self, start_date, end_date):\n",
    "        Technical = self._base.classes['technical']\n",
    "        query = select([Technical]).where(\n",
    "            and_(\n",
    "                Technical.trade_date >= start_date,\n",
    "                Technical.trade_date <= end_date,\n",
    "        ))\n",
    "        return pd.read_sql(query, self._engine)\n",
    "    \n",
    "    \n",
    "    def fetch_institution(self, start_date, end_date):\n",
    "        InstitutionHaitong = self._base.classes['institution_haitong']\n",
    "        query = select([InstitutionHaitong]).where(\n",
    "            and_(\n",
    "                InstitutionHaitong.trade_date >= start_date,\n",
    "                InstitutionHaitong.trade_date <= end_date,\n",
    "            ))\n",
    "        return pd.read_sql(query, self._engine)\n",
    "    \n",
    "    def fetch_uqer(self, start_date, end_date, keys=[], columns=[]):\n",
    "        Uqer = self._base.classes['uqer']\n",
    "        if len(keys) > 0 and len(columns) > 0:\n",
    "            cols = []\n",
    "            for key in keys:\n",
    "                cols.append(Uqer.__dict__[key])\n",
    "            for col in columns:\n",
    "                cols.append(Uqer.__dict__[col])\n",
    "                query = select(cols).where(\n",
    "                    and_(\n",
    "                        Uqer.trade_date >= start_date,\n",
    "                        Uqer.trade_date <= end_date\n",
    "                        ))\n",
    "        else:\n",
    "            query = select([Uqer]).where(\n",
    "                and_(\n",
    "                    Uqer.trade_date >= start_date,\n",
    "                    Uqer.trade_date <= end_date,\n",
    "                ))\n",
    "        return pd.read_sql(query, self._engine)\n",
    "\n",
    "    def fetch_experimental(self, start_date, end_date):\n",
    "        Experimental = self._base.classes['experimental']\n",
    "        query = select([Experimental]).where(\n",
    "            and_(\n",
    "                Experimental.trade_date >= start_date,\n",
    "                Experimental.trade_date <= end_date,\n",
    "            ))\n",
    "        return pd.read_sql(query, self._engine)\n",
    "    \n",
    "    def fetch_industry(self, start_date, end_date, category='sw_adj'):\n",
    "        code_name = 'industryID' + str(1)\n",
    "        category_name = 'industryName' + str(1)\n",
    "        Industry = self._base.classes['industry']\n",
    "        query = select([Industry.trade_date, Industry.code, getattr(Industry, code_name).label('industry_code'),\n",
    "                       getattr(Industry, category_name).label('industry')]).where(\n",
    "            and_(\n",
    "                Industry.trade_date >= start_date,\n",
    "                Industry.trade_date <= end_date,\n",
    "                Industry.industry == '申万行业分类修订'\n",
    "            )\n",
    "        )\n",
    "        return pd.read_sql(query, self._engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_time = '2020-02-14' #模型开始时候\n",
    "start_time = '2020-01-12'\n",
    "end_time = '2020-02-14' # 因子数据时间\n",
    "trade_time = '2020-02-17' # 交易时间\n",
    "weights_bandwidth=0.01\n",
    "method='risk_neutral'\n",
    "turn_over_target = 0.6\n",
    "\n",
    "\n",
    "indexCode = '905'\n",
    "benchmark = 'zz500'\n",
    "uqer_columns = [\"DAVOL10\",\"DAVOL20\",\"DAVOL5\",\"DDNBT\",\"DDNCR\",\"DDNSR\",\"DHILO\",\"DVRAT\",\"EMA10\",\"EMA120\",\"EMA20\",\n",
    "                        \"EMA5\",\"EMA60\",\"HBETA\",\"HSIGMA\",\"MA10\",\"MA120\",\"MA20\",\"MA5\",\"MA60\",\"MAWVAD\",\"PSY\",\"RSTR12\",\n",
    "                        \"RSTR24\",\"VOL10\",\"VOL120\",\"VOL20\",\"VOL240\",\"VOL5\",\"VOL60\",\"WVAD\",\"Skewness\",\"ILLIQUIDITY\",\n",
    "                        \"BackwardADJ\",\"MACD\",\"ADTM\",\"ATR14\",\"BIAS10\",\"BIAS20\",\"BIAS5\",\"BIAS60\",\"BollDown\",\"BollUp\",\n",
    "                        \"CCI10\",\"CCI20\",\"CCI5\",\"CCI88\",\"KDJ_K\",\"KDJ_D\",\"KDJ_J\",\"ROC6\",\"ROC20\",\"SBM\",\"STM\",\"UpRVI\",\n",
    "                        \"DownRVI\",\"RVI\",\"SRMI\",\"ChandeSD\",\"ChandeSU\",\"CMO\",\"DBCD\",\"ARC\",\"OBV\",\"OBV6\",\"OBV20\",\n",
    "                        \"TVMA20\",\"TVSTD20\",\"TVSTD6\",\"VDEA\",\"VDIFF\",\"VEMA10\",\"VEMA12\",\"VEMA26\",\"VEMA5\",\n",
    "                        \"VMACD\",\"VOSC\",\"VR\",\"VROC12\",\"VROC6\",\"VSTD10\",\"VSTD20\",\"KlingerOscillator\",\"MoneyFlow20\",\n",
    "                        \"AD\",\"AD20\",\"AD6\",\"CoppockCurve\",\"ASI\",\"ChaikinOscillator\",\"ChaikinVolatility\",\"EMV14\",\n",
    "                        \"EMV6\",\"plusDI\",\"minusDI\",\"ADX\",\"ADXR\",\"Aroon\",\"AroonDown\",\"AroonUp\",\"DEA\",\"DIFF\",\"DDI\",\n",
    "                        \"DIZ\",\"MTM\",\"MTMMA\",\"PVT\",\"PVT6\",\"PVT12\",\"TRIX5\",\"TRIX10\",\"UOS\",\"MA10RegressCoeff12\",\n",
    "                        \"MA10RegressCoeff6\",\"PLRC6\",\"PLRC12\",\"SwingIndex\",\"Ulcer10\",\"Ulcer5\",\"Hurst\",\"ACD6\",\"ACD20\",\n",
    "                        \"EMA12\",\"EMA26\",\"APBMA\",\"BBI\",\"BBIC\",\"TEMA10\",\"TEMA5\",\"MA10Close\",\"AR\",\"BR\",\"ARBR\",\"CR20\",\n",
    "                        \"MassIndex\",\"BearPower\",\"BullPower\",\"Elder\",\"NVI\",\"PVI\",\"RC12\",\"RC24\",\"JDQS20\"]\n",
    "        \n",
    "experimental_columns= [\"IVR\",\"CHV\",\"vretd_bar15\",\"retd_bar15\",\"vretd_bar5\",\"retd_bar5\",\"abs_vretd\",\"vretd\",\"abs_retd\",\n",
    "                               \"retd\",\"ivr_bar30\",\"ivr_bar60\",\"ivr_day\",\"vhhi_std\",\"vskew_std\",\"vvol_std\",\"rhhi_std\",\"rskew_std\",\n",
    "                               \"rvol_std\",\"vhhi\",\"vkurt\",\"vskew\",\"vvol\",\"rkurt\",\"rskew\",\"rvol\",\"idl_mtm_20\",\"cvvwap\",\"clv\",\"ccv\",\n",
    "                               \"chlv\",\"chlvwap\",\"chlc\",\"ideal_mtm_20\",\"low_mtm_20\",\"high_mtm_20\",\"mix_cap_liq\",\"mix_liq\",\"amh_20\",\n",
    "                               \"amh_10\",\"apm_20\",\"apm_10\",\"pure_cap_liq_4\",\"pure_cap_liq_3\",\"pure_cap_liq_2\",\"pure_cap_liq_1\",\"pure_cap_liq_0\",\n",
    "                               \"cap_liq\",\"pe_hist60\",\"pure_liq_4\",\"pure_liq_3\",\"pure_liq_2\",\"pure_liq_1\",\"pure_liq_0\",\"liq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "512647@wmcloud.com 账号登录成功\n"
     ]
    }
   ],
   "source": [
    "factors_db = FactorsDB()\n",
    "client = uqer.Client(token='07b082b1f42b91987660f0c2c19097bc3b10fa4b12f6af3274f82df930185f04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "univ_df = factors_db.fetch_universes(['zz800','zz500'], start_time, end_time)\n",
    "risk_exposure_df = factors_db.fetch_risk_exposure(start_time, end_time)\n",
    "market_df = factors_db.fetch_market(start_time, end_time)\n",
    "mktbar_raw_df = factors_db.fetch_marketraw(start_time, end_time)\n",
    "index_component_df = factors_db.fetch_index_component(['906','905'],start_time, end_time)\n",
    "uqer_df = factors_db.fetch_uqer(start_time, end_time, ['trade_date','code'],uqer_columns)\n",
    "industry_df = factors_db.fetch_industry(start_time, end_time)\n",
    "\n",
    "risk_cov_df = factors_db.fetch_risk_cov(start_time, end_time)\n",
    "specical_risk_df = factors_db.fetch_special_risk(start_time, end_time)\n",
    "#修改名称\n",
    "rename_dict = {}\n",
    "for col in uqer_columns:\n",
    "    rename_dict[col] = 'uqer_' + str(col)\n",
    "uqer_df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "experimental_df = factors_db.fetch_experimental(start_time, end_time)\n",
    "#修改名称\n",
    "rename_dict = {}\n",
    "for col in experimental_columns:\n",
    "    rename_dict[col] = 'exper_' + str(col)\n",
    "experimental_df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "mkt_df = market_processing(mktbar_raw_df, market_df)\n",
    "\n",
    "factors_df = uqer_df.merge(experimental_df, on=['trade_date','code'])\n",
    "\n",
    "\n",
    "## 行业数据\n",
    "industry_df['industry_name'] = industry_df['industry']\n",
    "industry_se = pd.get_dummies(industry_df, columns=['industry'], prefix=\"\", prefix_sep=\"\").drop(\n",
    "        'industry_code', axis=1)\n",
    "\n",
    "## 风险模型数据\n",
    "risk_exp_df = risk_exposure_df.merge(specical_risk_df, on=['trade_date','code']).dropna()\n",
    "factor_names = risk_cov_df.Factor.tolist()\n",
    "new_risk_cov = risk_cov_df.set_index('Factor')\n",
    "factor_cov = new_risk_cov.loc[factor_names, factor_names] / 10000.\n",
    "new_risk_exp = risk_exp_df.set_index('code')\n",
    "factor_loading = new_risk_exp.loc[:, factor_names]\n",
    "idsync = new_risk_exp['SRISK'] * new_risk_exp['SRISK'] / 10000\n",
    "#FactorRiskModel(factor_cov, factor_loading, idsync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_exposure_se, mkt_se, factors_se = index_processing(univ_df, risk_exposure_df, mkt_df, \n",
    "                                                        factors_df, [benchmark])\n",
    "index_component_se = index_component_df.set_index('indexCode').loc[indexCode].reset_index()\n",
    "mkt_se = returns_processing(mkt_se, horizon=2)\n",
    "factor_sets = factors_se.columns\n",
    "standard_data = data_processing(risk_se = risk_exposure_se, mkt_se = mkt_se,\n",
    "                               factors_se = factors_se, factor_sets = factor_sets,\n",
    "                               is_process=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression = \"\"\"\n",
    "SecurityMaximumValueHolder(SecurityMaximumValueHolder('exper_pure_cap_liq_4','exper_retd'),'exper_vhhi_std')\n",
    "\"\"\"\n",
    "expression_name = 'ultron_1581149987701014'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_data = calc_factor(expression=expression,\n",
    "            total_data=standard_data.copy(), indexs='trade_date', key='code', name=expression_name\n",
    "                      )\n",
    "\n",
    "'''\n",
    "历史回测数据判断方向\n",
    "'''\n",
    "## 基于历史因子判断方向\n",
    "total_data = factors_data.reset_index().merge(mkt_se.reset_index(), on=['trade_date','code']\n",
    "                                ).set_index(\n",
    "    ['trade_date','code'])[[expression_name,'nxt1_ret']].dropna(subset=['nxt1_ret'])\n",
    "#求每期的IC\n",
    "ic_serialize = total_data.groupby('trade_date').apply(lambda x: np.corrcoef(x[expression_name].values,\n",
    "                                                            x.nxt1_ret.values)[0,1])\n",
    "direction = np.sign(ic_serialize.mean())\n",
    "factors_data[expression_name] = direction * factors_data[expression_name]\n",
    "factors_data = factors_data.rename(columns={expression_name:'factor'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##数据合并\n",
    "total_data = pd.merge(factors_data, industry_se, on=['trade_date','code'])\n",
    "total_data = pd.merge(total_data,index_component_se, on=['trade_date','code'], how='left')\n",
    "total_data.fillna({'weight': 0.}, inplace=True)\n",
    "total_data = pd.merge(total_data, mkt_se['nxt1_ret'].reset_index().dropna(subset=['nxt1_ret']), \n",
    "                      on=['trade_date','code'], how='left')\n",
    "#trade_list = total_data.trade_date.unique()\n",
    "#total_data = total_data.set_index('trade_date').loc[:trade_list[-4]].reset_index()\n",
    "total_data = pd.merge(total_data, risk_exp_df, on=['trade_date','code'])\n",
    "is_in_benchmark = (total_data.weight > 0.).astype(float).values.reshape((-1, 1))\n",
    "total_data.loc[:, 'benchmark'] = is_in_benchmark\n",
    "total_data.loc[:, 'total'] = np.ones_like(is_in_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_groups = total_data.set_index('trade_date').loc[end_time].reset_index().groupby('trade_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 构建模型\n",
    "models = {}\n",
    "alpha_model = ConstLinearModel(features=['factor'], weights={'factor': 1.0})\n",
    "for ref_date, _ in total_data_groups:\n",
    "    models[ref_date] = alpha_model\n",
    "alpha_models = models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningSetting(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 lbound=None,\n",
    "                 ubound=None,\n",
    "                 weights_bandwidth=None,\n",
    "                 rebalance_method='risk_neutral',\n",
    "                 bounds=None,\n",
    "                 **kwargs):\n",
    "        self.lbound = lbound\n",
    "        self.ubound = ubound\n",
    "        self.weights_bandwidth = weights_bandwidth\n",
    "        self.executor = NaiveExecutor()\n",
    "        self.rebalance_method = rebalance_method\n",
    "        self.bounds = bounds\n",
    "        self.more_opts = kwargs\n",
    "        \n",
    "industry_name = 'sw_adj'\n",
    "industry_level = 1\n",
    "    \n",
    "industry_names = industry_list(industry_name, industry_level)\n",
    "constraint_risk = industry_names\n",
    "constraint_risk = risk_styles + industry_names\n",
    "total_risk_names = constraint_risk + ['benchmark', 'total']\n",
    "        \n",
    "effective_industry_names = ['建筑材料','机械设备','家用电器','交通工具',\n",
    "                            '化工','电器设备','信息服务','建筑装饰','计算机','轻工制造',\n",
    "                           '交运设备','建筑建材','商业贸易','房地产','汽车','公用事业',\n",
    "                           '保险','休闲服务','证券','多元金融']\n",
    "best_industry_names = ['电子','家用电器','食品饮料','医药生物','通信']\n",
    "invalid_industry_names = ['农林牧渔','采掘','钢铁','有色金属','纺织服装','商业贸易',\n",
    "                          '综合','国防军工','传媒','银行']\n",
    "b_type = []\n",
    "l_val = []\n",
    "u_val = []\n",
    "\n",
    "\n",
    "for name in total_risk_names:\n",
    "    if name == 'benchmark':\n",
    "        b_type.append(BoundaryType.RELATIVE)\n",
    "        l_val.append(0.0)\n",
    "        u_val.append(1.0)\n",
    "    elif name == 'total':\n",
    "        b_type.append(BoundaryType.ABSOLUTE)\n",
    "        l_val.append(-0.0)\n",
    "        u_val.append(0.0)\n",
    "    elif name in effective_industry_names:\n",
    "        b_type.append(BoundaryType.ABSOLUTE)\n",
    "        l_val.append(-0.025)\n",
    "        u_val.append(0.025)\n",
    "    elif name in best_industry_names:\n",
    "        b_type.append(BoundaryType.ABSOLUTE)\n",
    "        l_val.append(-0.065)\n",
    "        u_val.append(0.065)\n",
    "    elif name in invalid_industry_names:\n",
    "        b_type.append(BoundaryType.RELATIVE)\n",
    "        l_val.append(-0.005)\n",
    "        u_val.append(0.005)\n",
    "    else:\n",
    "        b_type.append(BoundaryType.ABSOLUTE)\n",
    "        l_val.append(-0.3)\n",
    "        u_val.append(0.3)\n",
    "bounds = create_box_bounds(total_risk_names, b_type, l_val, u_val)\n",
    "running_setting = RunningSetting(lbound=0., \n",
    "                                 ubound=0.02,\n",
    "                                 weights_bandwidth=weights_bandwidth,\n",
    "                                 rebalance_method=method,\n",
    "                                 bounds=bounds,\n",
    "                                 turn_over_target=turn_over_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_lu_bounds(running_setting, codes, benchmark_w):\n",
    "    codes = np.array(codes)\n",
    "    \n",
    "    if running_setting.weights_bandwidth:\n",
    "        lbound = np.maximum(0., benchmark_w - running_setting.weights_bandwidth)\n",
    "        ubound = running_setting.weights_bandwidth + benchmark_w\n",
    "\n",
    "    lb = running_setting.lbound\n",
    "    ub = running_setting.ubound\n",
    "\n",
    "    if lb or ub:\n",
    "        if not isinstance(lb, dict):\n",
    "            lbound = np.ones_like(benchmark_w) * lb\n",
    "        else:\n",
    "            lbound = np.zeros_like(benchmark_w)\n",
    "            for c in lb:\n",
    "                lbound[codes == c] = lb[c]\n",
    "\n",
    "            if 'other' in lb:\n",
    "                for i, c in enumerate(codes):\n",
    "                    if c not in lb:\n",
    "                        lbound[i] = lb['other']\n",
    "        if not isinstance(ub, dict):\n",
    "            ubound = np.ones_like(benchmark_w) * ub\n",
    "        else:\n",
    "            ubound = np.ones_like(benchmark_w)\n",
    "            for c in ub:\n",
    "                ubound[codes == c] = ub[c]\n",
    "\n",
    "            if 'other' in ub:\n",
    "                for i, c in enumerate(codes):\n",
    "                    if c not in ub:\n",
    "                        ubound[i] = ub['other']\n",
    "    return lbound, ubound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_pos = pd.DataFrame()\n",
    "positions = pd.DataFrame()\n",
    "target_position = []\n",
    "for ref_date, this_data in total_data_groups:\n",
    "    more_opts = running_setting.more_opts\n",
    "    new_model = alpha_models[ref_date]\n",
    "    codes = this_data.code.values.tolist()\n",
    "    if previous_pos.empty:\n",
    "        current_position = None\n",
    "    else:\n",
    "        previous_pos.set_index('code', inplace=True)\n",
    "        remained_pos = previous_pos.reindex(codes)\n",
    "        remained_pos.fillna(0., inplace=True)\n",
    "        current_position = remained_pos.weight.values\n",
    "    benchmark_w = this_data.weight.values\n",
    "    constraints = LinearConstraints(running_setting.bounds,\n",
    "                                            this_data,\n",
    "                                            benchmark_w)\n",
    "    lbound, ubound = _create_lu_bounds(running_setting, codes, benchmark_w)\n",
    "    this_data.fillna(0, inplace=True)\n",
    "    new_factors = factor_processing(this_data[new_model.features].values,\n",
    "                                    pre_process=[winsorize_normal, standardize])\n",
    "    new_factors = pd.DataFrame(new_factors, columns=['factor'], index=codes)\n",
    "    er = new_model.predict(new_factors).astype(float)\n",
    "    target_pos, _ = er_portfolio_analysis(er=er,industry=this_data.industry_name.values,\n",
    "                         dx_return=None,constraints=constraints,\n",
    "                         detail_analysis=False,benchmark=benchmark_w,\n",
    "                         method=running_setting.rebalance_method,\n",
    "                         lbound=lbound,ubound=ubound,current_position=current_position,\n",
    "                         target_vol=more_opts.get('target_vol'),\n",
    "                         risk_model=None,\n",
    "                         turn_over_target=more_opts.get('turn_over_target'))\n",
    "    target_pos['code'] = codes\n",
    "    target_pos['trade_date'] = ref_date\n",
    "    target_position.append(target_pos)\n",
    "    previous_pos = target_pos\n",
    "target_position = pd.concat(target_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target_position[target_position.weight.abs() > 0.0015].set_index('trade_date').loc[end_time]\n",
    "target['exchange'] = target['code'].apply(lambda x: 'XSHG' if \\\n",
    "                                      len(str(x))==6 and str(x)[0] in '6' else  'XSHE')\n",
    "target['code'] = target['code'].apply(lambda x: \"{:06d}\".format(x) + '.XSHG' if \\\n",
    "                                      len(str(x))==6 and str(x)[0] in '6' else \"{:06d}\".format(x)\\\n",
    "                                        + '.XSHE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uqer_market(univ, exchange, ref_date):\n",
    "    return DataAPI.SHSZBarHistOneDayGet(tradeDate=ref_date,exchangeCD=exchange,\n",
    "                             ticker=univ,unit=\"30\",\n",
    "                             startTime=u\"10:00\",endTime=u\"10:00\",\n",
    "                             field=u\"\",pandas=\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_codes = target[target['exchange'] == 'XSHE'].code.values.tolist()\n",
    "sh_codes = target[target['exchange'] == 'XSHG'].code.values.tolist()\n",
    "mkt_data = pd.concat([uqer_market(sz_codes, 'XSHE', '20200217'),uqer_market(sh_codes, 'XSHG', '20200217')])\n",
    "mkt_data['ticker'] = mkt_data['ticker'].apply(lambda x: str(x) + '.XSHG' if \\\n",
    "                                      len(str(x))==6 and str(x)[0] in '6' else str(x)\\\n",
    "                                        + '.XSHE')\n",
    "mkt_data = mkt_data.rename(columns={'ticker':'code'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_data = industry_df.set_index('trade_date').loc[end_time]\n",
    "industry_data['code'] = industry_data['code'].apply(\n",
    "                            lambda x: \"{:06d}\".format(x) + '.XSHG' if len(str(x))==6 and str(x)[0] in '6' else \"{:06d}\".format(x)\\\n",
    "                            + '.XSHE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trader = mkt_data.merge(target,on=['code'])[['vwap','weight','code','barTime','closePrice','shortNM']].merge(\n",
    "industry_data.reset_index()[['code','industry_name']], on=['code'])\n",
    "trader['cost'] = 10000000 * trader['weight']\n",
    "trader['count'] = trader['cost'] / trader['vwap']\n",
    "trader['count'] = (trader['count']/ 100).astype('int') * 100\n",
    "trader['fee'] = trader['count'] * trader['vwap'] * 0.001\n",
    "trader['operation'] = '买入'\n",
    "trader['trade_date'] = '2020-02-17'\n",
    "trader['profit'] = (trader['closePrice'] - trader['vwap']) * trader['count'] - trader['fee']\n",
    "trader = trader[['trade_date','code','shortNM','industry_name','operation','count','vwap','closePrice','fee','barTime','profit']].rename(columns={\n",
    "    'trade_date':'成交日期','barTime':'成交时间','code':'证券代码','operation':'操作类型',\n",
    "    'count':'成交数量','vwap':'成交价格','fee':'佣金','shortNM':'名称','profit':'收益',\n",
    "    'closePrice':'收盘价','industry_name':'行业'\n",
    "}).to_csv(trade_time + '_' + expression_name + '.csv', encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
