{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "* 20181225: \n",
    "    * Predict stock price in next day using XGBoost\n",
    "    * Given prices and other features for the last N days, we do prediction for day N+1\n",
    "    * Here we split 3 years of data into train(60%), dev(20%) and test(20%)\n",
    "* 20190110 - Diff from StockPricePrediction_v1_xgboost.ipynb:\n",
    "    * Here we scale the train set to have mean 0 and variance 1, and apply the same transformation to dev and test sets\n",
    "* 20190111 - Diff from StockPricePrediction_v1a_xgboost.ipynb:\n",
    "    * Here for the past N values for the dev set, we scale them to have mean 0 and variance 1, and do prediction on them\n",
    "* 20190112 - Diff from StockPricePrediction_v1b_xgboost.ipynb:\n",
    "    * Instead of using the same mean and variance to do scaling for train, dev and test sets, we scale the train set to have mean 0 and var 1, and then whenever we do prediction on dev or test set we scale the previous N values to also have mean 0 and var 1 (ie. use the means and variances of the previous N values to do scaling. We do this for both feature columns and target columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from datetime import date\n",
    "from matplotlib import pyplot as plt\n",
    "from pylab import rcParams\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm_notebook\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#### Input params ##################\n",
    "stk_path = \"./data/VTI.csv\"\n",
    "test_size = 0.2                # proportion of dataset to be used as test set\n",
    "cv_size = 0.2                  # proportion of dataset to be used as cross-validation set\n",
    "N = 3                         # for feature at day t, we use lags from t-1, t-2, ..., t-N as features\n",
    "\n",
    "n_estimators = 100             # Number of boosted trees to fit. default = 100\n",
    "max_depth = 3                  # Maximum tree depth for base learners. default = 3\n",
    "learning_rate = 0.1            # Boosting learning rate (xgb’s “eta”). default = 0.1\n",
    "min_child_weight = 1           # Minimum sum of instance weight(hessian) needed in a child. default = 1\n",
    "subsample = 1                  # Subsample ratio of the training instance. default = 1\n",
    "colsample_bytree = 1           # Subsample ratio of columns when constructing each tree. default = 1\n",
    "colsample_bylevel = 1          # Subsample ratio of columns for each split, in each level. default = 1\n",
    "gamma = 0                      # Minimum loss reduction required to make a further partition on a leaf node of the tree. default=0\n",
    "\n",
    "model_seed = 100\n",
    "\n",
    "fontsize = 14\n",
    "ticklabelsize = 14\n",
    "####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_mov_avg_std(df, col, N):\n",
    "    \"\"\"\n",
    "    Given a dataframe, get mean and std dev at timestep t using values from t-1, t-2, ..., t-N.\n",
    "    Inputs\n",
    "        df         : dataframe. Can be of any length.\n",
    "        col        : name of the column you want to calculate mean and std dev\n",
    "        N          : get mean and std dev at timestep t using values from t-1, t-2, ..., t-N\n",
    "    Outputs\n",
    "        df_out     : same as df but with additional column containing mean and std dev\n",
    "    \"\"\"\n",
    "    mean_list = df[col].rolling(window = N, min_periods=1).mean() # len(mean_list) = len(df)\n",
    "    std_list = df[col].rolling(window = N, min_periods=1).std()   # first value will be NaN, because normalized by N-1\n",
    "    \n",
    "    # Add one timestep to the predictions\n",
    "    mean_list = np.concatenate((np.array([np.nan]), np.array(mean_list[:-1])))\n",
    "    std_list = np.concatenate((np.array([np.nan]), np.array(std_list[:-1])))\n",
    "    \n",
    "    # Append mean_list to df\n",
    "    df_out = df.copy()\n",
    "    df_out[col + '_mean'] = mean_list\n",
    "    df_out[col + '_std'] = std_list\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "def scale_row(row, feat_mean, feat_std):\n",
    "    \"\"\"\n",
    "    Given a pandas series in row, scale it to have 0 mean and var 1 using feat_mean and feat_std\n",
    "    Inputs\n",
    "        row      : pandas series. Need to scale this.\n",
    "        feat_mean: mean  \n",
    "        feat_std : standard deviation\n",
    "    Outputs\n",
    "        row_scaled : pandas series with same length as row, but scaled\n",
    "    \"\"\"\n",
    "    # If feat_std = 0 (this happens if adj_close doesn't change over N days), \n",
    "    # set it to a small number to avoid division by zero\n",
    "    feat_std = 0.001 if feat_std == 0 else feat_std\n",
    "    \n",
    "    row_scaled = (row-feat_mean) / feat_std\n",
    "    \n",
    "    return row_scaled\n",
    "\n",
    "def get_mape(y_true, y_pred): \n",
    "    \"\"\"\n",
    "    Compute mean absolute percentage error (MAPE)\n",
    "    \"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def train_pred_eval_model(X_train_scaled, \\\n",
    "                          y_train_scaled, \\\n",
    "                          X_test_scaled, \\\n",
    "                          y_test, \\\n",
    "                          col_mean, \\\n",
    "                          col_std, \\\n",
    "                          seed=100, \\\n",
    "                          n_estimators=100, \\\n",
    "                          max_depth=3, \\\n",
    "                          learning_rate=0.1, \\\n",
    "                          min_child_weight=1, \\\n",
    "                          subsample=1, \\\n",
    "                          colsample_bytree=1, \\\n",
    "                          colsample_bylevel=1, \\\n",
    "                          gamma=0):\n",
    "    '''\n",
    "    Train model, do prediction, scale back to original range and do evaluation\n",
    "    Use XGBoost here.\n",
    "    Inputs\n",
    "        X_train_scaled     : features for training. Scaled to have mean 0 and variance 1\n",
    "        y_train_scaled     : target for training. Scaled to have mean 0 and variance 1\n",
    "        X_test_scaled      : features for test. Each sample is scaled to mean 0 and variance 1\n",
    "        y_test             : target for test. Actual values, not scaled.\n",
    "        col_mean           : means used to scale each sample of X_test_scaled. Same length as X_test_scaled and y_test\n",
    "        col_std            : standard deviations used to scale each sample of X_test_scaled. Same length as X_test_scaled and y_test\n",
    "        seed               : model seed\n",
    "        n_estimators       : number of boosted trees to fit\n",
    "        max_depth          : maximum tree depth for base learners\n",
    "        learning_rate      : boosting learning rate (xgb’s “eta”)\n",
    "        min_child_weight   : minimum sum of instance weight(hessian) needed in a child\n",
    "        subsample          : subsample ratio of the training instance\n",
    "        colsample_bytree   : subsample ratio of columns when constructing each tree\n",
    "        colsample_bylevel  : subsample ratio of columns for each split, in each level\n",
    "        gamma              : \n",
    "    Outputs\n",
    "        rmse               : root mean square error of y_test and est\n",
    "        mape               : mean absolute percentage error of y_test and est\n",
    "        est                : predicted values. Same length as y_test\n",
    "    '''\n",
    "\n",
    "    model = XGBRegressor(seed=model_seed,\n",
    "                         n_estimators=n_estimators,\n",
    "                         max_depth=max_depth,\n",
    "                         learning_rate=learning_rate,\n",
    "                         min_child_weight=min_child_weight,\n",
    "                         subsample=subsample,\n",
    "                         colsample_bytree=colsample_bytree,\n",
    "                         colsample_bylevel=colsample_bylevel,\n",
    "                         gamma=gamma)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train_scaled)\n",
    "    \n",
    "    # Get predicted labels and scale back to original range\n",
    "    est_scaled = model.predict(X_test_scaled)\n",
    "    est = est_scaled * col_std + col_mean\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = math.sqrt(mean_squared_error(y_test, est))\n",
    "    mape = get_mape(y_test, est)\n",
    "    \n",
    "    return rmse, mape, est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-e53add1b6720>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstk_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Convert Date column to datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%Y-%m-%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workshop/virtualenv/venv3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workshop/virtualenv/venv3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workshop/virtualenv/venv3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workshop/virtualenv/venv3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workshop/virtualenv/venv3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'./data/VTI.csv' does not exist"
     ],
     "ename": "FileNotFoundError",
     "evalue": "File b'./data/VTI.csv' does not exist",
     "output_type": "error"
    }
   ],
   "source": [
    "df = pd.read_csv(stk_path, sep = \",\")\n",
    "\n",
    "# Convert Date column to datetime\n",
    "df.loc[:, 'Date'] = pd.to_datetime(df['Date'],format='%Y-%m-%d')\n",
    "\n",
    "# Change all column headings to be lower case, and remove spacing\n",
    "df.columns = [str(x).lower().replace(' ', '_') for x in df.columns]\n",
    "\n",
    "# Get month of each sample\n",
    "df['month'] = df['date'].dt.month\n",
    "\n",
    "# Sort by datetime\n",
    "df.sort_values(by='date', inplace=True, ascending=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot adjusted close over time\n",
    "rcParams['figure.figsize'] = 10, 8 # width 10, height 8\n",
    "\n",
    "ax = df.plot(x='date', y='adj_close', style='b-', grid=True)\n",
    "ax.set_xlabel(\"date\")\n",
    "ax.set_ylabel(\"USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We will generate the following features:\n",
    "* Mean 'adj_close' of each month\n",
    "* Difference between high and low of each day\n",
    "* Difference between open and close of each day\n",
    "* Mean volume of each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Get difference between high and low of each day\n",
    "df['range_hl'] = df['high'] - df['low']\n",
    "df.drop(['high', 'low'], axis=1, inplace=True)\n",
    "\n",
    "# Get difference between open and close of each day\n",
    "df['range_oc'] = df['open'] - df['close']\n",
    "df.drop(['open', 'close'], axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use lags up to N number of days to use as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Add a column 'order_day' to indicate the order of the rows by date\n",
    "df['order_day'] = [x for x in list(range(len(df)))]\n",
    "\n",
    "# merging_keys\n",
    "merging_keys = ['order_day']\n",
    "\n",
    "# List of columns that we will use to create lags\n",
    "lag_cols = ['adj_close', 'range_hl', 'range_oc', 'volume']\n",
    "lag_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "shift_range = [x+1 for x in range(N)]\n",
    "\n",
    "for shift in tqdm_notebook(shift_range):\n",
    "    train_shift = df[merging_keys + lag_cols].copy()\n",
    "    \n",
    "    # E.g. order_day of 0 becomes 1, for shift = 1.\n",
    "    # So when this is merged with order_day of 1 in df, this will represent lag of 1.\n",
    "    train_shift['order_day'] = train_shift['order_day'] + shift\n",
    "    \n",
    "    foo = lambda x: '{}_lag_{}'.format(x, shift) if x in lag_cols else x\n",
    "    train_shift = train_shift.rename(columns=foo)\n",
    "\n",
    "    df = pd.merge(df, train_shift, on=merging_keys, how='left') #.fillna(0)\n",
    "    \n",
    "del train_shift\n",
    "\n",
    "# Remove the first N rows which contain NaNs\n",
    "df = df[N:]\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# # Get mean of adj_close of each month\n",
    "# df_gb = df.groupby(['month'], as_index=False).agg({'adj_close':'mean'})\n",
    "# df_gb = df_gb.rename(columns={'adj_close':'adj_close_mean'})\n",
    "# df_gb\n",
    "\n",
    "# # Merge to main df\n",
    "# df = df.merge(df_gb, \n",
    "#               left_on=['month'], \n",
    "#               right_on=['month'],\n",
    "#               how='left').fillna(0)\n",
    "\n",
    "# # Merge to main df\n",
    "# shift_range = [x+1 for x in range(2)]\n",
    "\n",
    "# for shift in tqdm_notebook(shift_range):\n",
    "#     train_shift = df[merging_keys + lag_cols].copy()\n",
    "    \n",
    "#     # E.g. order_day of 0 becomes 1, for shift = 1.\n",
    "#     # So when this is merged with order_day of 1 in df, this will represent lag of 1.\n",
    "#     train_shift['order_day'] = train_shift['order_day'] + shift\n",
    "    \n",
    "#     foo = lambda x: '{}_lag_{}'.format(x, shift) if x in lag_cols else x\n",
    "#     train_shift = train_shift.rename(columns=foo)\n",
    "\n",
    "#     df = pd.merge(df, train_shift, on=merging_keys, how='left') #.fillna(0)\n",
    "    \n",
    "# del train_shift\n",
    "    \n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# # Get mean of volume of each month\n",
    "# df_gb = df.groupby(['month'], as_index=False).agg({'volume':'mean'})\n",
    "# df_gb = df_gb.rename(columns={'volume':'volume_mean'})\n",
    "# df_gb\n",
    "\n",
    "# # Merge to main df\n",
    "# df = df.merge(df_gb, \n",
    "#               left_on=['month'], \n",
    "#               right_on=['month'],\n",
    "#               how='left').fillna(0)\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get mean and std dev at timestamp t using values from t-1, ..., t-N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "cols_list = [\n",
    "\"adj_close\",\n",
    "\"range_hl\",\n",
    "\"range_oc\",\n",
    "\"volume\"\n",
    "]\n",
    "\n",
    "for col in cols_list:\n",
    "    df = get_mov_avg_std(df, col, N)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into train, dev and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Get sizes of each of the datasets\n",
    "num_cv = int(cv_size*len(df))\n",
    "num_test = int(test_size*len(df))\n",
    "num_train = len(df) - num_cv - num_test\n",
    "print(\"num_train = \" + str(num_train))\n",
    "print(\"num_cv = \" + str(num_cv))\n",
    "print(\"num_test = \" + str(num_test))\n",
    "\n",
    "# Split into train, cv, and test\n",
    "train = df[:num_train]\n",
    "cv = df[num_train:num_train+num_cv]\n",
    "train_cv = df[:num_train+num_cv]\n",
    "test = df[num_train+num_cv:]\n",
    "print(\"train.shape = \" + str(train.shape))\n",
    "print(\"cv.shape = \" + str(cv.shape))\n",
    "print(\"train_cv.shape = \" + str(train_cv.shape))\n",
    "print(\"test.shape = \" + str(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale the train, dev and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "cols_to_scale = [\n",
    "\"adj_close\"\n",
    "]\n",
    "\n",
    "for i in range(1,N+1):\n",
    "    cols_to_scale.append(\"adj_close_lag_\"+str(i))\n",
    "    cols_to_scale.append(\"range_hl_lag_\"+str(i))\n",
    "    cols_to_scale.append(\"range_oc_lag_\"+str(i))\n",
    "    cols_to_scale.append(\"volume_lag_\"+str(i))\n",
    "\n",
    "# Do scaling for train set\n",
    "# Here we only scale the train dataset, and not the entire dataset to prevent information leak\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train[cols_to_scale])\n",
    "print(\"scaler.mean_ = \" + str(scaler.mean_))\n",
    "print(\"scaler.var_ = \" + str(scaler.var_))\n",
    "print(\"train_scaled.shape = \" + str(train_scaled.shape))\n",
    "\n",
    "# Convert the numpy array back into pandas dataframe\n",
    "train_scaled = pd.DataFrame(train_scaled, columns=cols_to_scale)\n",
    "train_scaled[['date', 'month']] = train.reset_index()[['date', 'month']]\n",
    "print(\"train_scaled.shape = \" + str(train_scaled.shape))\n",
    "train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Do scaling for train+dev set\n",
    "scaler_train_cv = StandardScaler()\n",
    "train_cv_scaled = scaler_train_cv.fit_transform(train_cv[cols_to_scale])\n",
    "print(\"scaler_train_cv.mean_ = \" + str(scaler_train_cv.mean_))\n",
    "print(\"scaler_train_cv.var_ = \" + str(scaler_train_cv.var_))\n",
    "print(\"train_cv_scaled.shape = \" + str(train_cv_scaled.shape))\n",
    "\n",
    "# Convert the numpy array back into pandas dataframe\n",
    "train_cv_scaled = pd.DataFrame(train_cv_scaled, columns=cols_to_scale)\n",
    "train_cv_scaled[['date', 'month']] = train_cv.reset_index()[['date', 'month']]\n",
    "print(\"train_cv_scaled.shape = \" + str(train_cv_scaled.shape))\n",
    "train_cv_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Do scaling for dev set\n",
    "cv_scaled = cv[['date']]\n",
    "for col in tqdm_notebook(cols_list):\n",
    "    feat_list = [col + '_lag_' + str(shift) for shift in range(1, N+1)]\n",
    "    temp = cv.apply(lambda row: scale_row(row[feat_list], row[col+'_mean'], row[col+'_std']), axis=1)\n",
    "    cv_scaled = pd.concat([cv_scaled, temp], axis=1)\n",
    "    \n",
    "# Now the entire dev set is scaled\n",
    "cv_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Do scaling for test set\n",
    "test_scaled = test[['date']]\n",
    "for col in tqdm_notebook(cols_list):\n",
    "    feat_list = [col + '_lag_' + str(shift) for shift in range(1, N+1)]\n",
    "    temp = test.apply(lambda row: scale_row(row[feat_list], row[col+'_mean'], row[col+'_std']), axis=1)\n",
    "    test_scaled = pd.concat([test_scaled, temp], axis=1)\n",
    "    \n",
    "# Now the entire test set is scaled\n",
    "test_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "features = []\n",
    "for i in range(1,N+1):\n",
    "    features.append(\"adj_close_lag_\"+str(i))\n",
    "    features.append(\"range_hl_lag_\"+str(i))\n",
    "    features.append(\"range_oc_lag_\"+str(i))\n",
    "    features.append(\"volume_lag_\"+str(i))\n",
    "\n",
    "target = \"adj_close\"\n",
    "\n",
    "\n",
    "# Split into X and y\n",
    "X_train = train[features]\n",
    "y_train = train[target]\n",
    "X_cv = cv[features]\n",
    "y_cv = cv[target]\n",
    "X_train_cv = train_cv[features]\n",
    "y_train_cv = train_cv[target]\n",
    "X_sample = test[features]\n",
    "y_sample = test[target]\n",
    "print(\"X_train.shape = \" + str(X_train.shape))\n",
    "print(\"y_train.shape = \" + str(y_train.shape))\n",
    "print(\"X_cv.shape = \" + str(X_cv.shape))\n",
    "print(\"y_cv.shape = \" + str(y_cv.shape))\n",
    "print(\"X_train_cv.shape = \" + str(X_train_cv.shape))\n",
    "print(\"y_train_cv.shape = \" + str(y_train_cv.shape))\n",
    "print(\"X_sample.shape = \" + str(X_sample.shape))\n",
    "print(\"y_sample.shape = \" + str(y_sample.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Split into X and y\n",
    "X_train_scaled = train_scaled[features]\n",
    "y_train_scaled = train_scaled[target]\n",
    "X_cv_scaled = cv_scaled[features]\n",
    "X_train_cv_scaled = train_cv_scaled[features]\n",
    "y_train_cv_scaled = train_cv_scaled[target]\n",
    "X_sample_scaled = test_scaled[features]\n",
    "print(\"X_train_scaled.shape = \" + str(X_train_scaled.shape))\n",
    "print(\"y_train_scaled.shape = \" + str(y_train_scaled.shape))\n",
    "print(\"X_cv_scaled.shape = \" + str(X_cv_scaled.shape))\n",
    "print(\"X_train_cv_scaled.shape = \" + str(X_train_cv_scaled.shape))\n",
    "print(\"y_train_cv_scaled.shape = \" + str(y_train_cv_scaled.shape))\n",
    "print(\"X_sample_scaled.shape = \" + str(X_sample_scaled.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot adjusted close over time\n",
    "rcParams['figure.figsize'] = 10, 8 # width 10, height 8\n",
    "\n",
    "ax = train.plot(x='date', y='adj_close', style='b-', grid=True)\n",
    "ax = cv.plot(x='date', y='adj_close', style='y-', grid=True, ax=ax)\n",
    "ax = test.plot(x='date', y='adj_close', style='g-', grid=True, ax=ax)\n",
    "ax.legend(['train', 'dev', 'test'])\n",
    "ax.set_xlabel(\"date\")\n",
    "ax.set_ylabel(\"USD\")\n",
    "ax.set_title(\"Without scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot adjusted close over time\n",
    "rcParams['figure.figsize'] = 10, 8 # width 10, height 8\n",
    "\n",
    "ax = train_scaled.plot(x='date', y='adj_close', style='b-', grid=True)\n",
    "ax.legend(['train_scaled'])\n",
    "ax.set_xlabel(\"date\")\n",
    "ax.set_ylabel(\"USD (scaled)\")\n",
    "ax.set_title(\"With scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train the model using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = XGBRegressor(seed=model_seed,\n",
    "                     n_estimators=n_estimators,\n",
    "                     max_depth=max_depth,\n",
    "                     learning_rate=learning_rate,\n",
    "                     min_child_weight=min_child_weight,\n",
    "                     subsample=subsample,\n",
    "                     colsample_bytree=colsample_bytree,\n",
    "                     colsample_bylevel=colsample_bylevel,\n",
    "                     gamma=gamma)\n",
    "\n",
    "# Train the regressor\n",
    "model.fit(X_train_scaled, y_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Do prediction on train set\n",
    "est_scaled = model.predict(X_train_scaled)\n",
    "est = est_scaled * math.sqrt(scaler.var_[0]) + scaler.mean_[0]\n",
    "\n",
    "# Calculate RMSE\n",
    "print(\"RMSE on train set = %0.3f\" % math.sqrt(mean_squared_error(y_train, est)))\n",
    "\n",
    "# Calculate MAPE\n",
    "print(\"MAPE on train set = %0.3f%%\" % get_mape(y_train, est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot adjusted close over time\n",
    "rcParams['figure.figsize'] = 10, 8 # width 10, height 8\n",
    "\n",
    "est_df = pd.DataFrame({'est': est, \n",
    "                       'date': train['date']})\n",
    "\n",
    "ax = train.plot(x='date', y='adj_close', style='b-', grid=True)\n",
    "ax = cv.plot(x='date', y='adj_close', style='y-', grid=True, ax=ax)\n",
    "ax = test.plot(x='date', y='adj_close', style='g-', grid=True, ax=ax)\n",
    "ax = est_df.plot(x='date', y='est', style='r-', grid=True, ax=ax)\n",
    "ax.legend(['train', 'dev', 'test', 'predictions'])\n",
    "ax.set_xlabel(\"date\")\n",
    "ax.set_ylabel(\"USD\")\n",
    "ax.set_title('Without scaling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Do prediction on test set\n",
    "est_scaled = model.predict(X_cv_scaled)\n",
    "cv['est_scaled'] = est_scaled\n",
    "cv['est'] = cv['est_scaled'] * cv['adj_close_std'] + cv['adj_close_mean']\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_bef_tuning = math.sqrt(mean_squared_error(y_cv, cv['est']))\n",
    "print(\"RMSE on dev set = %0.3f\" % rmse_bef_tuning)\n",
    "\n",
    "# Calculate MAPE\n",
    "mape_bef_tuning = get_mape(y_cv, cv['est'])\n",
    "print(\"MAPE on dev set = %0.3f%%\" % mape_bef_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot adjusted close over time\n",
    "rcParams['figure.figsize'] = 10, 8 # width 10, height 8\n",
    "\n",
    "est_df = pd.DataFrame({'est': cv['est'], \n",
    "                       'y_cv': y_cv,\n",
    "                       'date': cv['date']})\n",
    "\n",
    "ax = train.plot(x='date', y='adj_close', style='b-', grid=True)\n",
    "ax = cv.plot(x='date', y='adj_close', style='y-', grid=True, ax=ax)\n",
    "ax = test.plot(x='date', y='adj_close', style='g-', grid=True, ax=ax)\n",
    "ax = est_df.plot(x='date', y='est', style='r-', grid=True, ax=ax)\n",
    "ax.legend(['train', 'dev', 'test', 'predictions'])\n",
    "ax.set_xlabel(\"date\")\n",
    "ax.set_ylabel(\"USD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot adjusted close over time, for dev set only\n",
    "rcParams['figure.figsize'] = 10, 8 # width 10, height 8\n",
    "ax = train.plot(x='date', y='adj_close', style='b-', grid=True)\n",
    "ax = cv.plot(x='date', y='adj_close', style='y-', grid=True, ax=ax)\n",
    "ax = test.plot(x='date', y='adj_close', style='g-', grid=True, ax=ax)\n",
    "ax = est_df.plot(x='date', y='est', style='r-', grid=True, ax=ax)\n",
    "ax.legend(['train', 'dev', 'test', 'predictions'])\n",
    "ax.set_xlabel(\"date\")\n",
    "ax.set_ylabel(\"USD\")\n",
    "ax.set_xlim([date(2017, 8, 1), date(2018, 5, 31)])\n",
    "ax.set_title(\"Zoom in to dev set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions capture the turn in directions with a slight lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# View a list of the features and their importance scores\n",
    "imp = list(zip(train[features], model.feature_importances_))\n",
    "imp.sort(key=lambda tup: tup[1]) \n",
    "imp[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance features dominated by adj_close and volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning N (no. of days to use as features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "d = {'N': [2, 3, 4, 5, 6, 7, 14],\n",
    "     'rmse_dev_set': [1.225, 1.214, 1.231, 1.249, 1.254, 1.251, 1.498],\n",
    "     'mape_pct_dev_set': [0.585, 0.581, 0.590, 0.601, 0.609, 0.612, 0.763]}\n",
    "pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use N = 3 for lowest RMSE and MAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning XGBoost - n_estimators (default=100) and max_depth (default=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "param_label = 'n_estimators'\n",
    "param_list = range(10, 310, 10)\n",
    "\n",
    "param2_label = 'max_depth'\n",
    "param2_list = [2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "error_rate = {param_label: [] , param2_label: [], 'rmse': [], 'mape_pct': []}\n",
    "tic = time.time()\n",
    "for param in tqdm_notebook(param_list):\n",
    "#     print(\"param = \" + str(param))\n",
    "    \n",
    "    for param2 in param2_list:\n",
    "        # Train, predict and eval model\n",
    "        rmse, mape, _ = train_pred_eval_model(X_train_scaled, \n",
    "                                     y_train_scaled, \n",
    "                                     X_cv_scaled, \n",
    "                                     y_cv, \n",
    "                                     cv['adj_close_mean'],\n",
    "                                     cv['adj_close_std'],\n",
    "                                     seed=model_seed,\n",
    "                                     n_estimators=param, \n",
    "                                     max_depth=param2, \n",
    "                                     learning_rate=learning_rate, \n",
    "                                     min_child_weight=min_child_weight, \n",
    "                                     subsample=subsample, \n",
    "                                     colsample_bytree=colsample_bytree, \n",
    "                                     colsample_bylevel=colsample_bylevel, \n",
    "                                     gamma=gamma)\n",
    "    \n",
    "        # Collect results\n",
    "        error_rate[param_label].append(param)\n",
    "        error_rate[param2_label].append(param2)\n",
    "        error_rate['rmse'].append(rmse)\n",
    "        error_rate['mape_pct'].append(mape)\n",
    "    \n",
    "error_rate = pd.DataFrame(error_rate)\n",
    "toc = time.time()\n",
    "print(\"Minutes taken = \" + str((toc-tic)/60.0))\n",
    "error_rate   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot performance versus params\n",
    "rcParams['figure.figsize'] = 10, 8 # width 10, height 8\n",
    "temp = error_rate[error_rate[param2_label]==param2_list[0]]\n",
    "ax = temp.plot(x=param_label, y='rmse', style='bs-', grid=True)\n",
    "legend_list = [param2_label + '_' + str(param2_list[0])]\n",
    "\n",
    "color_list = ['r', 'g', 'k', 'y', 'm', 'c', '0.75']\n",
    "for i in range(1,len(param2_list)):\n",
    "    temp = error_rate[error_rate[param2_label]==param2_list[i]]\n",
    "    ax = temp.plot(x=param_label, y='rmse', color=color_list[i%len(color_list)], marker='s', grid=True, ax=ax)\n",
    "    legend_list.append(param2_label + '_' + str(param2_list[i]))\n",
    "\n",
    "ax.set_xlabel(param_label)\n",
    "ax.set_ylabel(\"RMSE\")\n",
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "plt.legend(legend_list, loc='center left', bbox_to_anchor=(1.0, 0.5)) # positions legend outside figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Get optimum value for param and param2\n",
    "temp = error_rate[error_rate['rmse'] == error_rate['rmse'].min()]\n",
    "n_estimators_opt = temp['n_estimators'].values[0]\n",
    "max_depth_opt = temp['max_depth'].values[0]\n",
    "print(\"min RMSE = %0.3f\" % error_rate['rmse'].min())\n",
    "print(\"optimum params = \")\n",
    "n_estimators_opt, max_depth_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Get optimum value for param and param2, using MAPE\n",
    "temp = error_rate[error_rate['mape_pct'] == error_rate['mape_pct'].min()]\n",
    "print(\"min MAPE = %0.3f%%\" % error_rate['mape_pct'].min())\n",
    "print(\"optimum params = \")\n",
    "temp['n_estimators'].values[0], temp['max_depth'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning XGBoost - learning_rate(default=0.1) and min_child_weight(default=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "param_label = 'learning_rate'\n",
    "param_list = [0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "\n",
    "param2_label = 'min_child_weight'\n",
    "param2_list = range(5, 21, 1)\n",
    "\n",
    "error_rate = {param_label: [] , param2_label: [], 'rmse': [], 'mape_pct': []}\n",
    "tic = time.time()\n",
    "for param in tqdm_notebook(param_list):\n",
    "#     print(\"param = \" + str(param))\n",
    "    \n",
    "    for param2 in param2_list:\n",
    "        # Train, predict and eval model\n",
    "        rmse, mape, _ = train_pred_eval_model(X_train_scaled, \n",
    "                                     y_train_scaled, \n",
    "                                     X_cv_scaled, \n",
    "                                     y_cv, \n",
    "                                     cv['adj_close_mean'],\n",
    "                                     cv['adj_close_std'],\n",
    "                                     seed=model_seed,\n",
    "                                     n_estimators=n_estimators_opt, \n",
    "                                     max_depth=max_depth_opt, \n",
    "                                     learning_rate=param, \n",
    "                                     min_child_weight=param2, \n",
    "                                     subsample=subsample, \n",
    "                                     colsample_bytree=colsample_bytree, \n",
    "                                     colsample_bylevel=colsample_bylevel, \n",
    "                                     gamma=gamma)\n",
    "    \n",
    "        # Collect results\n",
    "        error_rate[param_label].append(param)\n",
    "        error_rate[param2_label].append(param2)\n",
    "        error_rate['rmse'].append(rmse)\n",
    "        error_rate['mape_pct'].append(mape)\n",
    "    \n",
    "error_rate = pd.DataFrame(error_rate)\n",
    "toc = time.time()\n",
    "print(\"Minutes taken = \" + str((toc-tic)/60.0))\n",
    "error_rate   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot performance versus params\n",
    "rcParams['figure.figsize'] = 10, 8 # width 10, height 8\n",
    "temp = error_rate[error_rate[param2_label]==param2_list[0]]\n",
    "ax = temp.plot(x=param_label, y='rmse', style='bs-', grid=True)\n",
    "legend_list = [param2_label + '_' + str(param2_list[0])]\n",
    "\n",
    "color_list = ['r', 'g', 'k', 'y', 'm', 'c', '0.75']\n",
    "for i in range(1,len(param2_list)):\n",
    "    temp = error_rate[error_rate[param2_label]==param2_list[i]]\n",
    "    ax = temp.plot(x=param_label, y='rmse', color=color_list[i%len(color_list)], marker='s', grid=True, ax=ax)\n",
    "    legend_list.append(param2_label + '_' + str(param2_list[i]))\n",
    "\n",
    "ax.set_xlabel(param_label)\n",
    "ax.set_ylabel(\"RMSE\")\n",
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "plt.legend(legend_list, loc='center left', bbox_to_anchor=(1.0, 0.5)) # positions legend outside figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Get optimum value for param and param2\n",
    "temp = error_rate[error_rate['rmse'] == error_rate['rmse'].min()]\n",
    "learning_rate_opt = temp['learning_rate'].values[0]\n",
    "min_child_weight_opt = temp['min_child_weight'].values[0]\n",
    "print(\"min RMSE = %0.3f\" % error_rate['rmse'].min())\n",
    "print(\"optimum params = \")\n",
    "learning_rate_opt, min_child_weight_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Get optimum value for param and param2, using MAPE\n",
    "# We will use RMSE to decide the final optimum params to use\n",
    "temp = error_rate[error_rate['mape_pct'] == error_rate['mape_pct'].min()]\n",
    "print(\"min MAPE = %0.3f%%\" % error_rate['mape_pct'].min())\n",
    "print(\"optimum params = \")\n",
    "temp['learning_rate'].values[0], temp['min_child_weight'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning XGBoost - subsample(default=1) and gamma(default=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "param_label = 'subsample'\n",
    "param_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "param2_label = 'gamma'\n",
    "param2_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "error_rate = {param_label: [] , param2_label: [], 'rmse': [], 'mape_pct': []}\n",
    "tic = time.time()\n",
    "for param in tqdm_notebook(param_list):\n",
    "#     print(\"param = \" + str(param))\n",
    "    \n",
    "    for param2 in param2_list:\n",
    "        # Train, predict and eval model\n",
    "        rmse, mape, _ = train_pred_eval_model(X_train_scaled, \n",
    "                                     y_train_scaled, \n",
    "                                     X_cv_scaled, \n",
    "                                     y_cv, \n",
    "                                     cv['adj_close_mean'],\n",
    "                                     cv['adj_close_std'],\n",
    "                                     seed=model_seed,\n",
    "                                     n_estimators=n_estimators_opt, \n",
    "                                     max_depth=max_depth_opt, \n",
    "                                     learning_rate=learning_rate_opt, \n",
    "                                     min_child_weight=min_child_weight_opt, \n",
    "                                     subsample=param, \n",
    "                                     colsample_bytree=colsample_bytree, \n",
    "                                     colsample_bylevel=colsample_bylevel, \n",
    "                                     gamma=param2)\n",
    "    \n",
    "        # Collect results\n",
    "        error_rate[param_label].append(param)\n",
    "        error_rate[param2_label].append(param2)\n",
    "        error_rate['rmse'].append(rmse)\n",
    "        error_rate['mape_pct'].append(mape)\n",
    "    \n",
    "error_rate = pd.DataFrame(error_rate)\n",
    "toc = time.time()\n",
    "print(\"Minutes taken = \" + str((toc-tic)/60.0))\n",
    "error_rate   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot performance versus params\n",
    "rcParams['figure.figsize'] = 10, 8 # width 10, height 8\n",
    "temp = error_rate[error_rate[param2_label]==param2_list[0]]\n",
    "ax = temp.plot(x=param_label, y='rmse', style='bs-', grid=True)\n",
    "legend_list = [param2_label + '_' + str(param2_list[0])]\n",
    "\n",
    "color_list = ['r', 'g', 'k', 'y', 'm', 'c', '0.75']\n",
    "for i in range(1,len(param2_list)):\n",
    "    temp = error_rate[error_rate[param2_label]==param2_list[i]]\n",
    "    ax = temp.plot(x=param_label, y='rmse', color=color_list[i%len(color_list)], marker='s', grid=True, ax=ax)\n",
    "    legend_list.append(param2_label + '_' + str(param2_list[i]))\n",
    "\n",
    "ax.set_xlabel(param_label)\n",
    "ax.set_ylabel(\"RMSE\")\n",
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "plt.legend(legend_list, loc='center left', bbox_to_anchor=(1.0, 0.5)) # positions legend outside figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Get optimum value for param and param2\n",
    "temp = error_rate[error_rate['rmse'] == error_rate['rmse'].min()]\n",
    "subsample_opt = temp['subsample'].values[0]\n",
    "gamma_opt = temp['gamma'].values[0]\n",
    "print(\"min RMSE = %0.3f\" % error_rate['rmse'].min())\n",
    "print(\"optimum params = \")\n",
    "subsample_opt, gamma_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Get optimum value for param and param2, using MAPE\n",
    "# We will use RMSE to decide the final optimum params to use\n",
    "temp = error_rate[error_rate['mape_pct'] == error_rate['mape_pct'].min()]\n",
    "print(\"min MAPE = %0.3f%%\" % error_rate['mape_pct'].min())\n",
    "print(\"optimum params = \")\n",
    "temp['subsample'].values[0], temp['gamma'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning XGBoost - colsample_bytree(default=1) and colsample_bylevel(default=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "param_label = 'colsample_bytree'\n",
    "param_list = [0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "param2_label = 'colsample_bylevel'\n",
    "param2_list = [0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "error_rate = {param_label: [] , param2_label: [], 'rmse': [], 'mape_pct': []}\n",
    "tic = time.time()\n",
    "for param in tqdm_notebook(param_list):\n",
    "#     print(\"param = \" + str(param))\n",
    "    \n",
    "    for param2 in param2_list:\n",
    "        # Train, predict and eval model\n",
    "        rmse, mape, _ = train_pred_eval_model(X_train_scaled, \n",
    "                                     y_train_scaled, \n",
    "                                     X_cv_scaled, \n",
    "                                     y_cv, \n",
    "                                     cv['adj_close_mean'],\n",
    "                                     cv['adj_close_std'],\n",
    "                                     seed=model_seed,\n",
    "                                     n_estimators=n_estimators_opt, \n",
    "                                     max_depth=max_depth_opt, \n",
    "                                     learning_rate=learning_rate_opt, \n",
    "                                     min_child_weight=min_child_weight_opt, \n",
    "                                     subsample=subsample_opt, \n",
    "                                     colsample_bytree=param, \n",
    "                                     colsample_bylevel=param2, \n",
    "                                     gamma=gamma_opt)\n",
    "    \n",
    "        # Collect results\n",
    "        error_rate[param_label].append(param)\n",
    "        error_rate[param2_label].append(param2)\n",
    "        error_rate['rmse'].append(rmse)\n",
    "        error_rate['mape_pct'].append(mape)\n",
    "    \n",
    "error_rate = pd.DataFrame(error_rate)\n",
    "toc = time.time()\n",
    "print(\"Minutes taken = \" + str((toc-tic)/60.0))\n",
    "error_rate   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot performance versus params\n",
    "rcParams['figure.figsize'] = 10, 8 # width 10, height 8\n",
    "temp = error_rate[error_rate[param2_label]==param2_list[0]]\n",
    "ax = temp.plot(x=param_label, y='rmse', style='bs-', grid=True)\n",
    "legend_list = [param2_label + '_' + str(param2_list[0])]\n",
    "\n",
    "color_list = ['r', 'g', 'k', 'y', 'm', 'c', '0.75']\n",
    "for i in range(1,len(param2_list)):\n",
    "    temp = error_rate[error_rate[param2_label]==param2_list[i]]\n",
    "    ax = temp.plot(x=param_label, y='rmse', color=color_list[i%len(color_list)], marker='s', grid=True, ax=ax)\n",
    "    legend_list.append(param2_label + '_' + str(param2_list[i]))\n",
    "\n",
    "ax.set_xlabel(param_label)\n",
    "ax.set_ylabel(\"RMSE\")\n",
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "plt.legend(legend_list, loc='center left', bbox_to_anchor=(1.0, 0.5)) # positions legend outside figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Get optimum value for param and param2\n",
    "temp = error_rate[error_rate['rmse'] == error_rate['rmse'].min()]\n",
    "colsample_bytree_opt = temp['colsample_bytree'].values[0]\n",
    "colsample_bylevel_opt = temp['colsample_bylevel'].values[0]\n",
    "print(\"min RMSE = %0.3f\" % error_rate['rmse'].min())\n",
    "print(\"optimum params = \")\n",
    "colsample_bytree_opt, colsample_bylevel_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Get optimum value for param and param2, using MAPE\n",
    "# We will use RMSE to decide the final optimum params to use\n",
    "temp = error_rate[error_rate['mape_pct'] == error_rate['mape_pct'].min()]\n",
    "print(\"min MAPE = %0.3f%%\" % error_rate['mape_pct'].min())\n",
    "print(\"optimum params = \")\n",
    "temp['colsample_bytree'].values[0], temp['colsample_bylevel'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuned params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "d = {'param': ['n_estimators', 'max_depth', 'learning_rate', 'min_child_weight', 'subsample', 'colsample_bytree', 'colsample_bylevel', 'gamma', 'rmse', 'mape_pct'],\n",
    "     'original': [n_estimators, max_depth, learning_rate, min_child_weight, subsample, colsample_bytree, colsample_bylevel, gamma, rmse_bef_tuning, mape_bef_tuning],\n",
    "     'after_tuning': [n_estimators_opt, max_depth_opt, learning_rate_opt, min_child_weight_opt, subsample_opt, colsample_bytree_opt, colsample_bylevel_opt, gamma_opt, error_rate['rmse'].min(), error_rate['mape_pct'].min()]}\n",
    "tuned_params = pd.DataFrame(d)\n",
    "tuned_params = tuned_params.round(3)\n",
    "tuned_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "rmse, mape, est = train_pred_eval_model(X_train_cv_scaled, \n",
    "                             y_train_cv_scaled, \n",
    "                             X_sample_scaled, \n",
    "                             y_sample, \n",
    "                             test['adj_close_mean'],\n",
    "                             test['adj_close_std'],\n",
    "                             seed=model_seed,\n",
    "                             n_estimators=n_estimators_opt, \n",
    "                             max_depth=max_depth_opt, \n",
    "                             learning_rate=learning_rate_opt, \n",
    "                             min_child_weight=min_child_weight_opt, \n",
    "                             subsample=subsample_opt, \n",
    "                             colsample_bytree=colsample_bytree_opt, \n",
    "                             colsample_bylevel=colsample_bylevel_opt, \n",
    "                             gamma=gamma_opt)\n",
    "\n",
    "# Calculate RMSE\n",
    "print(\"RMSE on test set = %0.3f\" % rmse)\n",
    "\n",
    "# Calculate MAPE\n",
    "print(\"MAPE on test set = %0.3f%%\" % mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot adjusted close over time\n",
    "rcParams['figure.figsize'] = 10, 8 # width 10, height 8\n",
    "\n",
    "est_df = pd.DataFrame({'est': est, \n",
    "                       'y_sample': y_sample,\n",
    "                       'date': test['date']})\n",
    "\n",
    "ax = train.plot(x='date', y='adj_close', style='b-', grid=True)\n",
    "ax = cv.plot(x='date', y='adj_close', style='y-', grid=True, ax=ax)\n",
    "ax = test.plot(x='date', y='adj_close', style='g-', grid=True, ax=ax)\n",
    "ax = est_df.plot(x='date', y='est', style='r-', grid=True, ax=ax)\n",
    "ax.legend(['train', 'dev', 'test', 'predictions'])\n",
    "ax.set_xlabel(\"date\")\n",
    "ax.set_ylabel(\"USD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot adjusted close over time, for test set only\n",
    "rcParams['figure.figsize'] = 10, 8 # width 10, height 8\n",
    "ax = train.plot(x='date', y='adj_close', style='b-', grid=True)\n",
    "ax = cv.plot(x='date', y='adj_close', style='y-', grid=True, ax=ax)\n",
    "ax = test.plot(x='date', y='adj_close', style='g-', grid=True, ax=ax)\n",
    "ax = est_df.plot(x='date', y='est', style='r-', grid=True, ax=ax)\n",
    "ax.legend(['train', 'dev', 'test', 'predictions'])\n",
    "ax.set_xlabel(\"date\")\n",
    "ax.set_ylabel(\"USD\")\n",
    "ax.set_xlim([date(2018, 4, 1), date(2018, 11, 30)])\n",
    "ax.set_ylim([130, 155])\n",
    "ax.set_title(\"Zoom in to test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to dev set, the predictions capture turns in direction with a slight lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot adjusted close over time, only for test set\n",
    "rcParams['figure.figsize'] = 10, 8 # width 10, height 8\n",
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "\n",
    "ax = test.plot(x='date', y='adj_close', style='gx-', grid=True)\n",
    "ax = est_df.plot(x='date', y='est', style='rx-', grid=True, ax=ax)\n",
    "ax.legend(['test', 'predictions using xgboost'], loc='upper left')\n",
    "ax.set_xlabel(\"date\")\n",
    "ax.set_ylabel(\"USD\")\n",
    "ax.set_xlim([date(2018, 4, 23), date(2018, 11, 23)])\n",
    "ax.set_ylim([130, 155])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Save as csv\n",
    "test_xgboost = est_df\n",
    "test_xgboost.to_csv(\"./out/test_xgboost.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Findings\n",
    "* By scaling the features properly, we can get good results for our predictions\n",
    "* RMSE and MAPE changed very little with hyperparameter tuning\n",
    "* The final RMSE and MAPE for test set are 1.162 and 0.58% respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}